{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Quality Assessment for ERA5 Drought Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Production date: DD-MM-YYYY\n",
    "\n",
    "*Please note that this repository is used for development and review, so quality assessments should be considered work in progress until they are merged into the main branch.*\n",
    "\n",
    "Produced by: C3S2521 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸŒ Use case: Use case listed here in full "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## â“ Quality assessment question\n",
    "* **In most cases there should be one question listed here in bold**\n",
    "* **(In some cases a second related/follow-up question may be included)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "**â€˜Context paragraphâ€™ (no title/heading)** - a very short introduction before the assessment statement describing approach taken to answer the user question. One or two key references could be useful,  if the assessment summarises literature . These can be referenced directly in the text, like `[Rodriguez et. al. 2018](https://doi.org/10.1038/s41598-018-20628-2)` giving: [Rodriguez et. al. 2018](https://doi.org/10.1038/s41598-018-20628-2). For major references numerical labels like this should be used (which should also listed at the end) `Rodriguez et. al. 2018, [[1]](https://doi.org/10.1038/s41598-018-20628-2))`giving: Rodriguez et. al. 2018, [[1]](https://doi.org/10.1038/s41598-018-20628-2)). Please use DOI links where possible.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ“¢ Quality assessment statement\n",
    "\n",
    "```{admonition} These are the key outcomes of this assessment\n",
    ":class: note\n",
    "* Finding 1\n",
    "* Finding 2\n",
    "* Finding 3\n",
    "* etc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸ“‹ Methodology\n",
    "\n",
    "A â€˜free textâ€™ introduction to the data analysis steps or a description of the literature synthesis, with a justification of the approach taken, and limitations mentioned. **Mention which CDS catalogue entry is used, including a link, and also any other entries used for the assessment**.\n",
    "\n",
    "Followed by a numbered list of the methodology and results, with the same headings as the sections under â€˜Analysis and Resultsâ€™. These should be links to the sections below, using the format `[](section-label)`. The title of the section will be automatically populated, so no need to repeat the title of the section when referecing it like this.\n",
    "\n",
    "```{note}\n",
    "The section labels for the links need to be manually set, as seen below (`(section-1)=`, followed by the heading). These labels will be shown in GitHub but will not appear when the Jupyter Book page is built.\n",
    "```\n",
    "\n",
    "* These headings can be specific to the quality assessment, and help guide the user through the â€˜storyâ€™ of the assessment. This means we cannot pre-define the sections and headings here, as they will be different for each assessment.\n",
    "* Sub-bullets could be used to outline what will be done/shown/discussed in each section\n",
    "* The list below is just an example, or may need more or fewer sections, with different headings\n",
    "\n",
    "E.g. 'The analysis and results are organised in the following steps, which are detailed in the sections below:' \n",
    "\n",
    "**[](section-1)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-2)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-3)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-4)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    " \n",
    "**[](section-5)** \n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "Any further notes on the method could go here (explanations, caveats or limitations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Analysis and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-1)=\n",
    "### 1. Section 1 title\n",
    "\n",
    "#### Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does (**if code is included** - some assessment may review literature or reports like PQARs for ECVs, in which case, a markdown file could be provided instead). Note that some details may be better placed in code comments, rather than in the text above the code cell, to help the flow of the Notebook.\n",
    "\n",
    "**Code:**\n",
    "\n",
    "* Cell output should be cleaned up as needed (right click the cell after it has run and delete the output if needed), this can also be done with the `'hide-output'` cell tag, added under 'Common Tools -> Cell Tags' on the right in Jupyter Lab (under the cog icon).\n",
    "* Please consider when cell output would be useful to include, such as printing the summary of the xarray data cube, or an example image. \n",
    "* Please link to any non-standard libraries in the references, including the functions from B-Open (a standard line on this is included in the references section).\n",
    "* The code cells will be adjusted to be 'collapsed by default' when the Jupyter Book page is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Input / Output\n",
    "from pathlib import Path\n",
    "import earthkit.data as ekd\n",
    "import warnings\n",
    "\n",
    "# General data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "\n",
    "# Visualisation\n",
    "import earthkit.plots as ekp\n",
    "from earthkit.plots.styles import Style\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "from tqdm import tqdm  # Progress bars\n",
    "\n",
    "# Visualisation in Jupyter book -- automatically ignored otherwise\n",
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-2)=\n",
    "### ERA5-Drought SPI\n",
    "\n",
    "#### Reading in data from ERA5-Drought\n",
    "Describe what is done in this step/section and what the `code` in the cell does (if code is included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "request = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\", \"1981\",\n",
    "        \"1982\", \"1983\", \"1984\",\n",
    "        \"1985\", \"1986\", \"1987\",\n",
    "        \"1988\", \"1989\", \"1990\",\n",
    "        \"1991\", \"1992\", \"1993\",\n",
    "        \"1994\", \"1995\", \"1996\",\n",
    "        \"1997\", \"1998\", \"1999\",\n",
    "        \"2000\", \"2001\", \"2002\",\n",
    "        \"2003\", \"2004\", \"2005\",\n",
    "        \"2006\", \"2007\", \"2008\",\n",
    "        \"2009\", \"2010\", \"2011\",\n",
    "        \"2012\", \"2013\", \"2014\",\n",
    "        \"2015\", \"2016\", \"2017\",\n",
    "        \"2018\", \"2019\", \"2020\",\n",
    "        \"2021\", \"2022\", \"2023\",\n",
    "        \"2024\", \"2025\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drought = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "data_drought = data_drought.to_xarray(compat=\"equals\") # Converts to xarray.\n",
    "data_drought  # Display in notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Using Earthkit for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_map = data_drought.sel(time=\"2023-08-01\")\n",
    "fig = ekp.Figure(rows=1, columns=1, size=(5,5)) # Create the figure with dimensions.\n",
    "subplot = fig.add_map(domain=\"Europe\") # Put in a panel (add_map).\n",
    "subplot.grid_cells(global_map, z=\"SPEI12\") # Same as pcolormesh. Dataset, Variable.\n",
    "subplot.legend(location=\"right\")\n",
    "    \n",
    "fig.land() \n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()\n",
    "\n",
    "# Play around with the ensemble members- how to calculate standard uncertainty between all?\n",
    "# Creating a filtering function? Say if you wanted to select a range of longitudes and latitudes.\n",
    "# Creating an animated gif with ensemble members? A slider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try timeseries for a subset of time.\n",
    "\n",
    "# time_series = data_drought.sel(time = [\"2023-01-01\",\"2023-12-31\"],lat=51.5, lon=0.0, method=\"nearest\") # Only plots two datapoints.\n",
    "\n",
    "# # time_series = data_drought.sel(lat=slice(51,52), lon=0.0, method=\"nearest\") # This did not work as could not use method on range.\n",
    "# time_series = data_drought.sel(time=\"2023-01-01\",lon=0.0,method=\"nearest\")\n",
    "# time_series = time_series.sel(lat=slice(54,51)) # Longitude goes from 90 to -90 be careful !!!\n",
    "# time_series[\"Wind_Speed_10m_Mean_24h\"].plot.line()\n",
    "\n",
    "time_series = data_drought.sel(lat=25, lon=0.0, method=\"nearest\")\n",
    "time_series[\"pzero\"].plot.line() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try downloading a different variable.\n",
    "\n",
    "# time_series = data_drought.sel(lat=51.5, lon=0.0, method=\"nearest\")\n",
    "# time_series = data_drought.sel(time=\"2023-01-01\") # This did not work as could not use method on range.\n",
    "\n",
    "time_series = data_drought.sel(time=\"2023-01-01\",method=\"nearest\")\n",
    "global_mean = time_series[\"SPEI1\"].mean() # Global mean has been successfully computed, but it's still a lazy Dask array\n",
    "print(global_mean.compute()) # Triggers to print computation.\n",
    "\n",
    "\n",
    "time_series[\"SPEI1\"].plot.pcolormesh() # Why did I need to squeeze the dimension here? -> because there can be multiple measurements in a day.\n",
    "time_series[\"SPI1\"].plot.pcolormesh() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly average at each pixel.\n",
    "\n",
    "global_mean = data_drought[\"SPEI12\"].mean(\"time\") # Calculates mean along time axis.\n",
    "fig = ekp.Figure(rows=1, columns=1, size=(5,5)) # Create the figure with dimensions.\n",
    "subplot = fig.add_map(domain=\"Europe\") # Put in a panel (add_map).\n",
    "subplot.grid_cells(global_mean, z=\"SPEI12\") # Same as pcolormesh. Dataset, Variable.\n",
    "subplot.legend(location=\"right\")\n",
    "    \n",
    "fig.land() \n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Calculating SPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Read in total precipitation data (monthly) from ERA5 analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import earthkit.data\n",
    "import numpy as np\n",
    "import scipy.stats as stats    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 9.2\n",
    "lng = 40.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"total_precipitation\"\n",
    "date_range = [\"1940-01-01T06:00:00.000000000\", \"2020-12-31T06:00:00.000000000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tp_data(variable, date_range, lat, lng):\n",
    "    # Define the dataset and request parameters\n",
    "    dataset = \"reanalysis-era5-single-levels-timeseries\"\n",
    "    request = {\n",
    "        \"variable\": [\n",
    "        variable,  # Variable to retrieve\n",
    "        ],\n",
    "        \"date\": date_range,  # Date range for the data\n",
    "        \"location\": {\"longitude\": lng, \"latitude\": lat},  # Location coordinates\n",
    "        \"data_format\": \"netcdf\"  # Format of the retrieved data\n",
    "    }\n",
    "\n",
    "    # Use \"earthkit\" to retrieve the data\n",
    "    ekds = earthkit.data.from_source(\n",
    "        \"cds\", dataset, request\n",
    "    ).to_xarray()\n",
    "\n",
    "    return ekds\n",
    "    \n",
    "data = retrieve_tp_data(variable, date_range, lat, lng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to compute the monthly precipitation climatology\n",
    "def precipMonthly(data):\n",
    "    \"\"\"\n",
    "    Calculate the monthly climatology of precipitation.\n",
    "\n",
    "    This function reads precipitation data from a NetCDF file,\n",
    "    converts the time coordinate to a pandas datetime index,\n",
    "    and then resamples the data to calculate the monthly \n",
    "    climatology. The resulting climatology is returned in millimeters.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the monthly climatology\n",
    "        of precipitation in millimeters, indexed by month.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_tp_pt = data.tp # Accessing total precipitation.\n",
    "\n",
    "    # Convert the time coordinate to a pandas datetime index\n",
    "    time_index = pd.to_datetime(data_tp_pt.valid_time.values)\n",
    "\n",
    "    # Create a DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(data_tp_pt.values*1000, index=time_index, columns=['tp (mm)'])\n",
    "\n",
    "    df_monthly = df.resample('MS').sum() # .gives monthly totals by summing precipitation within each month.\n",
    "    df_monthly['month'] = df_monthly.index.month # Extracts the month number (1â€“12) from the index and adds it as a column.\n",
    "    monthly_climatology = df_monthly.groupby('month').mean()\n",
    "\n",
    "    # Get the actual lat/lon used\n",
    "    nearest_lat = data_tp_pt.latitude.values\n",
    "    nearest_lng = data_tp_pt.longitude.values\n",
    "\n",
    "    return df_monthly, monthly_climatology, nearest_lat, nearest_lng\n",
    "\n",
    "# Call our function\n",
    "df_monthly, clim, nearest_lat, nearest_lng = precipMonthly(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Calculate moving average for different accumulation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "# Does xarray have a moving window.\n",
    "\n",
    "for period in acc_periods:\n",
    "    x = df_monthly['tp (mm)'].values\n",
    "    conv_result = moving_average(x,period)  # rolling average\n",
    "    \n",
    "    # Create an array of full length with NaNs\n",
    "    aligned = np.full(len(df_monthly), np.nan)\n",
    "    \n",
    "    # Place convolution result starting at index period-1\n",
    "    aligned[period-1:] = conv_result\n",
    "    \n",
    "    df_monthly[f\"Accumulation-{period} months\"] = aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Plot of accumulation periods (1 to 48) months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for period in acc_periods:\n",
    "    plt.plot(df_monthly.index, df_monthly[f\"Accumulation-{period} months\"], label=f\"{period}-month\")\n",
    "plt.title(\"Accumulation Period Comparison\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Precipitation (mm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Fitting gamma distribution to different accumulation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_periods = [1,3,6,12,24, 36, 48]\n",
    "\n",
    "start_ref, end_ref  = \"1991-01-01\", \"2020-12-01\"\n",
    "\n",
    "# Dictionary to store fitted parameters for each period\n",
    "gamma_params = {}\n",
    "\n",
    "def fit_gamma_distributions(df, acc_periods, start_ref, end_ref):\n",
    "    \n",
    "    # Function to fit gamma distribution on any precipitation dataframe.\n",
    "    for period in acc_periods:\n",
    "        reference_data = df.loc[start_ref:end_ref, f\"Accumulation-{period} months\"].dropna()\n",
    "        alpha, loc, beta = stats.gamma.fit(reference_data)\n",
    "        gamma_params[period] = (alpha, loc, beta)\n",
    "    return gamma_params # returns the gamma paramaters, associated with that period.\n",
    "\n",
    "def spi_norm_ppf(df, acc_periods):\n",
    "    spi_df  = pd.DataFrame(index=df.index)\n",
    "    for period in acc_periods:\n",
    "        data = df[f\"SPI-{period}\"]\n",
    "        # Convert to standard normal\n",
    "        spi = stats.norm.ppf(data, loc=0, scale=1)\n",
    "        spi_df[f\"SPI-{period}\"] = spi\n",
    "    return spi_df # returns SPI df series.\n",
    "\n",
    "def compute_spi_series(df, acc_periods, gamma_params): \n",
    "    # Function to compute SPI time series from dataframe, period and gamma parameters.\n",
    "    spi_df = pd.DataFrame(index=df.index)\n",
    "    pdf_df = pd.DataFrame(index=df.index) \n",
    "    cdf_df = pd.DataFrame(index=df.index) \n",
    "    for period in acc_periods:\n",
    "        alpha, loc, beta = gamma_params[period]\n",
    "        data = df[f\"Accumulation-{period} months\"]\n",
    "        \n",
    "        pdf_values = stats.gamma.pdf(data, a=alpha, loc=loc, scale=beta)\n",
    "        cdf_values = stats.gamma.cdf(data, a=alpha, loc=loc, scale=beta)\n",
    "        spi_values = stats.norm.ppf(stats.gamma.cdf(data, a=alpha, loc=loc, scale=beta),loc=0, scale=1)\n",
    "        \n",
    "        spi_df[f\"SPI-{period}\"] = spi_values\n",
    "        pdf_df[f\"SPI-{period}\"] = pdf_values\n",
    "        cdf_df[f\"SPI-{period}\"] = cdf_values\n",
    "    return spi_df, pdf_df, cdf_df # returns SPI times series.\n",
    "\n",
    "def compute_spi(value, acc_period):\n",
    "    # Function to compute SPI for any tp value in any period.\n",
    "    alpha, loc, beta = gamma_params[acc_period]\n",
    "    # Compute CDF under gamma\n",
    "    cdf = stats.gamma.cdf(value, a=alpha, loc=loc, scale=beta)\n",
    "    # Convert to standard normal\n",
    "    spi = stats.norm.ppf(cdf, loc=0, scale=1)\n",
    "    return spi # returns SPI single value.\n",
    "\n",
    "def get_spi_for_month(spi_df, date, acc_period):\n",
    "    # Function to return SPI for any month in any accumulation period.\n",
    "    return spi_df.loc[date, f\"SPI-{acc_period}\"]\n",
    "\n",
    "def plot_spi_series(spi_series, acc_periods):\n",
    "    # Function to plot SPI series.\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for period in acc_periods:\n",
    "        plt.plot(spi_series.index, spi_series[f\"SPI-{period}\"], label=f\"SPI-{period}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Standardized Precipitation Index\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"SPI Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def compute_spi_tp(spi_df,acc_periods, gamma_params):\n",
    "    # Function to compute the SPI vs tp.\n",
    "    spi_results = {}\n",
    "    pdf_spi_values = {}\n",
    "    cdf_spi_values = {}\n",
    "    \n",
    "    for period in acc_periods:\n",
    "        col_name = f\"SPI-{period}\"\n",
    "        data = spi_df[col_name]\n",
    "        x = np.linspace(0, 200, 400)\n",
    "        alpha, loc, beta = gamma_params[period]\n",
    "        \n",
    "        pdf_spi_values[period] = stats.gamma.pdf(x, a=alpha, loc=loc, scale=beta)\n",
    "        cdf_spi_values[period] = stats.gamma.cdf(x, a=alpha, loc=loc, scale=beta)\n",
    "        spi_values = stats.norm.ppf(stats.gamma.cdf(x, a=alpha, loc=loc, scale=beta),loc=0, scale=1)\n",
    "        \n",
    "        spi_results[period] = (spi_values) \n",
    "        \n",
    "    return spi_results, pdf_spi_values, cdf_spi_values # returns SPI results as dictionary with period.\n",
    "\n",
    "def plot_spi_tp(spi_tp, acc_periods):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for period in acc_periods:\n",
    "        plt.plot( spi_tp[period], label=f\"SPI-{period}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Standardized Precipitation Index\")\n",
    "    plt.xlabel(\"Total Precipitation (mm)\")\n",
    "    plt.ylabel(\"SPI Value\")\n",
    "    plt.ylim([-8,8])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Look at ensemble for uncertainty.\n",
    "# Calculating the SPEI too.\n",
    "# Visual comparison of differences test at multiple locations.\n",
    "# Look at the quality test.\n",
    "# The zero precipitation adjustment.\n",
    "# Look at the ensemble. \n",
    "# Plot of uncertainty in precipitation in x axis and uncertainty in SPI in y axis for ensemble.\n",
    "# How does uncertainty in reference period affect uncertainty in SPI.- all the same for SPEI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit gamma distributions\n",
    "gamma_params = fit_gamma_distributions(df_monthly, acc_periods, start_ref, end_ref)\n",
    "\n",
    "# 2. Compute SPI DataFrame\n",
    "spi_df, pdf_spi_series, cdf_spi_series = compute_spi_series(df_monthly, acc_periods, gamma_params)\n",
    "\n",
    "# 2.1 Compute SPI vs tp curve\n",
    "spi_tp, pdf_spi_tp, cdf_spi_tp = compute_spi_tp(spi_df, acc_periods, gamma_params)\n",
    "\n",
    "# # 3. Plot SPI time series\n",
    "# plot_spi_series(spi_df, acc_periods)\n",
    "\n",
    "# # 4. Plot SPI precipitation curve\n",
    "# plot_spi_tp(spi_tp, acc_periods)\n",
    "\n",
    "# 5. Query SPI for a specific month\n",
    "spi_value = get_spi_for_month(spi_df, \"2003-07-01\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### Locations with zero precipitation months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Calculating historical ratio of months without precipitation and finding zero adjusted SPI - BAD WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "# Initialize DataFrame for adjusted CDF\n",
    "cdf_adjusted_series = pd.DataFrame(index=df_monthly.index)\n",
    "\n",
    "# Compute zero-precip stats and adjusted CDF in one loop\n",
    "stats_summary = {}\n",
    "\n",
    "for period in accum_periods:\n",
    "    col = f\"Accumulation-{period} months\"\n",
    "    spi_col = f\"SPI-{period}\"\n",
    "    \n",
    "    # Calculate zero-precip count and total months\n",
    "    n_zero = (df_monthly[col] < 0.1).sum()\n",
    "    n_month = df_monthly[col].count()\n",
    "    \n",
    "    # Probability of zero precipitation\n",
    "    p_zero = n_zero / (n_month + 1)\n",
    "    \n",
    "    # Adjusted CDF\n",
    "    cdf_adjusted_series[spi_col] = p_zero + (1 - p_zero) * cdf_spi_series[spi_col]\n",
    "    \n",
    "    # Store summary stats\n",
    "    stats_summary[spi_col] = {\n",
    "        \"Zero-Precip Count\": int(n_zero),\n",
    "        \"Total Months\": int(n_month),\n",
    "        \"Prob Zero Precip\": p_zero\n",
    "    }\n",
    "\n",
    "# Convert summary to DataFrame for easy viewing\n",
    "zero_prep_stats_df = pd.DataFrame(stats_summary)\n",
    "\n",
    "spi_series_adjusted = spi_norm_ppf(cdf_adjusted_series, acc_periods) # inverse norm for zero precipitation adjusted.\n",
    "\n",
    "plot_spi_series(spi_series_adjusted, acc_periods) # plot adjusted \n",
    "plot_spi_series(spi_df, acc_periods) # plot non adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "##### Calculating historical ratio of months without precipitation and finding zero adjusted SPI - GOOD WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame for adjusted CDF\n",
    "cdf_adjusted_series = pd.DataFrame(index=df_monthly.index)\n",
    "\n",
    "# Compute zero-precip stats and adjusted CDF in one loop\n",
    "stats_summary = {}\n",
    "\n",
    "for period in accum_periods:\n",
    "    col = f\"Accumulation-{period} months\"\n",
    "    spi_col = f\"SPI-{period}\"\n",
    "    \n",
    "    # Calculate zero-precip count and total months\n",
    "    n_zero = (df_monthly[col] < 0.1).sum()\n",
    "    n_month = df_monthly[col].count()\n",
    "    \n",
    "    # Probability of zero precipitation\n",
    "    p_zero = (n_zero+1)/ (2*(n+1)) # Using the centre of mass of the zero distribution instead of the maximum probability\n",
    "    \n",
    "    # Adjusted CDF\n",
    "    cdf_adjusted_series[spi_col] = p_zero + (1 - p_zero) * cdf_spi_series[spi_col]\n",
    "    \n",
    "    # Store summary stats\n",
    "    stats_summary[spi_col] = {\n",
    "        \"Zero-Precip Count\": int(n_zero),\n",
    "        \"Total Months\": int(n_month),\n",
    "        \"Prob Zero Precip\": p_zero\n",
    "    }\n",
    "\n",
    "spi_series_adjusted = spi_norm_ppf(cdf_adjusted_series, acc_periods) # inverse norm for zero precipitation adjusted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Check calculated SPI against given (one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import earthkit.data\n",
    "import numpy as np\n",
    "import scipy.stats as stats    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "\n",
    "request1 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75]\n",
    "}\n",
    "\n",
    "request2 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1981\", \"1982\", \"1983\",\n",
    "        \"1984\", \"1985\", \"1986\",\n",
    "        \"1987\", \"1988\", \"1989\",\n",
    "        \"1990\", \"1991\", \"1992\",\n",
    "        \"1993\", \"1994\", \"1995\",\n",
    "        \"1996\", \"1997\", \"1998\",\n",
    "        \"1999\", \"2000\", \"2001\",\n",
    "        \"2002\", \"2003\", \"2004\",\n",
    "        \"2005\", \"2006\", \"2007\",\n",
    "        \"2008\", \"2009\", \"2010\",\n",
    "        \"2011\", \"2012\", \"2013\",\n",
    "        \"2014\", \"2015\", \"2016\",\n",
    "        \"2017\", \"2018\", \"2019\",\n",
    "        \"2020\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75] # Ethiopia\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                          | 0/2 [00:00<?, ?it/s]2025-11-11 11:12:16,404 WARNING [2025-10-30T00:00:00] Downloading data from this dataset requires that the user be registered with the CDS. The use of the API requires a CDS API key.\n",
      "2025-11-11 11:12:16,405 INFO Request ID is f3b30f1b-d9af-4771-92be-4ebd316dfd17\n",
      "2025-11-11 11:12:16,407 WARNING [2025-10-30T00:00:00] Downloading data from this dataset requires that the user be registered with the CDS. The use of the API requires a CDS API key.\n",
      "2025-11-11 11:12:16,408 INFO Request ID is dbaecfab-4559-42af-98b1-6cc65991a1c5\n",
      "2025-11-11 11:12:16,484 INFO status has been updated to accepted\n",
      "2025-11-11 11:12:16,487 INFO status has been updated to accepted\n",
      "2025-11-11 11:12:30,028 INFO status has been updated to running\n",
      "2025-11-11 11:12:30,077 INFO status has been updated to running\n"
     ]
    }
   ],
   "source": [
    "data_drought = ekd.from_source(\"cds\", dataset, request1,request2) # Sends request for this dataset to CDS.\n",
    "data_drought = data_drought.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_drought = data_drought.sel()\n",
    "data_drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculated_spi_to_xarray(df):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame of SPI values into an xarray Dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame with columns like SPI-1, SPI-3, ..., SPI-48\n",
    "                               and datetime index.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: Dataset with dimensions (time) and variables for each SPI period.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary of variables for xarray\n",
    "    data_vars = {}\n",
    "    for col in df.columns:\n",
    "        var_name = col.replace(\"-\",\"\") \n",
    "        data_vars[var_name] = ([\"time\"], df[col].values)\n",
    "\n",
    "    # Build the Dataset\n",
    "    ds = xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords={\"time\": df.index}\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Add 6 hours to all time coordinates otherwise they don't match!\n",
    "    ds['time'] = ds['time'] + np.timedelta64(6, 'h')\n",
    "\n",
    "    return ds\n",
    "\n",
    "spi_calculated = calculated_spi_to_xarray(spi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_calculated, data_drought = xr.align(spi_calculated, data_drought, join=\"inner\")  # keep only matching times\n",
    "result = (spi_calculated - data_drought).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "acc_periods=[48]\n",
    "for period in acc_periods:\n",
    "    plt.plot(spi_calculated[f\"SPI{period}\"]/data_drought[f\"SPI{period}\"] - 1, label=f\"Calculated-SPI{period}\")\n",
    "    # plt.plot(data_drought[f\"SPI{period}\"], label=f\"ECMWF-SPI{period}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Standardized Precipitation Index\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"SPI Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Calculating SPEI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-3)=\n",
    "### 3. Section 3 title\n",
    "#### Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does (if code is included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cells\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-4)=\n",
    "### 4. Section 4 title\n",
    "\n",
    "#### Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does(if code is included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 3.749769,
     "end_time": "2024-03-08T17:24:00.248720",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.498951",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cell\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.245188,
     "end_time": "2024-03-08T17:39:21.277354",
     "exception": false,
     "start_time": "2024-03-08T17:39:21.032166",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-5)=\n",
    "### 5. Section 5 title \n",
    "\n",
    "#### Results Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does (if code is included). \n",
    "\n",
    "If this is the **results section**, we expect the final plots to be created here with a description of how to interpret them, and what information can be extracted for the specific use case and user question. The information in the 'quality assessment statement' should be derived here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 3.749769,
     "end_time": "2024-03-08T17:24:00.248720",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.498951",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cell\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.271597,
     "end_time": "2024-03-08T17:40:01.664067",
     "exception": false,
     "start_time": "2024-03-08T17:40:01.392470",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## â„¹ï¸ If you want to know more\n",
    "\n",
    "### Key resources\n",
    "\n",
    "List some key resources related to this assessment. E.g. CDS entries, applications, dataset documentation, external pages.\n",
    "Also list any code libraries used (if applicable).\n",
    "\n",
    "Code libraries used:\n",
    "* [C3S EQC custom functions](https://github.com/bopen/c3s-eqc-automatic-quality-control/tree/main/c3s_eqc_automatic_quality_control), `c3s_eqc_automatic_quality_control`,  prepared by [B-Open](https://www.bopen.eu/)\n",
    "\n",
    "### References\n",
    "\n",
    "List the references used in the Notebook here.\n",
    "\n",
    "E.g.\n",
    "\n",
    "[[1]](https://doi.org/10.1038/s41598-018-20628-2) Rodriguez, D., De Voil, P., Hudson, D., Brown, J. N., Hayman, P., Marrou, H., & Meinke, H. (2018). Predicting optimum crop designs using crop models and seasonal climate forecasts. Scientific reports, 8(1), 2231."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 968.520731,
   "end_time": "2024-03-08T17:40:03.783430",
   "environment_variables": {},
   "exception": null,
   "input_path": "D520.3.2.3b.SEASONAL_multimodel-bias_v5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T17:23:55.262699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
