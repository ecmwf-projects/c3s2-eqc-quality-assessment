{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Quality Assessment for ERA5 Drought Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Production date: DD-MM-YYYY\n",
    "\n",
    "*Please note that this repository is used for development and review, so quality assessments should be considered work in progress until they are merged into the main branch.*\n",
    "\n",
    "Produced by: C3S2521 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸŒ Use case: Use case listed here in full "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## â“ Quality assessment question\n",
    "* **In most cases there should be one question listed here in bold**\n",
    "* **(In some cases a second related/follow-up question may be included)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "**â€˜Context paragraphâ€™ (no title/heading)** - a very short introduction before the assessment statement describing approach taken to answer the user question. One or two key references could be useful,  if the assessment summarises literature . These can be referenced directly in the text, like `[Rodriguez et. al. 2018](https://doi.org/10.1038/s41598-018-20628-2)` giving: [Rodriguez et. al. 2018](https://doi.org/10.1038/s41598-018-20628-2). For major references numerical labels like this should be used (which should also listed at the end) `Rodriguez et. al. 2018, [[1]](https://doi.org/10.1038/s41598-018-20628-2))`giving: Rodriguez et. al. 2018, [[1]](https://doi.org/10.1038/s41598-018-20628-2)). Please use DOI links where possible.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ“¢ Quality assessment statement\n",
    "\n",
    "```{admonition} These are the key outcomes of this assessment\n",
    ":class: note\n",
    "* Finding 1\n",
    "* Finding 2\n",
    "* Finding 3\n",
    "* etc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸ“‹ Methodology\n",
    "\n",
    "A â€˜free textâ€™ introduction to the data analysis steps or a description of the literature synthesis, with a justification of the approach taken, and limitations mentioned. **Mention which CDS catalogue entry is used, including a link, and also any other entries used for the assessment**.\n",
    "\n",
    "Followed by a numbered list of the methodology and results, with the same headings as the sections under â€˜Analysis and Resultsâ€™. These should be links to the sections below, using the format `[](section-label)`. The title of the section will be automatically populated, so no need to repeat the title of the section when referecing it like this.\n",
    "\n",
    "```{note}\n",
    "The section labels for the links need to be manually set, as seen below (`(section-1)=`, followed by the heading). These labels will be shown in GitHub but will not appear when the Jupyter Book page is built.\n",
    "```\n",
    "\n",
    "* These headings can be specific to the quality assessment, and help guide the user through the â€˜storyâ€™ of the assessment. This means we cannot pre-define the sections and headings here, as they will be different for each assessment.\n",
    "* Sub-bullets could be used to outline what will be done/shown/discussed in each section\n",
    "* The list below is just an example, or may need more or fewer sections, with different headings\n",
    "\n",
    "E.g. 'The analysis and results are organised in the following steps, which are detailed in the sections below:' \n",
    "\n",
    "**[](section-1)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-2)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-3)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-4)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    " \n",
    "**[](section-5)** \n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "Any further notes on the method could go here (explanations, caveats or limitations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Analysis and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-1)=\n",
    "### 1. Section 1 title\n",
    "\n",
    "#### Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does (**if code is included** - some assessment may review literature or reports like PQARs for ECVs, in which case, a markdown file could be provided instead). Note that some details may be better placed in code comments, rather than in the text above the code cell, to help the flow of the Notebook.\n",
    "\n",
    "**Code:**\n",
    "\n",
    "* Cell output should be cleaned up as needed (right click the cell after it has run and delete the output if needed), this can also be done with the `'hide-output'` cell tag, added under 'Common Tools -> Cell Tags' on the right in Jupyter Lab (under the cog icon).\n",
    "* Please consider when cell output would be useful to include, such as printing the summary of the xarray data cube, or an example image. \n",
    "* Please link to any non-standard libraries in the references, including the functions from B-Open (a standard line on this is included in the references section).\n",
    "* The code cells will be adjusted to be 'collapsed by default' when the Jupyter Book page is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Input / Output\n",
    "from pathlib import Path\n",
    "import earthkit.data as ekd\n",
    "import warnings\n",
    "\n",
    "# General data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "\n",
    "# Visualisation\n",
    "import earthkit.plots as ekp\n",
    "from earthkit.plots.styles import Style\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "from tqdm import tqdm  # Progress bars\n",
    "\n",
    "# Visualisation in Jupyter book -- automatically ignored otherwise\n",
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = None\n",
    "\n",
    "import calendar\n",
    "import earthkit.data\n",
    "import scipy.stats as stats    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-2)=\n",
    "### ERA5-Drought SPI\n",
    "\n",
    "#### Reading in data from ERA5-Drought\n",
    "Describe what is done in this step/section and what the `code` in the cell does (if code is included)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Using Earthkit for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_map = data_drought.sel(time=\"2023-08-01\")\n",
    "fig = ekp.Figure(rows=1, columns=1, size=(5,5)) # Create the figure with dimensions.\n",
    "subplot = fig.add_map(domain=\"Europe\") # Put in a panel (add_map).\n",
    "subplot.grid_cells(global_map, z=\"SPEI12\") # Same as pcolormesh. Dataset, Variable.\n",
    "subplot.legend(location=\"right\")\n",
    "    \n",
    "fig.land() \n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()\n",
    "\n",
    "# Play around with the ensemble members- how to calculate standard uncertainty between all?\n",
    "# Creating a filtering function? Say if you wanted to select a range of longitudes and latitudes.\n",
    "# Creating an animated gif with ensemble members? A slider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try timeseries for a subset of time.\n",
    "\n",
    "# time_series = data_drought.sel(time = [\"2023-01-01\",\"2023-12-31\"],lat=51.5, lon=0.0, method=\"nearest\") # Only plots two datapoints.\n",
    "\n",
    "time_series = data_drought.sel(lat=25, lon=0.0, method=\"nearest\")\n",
    "time_series[\"pzero\"].plot.line() \n",
    "time_series = data_drought.sel(time=\"2023-01-01\",method=\"nearest\")\n",
    "global_mean = time_series[\"SPEI1\"].mean() # Global mean has been successfully computed, but it's still a lazy Dask array\n",
    "print(global_mean.compute()) # Triggers to print computation.\n",
    "time_series[\"SPEI1\"].plot.pcolormesh() # Why did I need to squeeze the dimension here? -> because there can be multiple measurements in a day.\n",
    "\n",
    "# Yearly average at each pixel.\n",
    "global_mean = data_drought[\"SPEI12\"].mean(\"time\") # Calculates mean along time axis.\n",
    "fig = ekp.Figure(rows=1, columns=1, size=(5,5)) # Create the figure with dimensions.\n",
    "subplot = fig.add_map(domain=\"Europe\") # Put in a panel (add_map).\n",
    "subplot.grid_cells(global_mean, z=\"SPEI12\") # Same as pcolormesh. Dataset, Variable.\n",
    "subplot.legend(location=\"right\")\n",
    "    \n",
    "fig.land() \n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Calculating SPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Read in total precipitation data (monthly) from ERA5 analysis. \n",
    "##### (https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-timeseries?tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 9.25\n",
    "lng = 40.5 # Ethiopia\n",
    "variable = \"total_precipitation\"\n",
    "date_range = [\"1940-01-01T06:00:00.000000000\", \"2020-12-31T06:00:00.000000000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 12:17:02,878 WARNING [2025-03-17T00:00:00] Please be aware that the generation of this dataset is using an alternative source for the ERA5 data and may be subject to changes over time (e.g. file format, data file structure, deprecation etc). This dataset should therefore be regarded as â€œexperimentalâ€ and is **not recommended for use in a production environment**. \n",
      "\n",
      "Notification of changes via this catalogue entry banner and/or in the [Forum](https://forum.ecmwf.int/) will be provided on best efforts.\n",
      "2025-11-19 12:17:02,881 INFO Request ID is cefd6aa0-fb73-4605-8cad-5becc66cb0e7\n",
      "2025-11-19 12:17:02,963 INFO status has been updated to accepted\n",
      "2025-11-19 12:17:16,737 INFO status has been updated to running\n",
      "2025-11-19 12:17:24,404 INFO status has been updated to successful\n",
      "                                                                                                                            "
     ]
    }
   ],
   "source": [
    "def retrieve_tp_data(variable, date_range, lat, lng):\n",
    "    # Define the dataset and request parameters\n",
    "    dataset = \"reanalysis-era5-single-levels-timeseries\"\n",
    "    request = {\n",
    "        \"variable\": [\n",
    "        variable,  # Variable to retrieve\n",
    "        ],\n",
    "        \"date\": date_range,  # Date range for the data\n",
    "        \"location\": {\"longitude\": lng, \"latitude\": lat},  # Location coordinates\n",
    "        \"data_format\": \"netcdf\"  # Format of the retrieved data\n",
    "    }\n",
    "\n",
    "    # Use \"earthkit\" to retrieve the data\n",
    "    ekds = earthkit.data.from_source(\n",
    "        \"cds\", dataset, request\n",
    "    ).to_xarray()\n",
    "\n",
    "    return ekds\n",
    "    \n",
    "data = retrieve_tp_data(variable, date_range, lat, lng)\n",
    "data_shifted = data.assign_coords(valid_time=data.valid_time - np.timedelta64(6, 'h'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to compute the monthly precipitation climatology\n",
    "def precipMonthly(data):\n",
    "    \"\"\"\n",
    "    Calculate the monthly climatology of precipitation.\n",
    "\n",
    "    This function reads precipitation data from a NetCDF file,\n",
    "    converts the time coordinate to a pandas datetime index,\n",
    "    and then resamples the data to calculate the monthly \n",
    "    climatology. The resulting climatology is returned in millimeters.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the monthly climatology\n",
    "        of precipitation in millimeters, indexed by month.\n",
    "    \"\"\"\n",
    "\n",
    "    data_tp_pt = data.tp # Accessing total precipitation.\n",
    "\n",
    "    # Convert the time coordinate to a pandas datetime index\n",
    "    time_index = pd.to_datetime(data_tp_pt.valid_time.values)\n",
    "\n",
    "    # Create a DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(data_tp_pt.values*1000, index=time_index, columns=['tp (mm)'])\n",
    "    df_monthly = df.resample('MS').sum() # .gives monthly totals by summing precipitation within each month.\n",
    "    # df_monthly_shifted = df.resample('MS').apply(lambda x: (x.index[0], x.index[-1]))\n",
    "    \n",
    "    df_monthly['month'] = df_monthly.index.month # Extracts the month number (1â€“12) from the index and adds it as a column.\n",
    "    monthly_climatology = df_monthly.groupby('month').mean()\n",
    "\n",
    "    # Get the actual lat/lon used\n",
    "    nearest_lat = data_tp_pt.latitude.values\n",
    "    nearest_lng = data_tp_pt.longitude.values\n",
    "\n",
    "    return df_monthly, monthly_climatology, nearest_lat, nearest_lng\n",
    "\n",
    "# Call our function\n",
    "df_monthly, clim, nearest_lat, nearest_lng = precipMonthly(data)\n",
    "df_monthly_shifted, clim_shifted, nearest_lat_shifted, nearest_lng_shifted = precipMonthly(data_shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculate moving average for different accumulation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid')\n",
    "\n",
    "# Does xarray have a moving window.\n",
    "\n",
    "def moving_window(acc_periods,df_monthly):\n",
    "    for period in acc_periods:\n",
    "        x = df_monthly['tp (mm)'].values\n",
    "        conv_result = moving_average(x,period)  # rolling accumulation window. \n",
    "        \n",
    "        # Create an array of full length with NaNs\n",
    "        aligned = np.full(len(df_monthly), np.nan)\n",
    "        \n",
    "        # Place convolution result starting at index period-1\n",
    "        aligned[period-1:] = conv_result\n",
    "        \n",
    "        df_monthly[f\"Accumulation-{period} months\"] = aligned\n",
    "    return df_monthly\n",
    "        \n",
    "df_monthly = moving_window(acc_periods,df_monthly)\n",
    "df_monthly_shifted = moving_window(acc_periods,df_monthly_shifted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of accumulation periods (1 to 48) months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for period in acc_periods:\n",
    "    plt.plot(df_monthly.index, df_monthly_shifted[f\"Accumulation-{period} months\"], label=f\"{period}-month\")\n",
    "plt.title(\"Accumulation Period Comparison\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Precipitation (mm)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Functions for fitting gamma distribution to different accumulation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gamma_distributions(df, accum_periods, start_ref, end_ref):\n",
    "    # Function to fit gamma distribution on any precipitation dataframe.\n",
    "    gamma_params = {}\n",
    "    for period in accum_periods:\n",
    "        reference_data = df.loc[start_ref:end_ref, f\"Accumulation-{period} months\"].dropna()\n",
    "        alpha, loc, beta = stats.gamma.fit(reference_data)\n",
    "        gamma_params[period] = (alpha, loc, beta)\n",
    "    return gamma_params # returns the gamma paramaters, associated with that period.\n",
    "\n",
    "def spi_norm_ppf(df, accum_periods):\n",
    "    spi_df  = pd.DataFrame(index=df.index)\n",
    "    for period in accum_periods:\n",
    "        data = df[f\"SPI-{period}\"]\n",
    "        # Convert to standard normal\n",
    "        spi = stats.norm.ppf(data, loc=0, scale=1)\n",
    "        spi_df[f\"SPI-{period}\"] = spi\n",
    "    return spi_df # returns SPI df series.\n",
    "\n",
    "def compute_spi_series(df, accum_periods, gamma_params):\n",
    "    spi_df = pd.DataFrame(index=df.index)\n",
    "    pdf_df = pd.DataFrame(index=df.index) \n",
    "    cdf_df = pd.DataFrame(index=df.index) \n",
    "\n",
    "    for period in accum_periods:\n",
    "        alpha, loc, beta = gamma_params[period]\n",
    "        data = df[f\"Accumulation-{period} months\"]\n",
    "        \n",
    "        pdf_values = stats.gamma.pdf(data, a=alpha, loc=loc, scale=beta)\n",
    "        cdf_values = stats.gamma.cdf(data, a=alpha, loc=loc, scale=beta)\n",
    "        spi_values = stats.norm.ppf(stats.gamma.cdf(data, a=alpha, loc=loc, scale=beta),loc=0, scale=1)\n",
    "        \n",
    "        spi_df[f\"SPI-{period}\"] = spi_values\n",
    "        pdf_df[f\"SPI-{period}\"] = pdf_values\n",
    "        cdf_df[f\"SPI-{period}\"] = cdf_values\n",
    "    return spi_df, pdf_df, cdf_df # returns SPI times series.\n",
    "\n",
    "def compute_spi(value, acc_period):\n",
    "    # Function to compute SPI for any tp value in any period.\n",
    "    alpha, loc, beta = gamma_params[acc_period]\n",
    "    # Compute CDF under gamma\n",
    "    cdf = stats.gamma.cdf(value, a=alpha, loc=loc, scale=beta)\n",
    "    # Convert to standard normal\n",
    "    spi = stats.norm.ppf(cdf, loc=0, scale=1)\n",
    "    return spi # returns SPI single value.\n",
    "\n",
    "def get_spi_for_month(spi_df, date, acc_period):\n",
    "    # Function to return SPI for any month in any accumulation period.\n",
    "    return spi_df.loc[date, f\"SPI-{acc_period}\"]\n",
    "\n",
    "def plot_spi_series(spi_series, accum_periods):\n",
    "    # Function to plot SPI series.\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for period in accum_periods:\n",
    "        plt.plot(spi_series.index, spi_series[f\"SPI-{period}\"], label=f\"SPI-{period}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Standardized Precipitation Index\")\n",
    "    plt.ylim([-10, 10])\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"SPI Value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def compute_spi_tp(spi_df,accum_periods, gamma_params):\n",
    "    # Function to compute the SPI vs tp.\n",
    "    spi_results = {}\n",
    "    pdf_spi_values = {}\n",
    "    cdf_spi_values = {}\n",
    "    \n",
    "    for period in accum_periods:\n",
    "        col_name = f\"SPI-{period}\"\n",
    "        data = spi_df[col_name]\n",
    "        x = np.linspace(0, 5000, 400)\n",
    "        alpha, loc, beta = gamma_params[period]\n",
    "        \n",
    "        pdf_spi_values[period] = stats.gamma.pdf(x, a=alpha, loc=loc, scale=beta)\n",
    "        cdf_spi_values[period] = stats.gamma.cdf(x, a=alpha, loc=loc, scale=beta)\n",
    "        spi_values = stats.norm.ppf(stats.gamma.cdf(x, a=alpha, loc=loc, scale=beta),loc=0, scale=1)\n",
    "        \n",
    "        spi_results[period] = (spi_values) \n",
    "        \n",
    "    return spi_results, pdf_spi_values, cdf_spi_values # returns SPI results as dictionary with period.\n",
    "\n",
    "def plot_spi_tp(spi_tp, accum_periods):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for period in accum_periods:\n",
    "        plt.plot( spi_tp[period], label=f\"SPI-{period}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Standardized Precipitation Index\")\n",
    "    plt.xlabel(\"Total Precipitation (mm)\")\n",
    "    plt.ylabel(\"SPI Value\")\n",
    "    plt.ylim([-10,10])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Functions to fit gamma distribution (over calendar months) to different accumulation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_monthly_gamma_distributions(df, accum_periods, start_ref, end_ref):\n",
    "    # Function to fit gamma distribution on any precipitation dataframe.\n",
    "    gamma_monthly_params = {}\n",
    "    months = range(1,13)\n",
    "    for period in accum_periods:\n",
    "        for month in months:\n",
    "            reference_data = df.loc[start_ref:end_ref, f\"Accumulation-{period} months\"].dropna()\n",
    "            reference_data = reference_data[reference_data.index.month==month]\n",
    "            alpha, loc, beta = stats.gamma.fit(reference_data)\n",
    "            gamma_monthly_params[month,period] = (alpha, loc, beta)\n",
    "    return gamma_monthly_params # returns the gamma paramaters, associated with that period.\n",
    "\n",
    "def compute_monthly_spi_series(df, accum_periods, gamma_monthly_params, months = range(1,13)): \n",
    "    # Function to compute SPI time series from dataframe, period and gamma parameters.\n",
    "    spi_df = pd.DataFrame(index=df.index)\n",
    "    pdf_df = pd.DataFrame(index=df.index) \n",
    "    cdf_df = pd.DataFrame(index=df.index) \n",
    "\n",
    "    for period in accum_periods:\n",
    "        data = df[f\"Accumulation-{period} months\"]\n",
    "        for month in months:\n",
    "            alpha, loc, beta = gamma_monthly_params[(month, period)]\n",
    "            pdf_values = stats.gamma.pdf(data[data.index.month==month], a=alpha, loc=loc, scale=beta)\n",
    "            cdf_values = stats.gamma.cdf(data[data.index.month==month], a=alpha, loc=loc, scale=beta)\n",
    "            spi_values = stats.norm.ppf(stats.gamma.cdf(data[data.index.month==month], a=alpha, loc=loc, scale=beta),loc=0, scale=1)\n",
    "            spi_df.loc[spi_df.index.month==month, f\"SPI-{period}\" ] = spi_values\n",
    "            pdf_df.loc[spi_df.index.month==month, f\"SPI-{period}\" ] = pdf_values\n",
    "            cdf_df.loc[spi_df.index.month==month, f\"SPI-{period}\" ] = cdf_values\n",
    "    return spi_df, pdf_df, cdf_df # returns SPI times series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fit gamma distribution (over calendar months) to different accumulation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [1,3,6,12,24,36,48]\n",
    "start_ref, end_ref  = \"1991-01-01\", \"2020-12-01\"\n",
    "\n",
    "# 1. Fit gamma distributions (ref dataset)\n",
    "gamma_params = fit_gamma_distributions(df_monthly, accum_periods, start_ref, end_ref)\n",
    "gamma_params_shifted = fit_gamma_distributions(df_monthly_shifted, accum_periods, start_ref, end_ref)\n",
    "\n",
    "# 1.1 Fit gamma distributions (ref dataset, calendar months)\n",
    "gamma_monthly_params = fit_monthly_gamma_distributions(df_monthly, accum_periods,start_ref,end_ref)\n",
    "gamma_monthly_params_shifted = fit_monthly_gamma_distributions(df_monthly_shifted, accum_periods,start_ref,end_ref)\n",
    "\n",
    "# 2. Compute SPI DataFrame\n",
    "spi_df, pdf_spi_series, cdf_spi_series = compute_spi_series(df_monthly, accum_periods, gamma_params)\n",
    "spi_df_shifted, pdf_spi_series_shifted, cdf_spi_series_shifted = compute_spi_series(df_monthly_shifted, accum_periods, gamma_params_shifted)\n",
    "\n",
    "# 2.1 Compute SPI DataFrame (calendar monthly)\n",
    "spi_df_monthly, pdf_spi_series_monthly, cdf_spi_series_monthly = compute_monthly_spi_series(df_monthly, accum_periods, gamma_monthly_params)\n",
    "spi_df_shifted_monthly, pdf_spi_series_shifted_monthly, cdf_spi_series_shifted_monthly = compute_monthly_spi_series(df_monthly_shifted, accum_periods, gamma_monthly_params)\n",
    "\n",
    "# # 2.1 Compute SPI vs tp curve\n",
    "# spi_tp, pdf_spi_tp, cdf_spi_tp = compute_spi_tp(spi_df, accum_periods, gamma_params)\n",
    "# spi_tp_shifted, pdf_spi_tp_shifted, cdf_spi_tp_shifted = compute_spi_tp(spi_df_shifted, accum_periods, gamma_params_shifted)\n",
    "\n",
    "# # 3. Plot SPI time series\n",
    "# plot_spi_series(spi_df, accum_periods)\n",
    "# plot_spi_series(spi_df_shifted, accum_periods)\n",
    "\n",
    "# # 4. Plot SPI precipitation curve\n",
    "# plot_spi_tp(spi_tp, accum_periods)\n",
    "\n",
    "# # 5. Query SPI for a specific month\n",
    "# spi_value = get_spi_for_month(spi_df, \"2003-07-01\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculating historical ratio of months without precipitation and finding zero adjusted SPI (entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "def zero_precip(df, cdf_spi_series, accum_periods):\n",
    "    # Initialize DataFrame for adjusted CDF\n",
    "    cdf_adjusted_series = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Compute zero-precip stats and adjusted CDF in one loop\n",
    "    stats_summary = {}\n",
    "    month_counts = {m: {f\"SPI-{p}\": 0 for p in accum_periods} for m in range(1, 13)}  # month: {SPI-period: count}\n",
    "    \n",
    "    df_ref = df[(df.index >= \"1991-01-01\") & (df.index <= \"2020-12-01\")]\n",
    "    \n",
    "    for period in accum_periods:\n",
    "        col = f\"Accumulation-{period} months\"\n",
    "        spi_col = f\"SPI-{period}\"\n",
    "        \n",
    "        # Calculate zero-precip count and total months\n",
    "        n_zero = (df_ref[col] < 0.1).sum()\n",
    "        n_month = df_ref[col].count()\n",
    "            \n",
    "        # Probability of zero precipitation\n",
    "        p_zero = (n_zero+1) / (2*(n_month + 1))      \n",
    "        \n",
    "        # Adjusted CDF\n",
    "        cdf_adjusted_series[spi_col] = p_zero + (1 - p_zero) * cdf_spi_series[spi_col]\n",
    "        \n",
    "        # Store summary stats\n",
    "        stats_summary[spi_col] = {\n",
    "            \"Zero-Precip Count\": int(n_zero),\n",
    "            \"Total Months\": int(n_month),\n",
    "            \"Prob Zero Precip\": p_zero\n",
    "        }\n",
    "    \n",
    "        # Count per month for values < 0.1\n",
    "        for idx, val in df_ref[col].items():\n",
    "            if val < 0.1:\n",
    "                month_counts[idx.month][spi_col] += 1\n",
    "    \n",
    "    \n",
    "    # Convert month_counts to a DataFrame for readability\n",
    "    month_counts_df = pd.DataFrame(month_counts).T\n",
    "    \n",
    "    month_counts_df.index = [\n",
    "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
    "    ]\n",
    "    \n",
    "    stats_summary_df = pd.DataFrame(stats_summary).T\n",
    "    \n",
    "    # Convert summary to DataFrame for easy viewing\n",
    "    zero_prep_stats_df = pd.DataFrame(month_counts_df)\n",
    "\n",
    "    return stats_summary_df, zero_prep_stats_df, cdf_adjusted_series\n",
    "    \n",
    "# spi_series_adjusted_old = spi_norm_ppf(cdf_adjusted_series, accum_periods) # inverse norm \n",
    "\n",
    "stats_df_shifted, zero_df_shifted, cdf_adjusted_series_shifted = zero_precip(df_monthly_shifted,cdf_spi_series_shifted,accum_periods)\n",
    "\n",
    "spi_series_adjusted_shifted = spi_norm_ppf(cdf_adjusted_series_shifted, accum_periods) # inverse norm\n",
    "\n",
    "# # plot_spi_series(spi_series_adjusted, accum_periods) # plot adjusted \n",
    "# plot_spi_series(spi_series_monthly_adjusted_shifted, accum_periods) # plot non adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculating historical ratio of months without precipitation and finding zero adjusted SPI (ref dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "def zero_precip_monthly(df, cdf_spi_series, accum_periods, months= range(1,13)):\n",
    "\n",
    "    # Copy original CDF series\n",
    "    cdf_adjusted_series = cdf_spi_series.copy()\n",
    "    \n",
    "    # List for summary stats\n",
    "    stats_summary = []\n",
    "\n",
    "    df_ref = df[(df.index >= \"1991-01-01\") & (df.index <= \"2020-12-01\")]\n",
    "    \n",
    "    for period in accum_periods:\n",
    "        col = f\"Accumulation-{period} months\"\n",
    "        spi_col = f\"SPI-{period}\"\n",
    "\n",
    "        for month in months:\n",
    "            monthly_data = df_ref[df_ref.index.month==month]\n",
    "\n",
    "            # Calculate zero-precip count and total months\n",
    "            n_zero = (monthly_data[col] < 0.1).sum()\n",
    "            n_month = monthly_data[col].count()\n",
    "                \n",
    "            # Probability of zero precipitation\n",
    "            p_zero = (n_zero+1) / (2*(n_month + 1))      \n",
    "            ratio_zero = n_zero / (n_month)\n",
    "\n",
    "            # Adjust CDF safely\n",
    "            mask = cdf_adjusted_series.index.month == month\n",
    "            cdf_adjusted_series.loc[mask, spi_col] = p_zero + (1 - p_zero) * cdf_spi_series.loc[mask, spi_col]\n",
    "\n",
    "            # Append summary\n",
    "            stats_summary.append({\n",
    "                \"Month\": int(month),\n",
    "                \"SPI\": int(period),\n",
    "                \"Zero-Precip Count\": int(n_zero),\n",
    "                \"Total Months\": int(n_month),\n",
    "                \"Prob Zero Precip\": p_zero,\n",
    "                \"Historical Ratio\": ratio_zero,\n",
    "            })\n",
    "    \n",
    "    stats_summary_df = pd.DataFrame(stats_summary)\n",
    "\n",
    "    return stats_summary_df, cdf_adjusted_series\n",
    "    \n",
    "# spi_series_adjusted_old = spi_norm_ppf(cdf_adjusted_series, accum_periods) # inverse norm \n",
    "\n",
    "# Without shifting\n",
    "stats_df_monthly, cdf_monthly_adjusted_series = zero_precip_monthly(df_monthly,cdf_spi_series_monthly,accum_periods) \n",
    "\n",
    "# With shifting\n",
    "stats_df_monthly_shifted, cdf_monthly_adjusted_series_shifted = zero_precip_monthly(df_monthly_shifted,cdf_spi_series_shifted_monthly,accum_periods) \n",
    "\n",
    "# Readjusted SPI for without shifting\n",
    "spi_series_monthly_adjusted = spi_norm_ppf(cdf_monthly_adjusted_series, accum_periods) # inverse norm\n",
    "\n",
    "# Readjusted SPI for with shifting\n",
    "spi_series_monthly_shifted_adjusted = spi_norm_ppf(cdf_monthly_adjusted_series_shifted, accum_periods) # inverse norm\n",
    "\n",
    "# Looking at the zero precip data.\n",
    "stats_df_monthly_shifted.loc[stats_df_monthly_shifted[\"SPI\"] == 1]\n",
    "\n",
    "#Plot \n",
    "\n",
    "# plot_spi_series(spi_series_adjusted, accum_periods) # plot adjusted \n",
    "# plot_spi_series(spi_series_monthly_adjusted_shifted, accum_periods) # plot non adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ERA5- Probability of zero precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "request = {\n",
    "    \"variable\": [\"probability_of_zero_precipitation_spi\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_zero_precip = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "era5_zero_precip = era5_zero_precip.to_xarray(compat=\"equals\") # Converts to xarray.\n",
    "era5_zero_precip.sel(lat=9.25, lon= 40.5).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### SPI Test of Normality: Shapiro-Wilks Test\n",
    "##### Quality parameter set to:\n",
    "##### 0 (rejection) - if p-value < alpha = 0.05, \n",
    "##### 1 (accepted) - if p-value > alpha = 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Shapiro-Wilks Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapiro_monthly_test(spi_df, accum_periods, months = range(1,13)):\n",
    "    results = []\n",
    "    # spi_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    spi_ref = spi_df[(spi_df.index >= \"1991-01-01\") & (spi_df.index <= \"2020-12-01\")]\n",
    "    for period in accum_periods:\n",
    "        data = spi_ref[f\"SPI-{period}\"]\n",
    "        for month in months:\n",
    "            month_data = data[data.index.month == month].dropna()\n",
    "            # month_data = month_data[np.isfinite(month_data)]  # remove inf/-inf\n",
    "\n",
    "            stat, pval = stats.shapiro(month_data,nan_policy=\"omit\")\n",
    "            normality = 0 if pval < 0.05 else 1\n",
    "\n",
    "            results.append((month, period, stat, pval,normality))\n",
    "        \n",
    "    # Convert to DataFrame\n",
    "    df_shapiro_results = pd.DataFrame(results, columns=[\"Month\", \"SPI\", \"Statistic\", \"P-Value\",\"Normality\"])\n",
    "\n",
    "    return df_shapiro_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculated Shapiro-Wilks Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shapiro_results = shapiro_monthly_test(spi_series_monthly_adjusted,accum_periods)\n",
    "df_shapiro_results= df_shapiro_results.loc[df_shapiro_results[\"SPI\"]==24]\n",
    "df_shapiro_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ERA5 Shapiro-Wilks Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "request = {\n",
    "    \"variable\": [\"test_for_normality_spi\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "quality_spi = quality_spi.to_xarray(compat=\"equals\") # Converts to xarray.\n",
    "quality_spi=quality_spi.sel(lat=9.25,lon=40.5, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Caclulated vs ERA5 Shapiro-Wilks Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi[\"significance\"].compute()\n",
    "df_shapiro_results\n",
    "# quality_spi[\"significance\"].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Check calculated SPI against given (one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "\n",
    "request1 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75]\n",
    "}\n",
    "\n",
    "request2 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1981\", \"1982\", \"1983\",\n",
    "        \"1984\", \"1985\", \"1986\",\n",
    "        \"1987\", \"1988\", \"1989\",\n",
    "        \"1990\", \"1991\", \"1992\",\n",
    "        \"1993\", \"1994\", \"1995\",\n",
    "        \"1996\", \"1997\", \"1998\",\n",
    "        \"1999\", \"2000\", \"2001\",\n",
    "        \"2002\", \"2003\", \"2004\",\n",
    "        \"2005\", \"2006\", \"2007\",\n",
    "        \"2008\", \"2009\", \"2010\",\n",
    "        \"2011\", \"2012\", \"2013\",\n",
    "        \"2014\", \"2015\", \"2016\",\n",
    "        \"2017\", \"2018\", \"2019\",\n",
    "        \"2020\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75] # Ethiopia\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drought = ekd.from_source(\"cds\", dataset, request1,request2) # Sends request for this dataset to CDS.\n",
    "data_drought = data_drought.to_xarray(compat=\"equals\") # Converts to xarray.\n",
    "data_drought = data_drought.sel(lat=9.25,lon=40.5, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculated_spi_to_xarray(df):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame of SPI values into an xarray Dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame with columns like SPI-1, SPI-3, ..., SPI-48\n",
    "                               and datetime index.\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: Dataset with dimensions (time) and variables for each SPI period.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary of variables for xarray\n",
    "    data_vars = {}\n",
    "    for col in df.columns:\n",
    "        var_name = col.replace(\"-\",\"\") \n",
    "        data_vars[var_name] = ([\"time\"], df[col].values)\n",
    "\n",
    "    # Build the Dataset\n",
    "    ds = xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords={\"time\": df.index}\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Add 6 hours to all time coordinates otherwise they don't match!\n",
    "    ds['time'] = ds['time'] + np.timedelta64(6, 'h')\n",
    "\n",
    "    return ds\n",
    "\n",
    "spi_calculated_shifted = calculated_spi_to_xarray(spi_series_adjusted_shifted)\n",
    "spi_calculated_monthly_shifted = calculated_spi_to_xarray(spi_series_monthly_adjusted_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spi_calculated, data_drought = xr.align(spi_calculated, data_drought, join=\"inner\")  # keep only matching times\n",
    "# result = (spi_calculated - data_drought).compute()\n",
    "# data_drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "accum_periods=[12]\n",
    "for period in accum_periods:\n",
    "    # shifted_diff = spi_calculated_monthly_shifted[f\"SPI{period}\"]-data_drought[f\"SPI{period}\"].dropna(dim='time')\n",
    "    diff = spi_calculated_monthly_shifted[f\"SPI{period}\"]-data_drought[f\"SPI{period}\"].dropna(dim='time')\n",
    "\n",
    "    median_diff_no_shift = np.median(diff)\n",
    "    median_abs_diff_no_shift = np.median(np.abs(diff))\n",
    "    \n",
    "    median_diff_shifted = np.median(shifted_diff)\n",
    "    median_abs_diff_shifted = np.median(np.abs(shifted_diff))\n",
    "\n",
    "    plt.plot(spi_calculated_monthly_shifted[f\"SPI{period}\"], label=f\"Calculated-SPI{period}\")\n",
    "    plt.plot(data_drought[f\"SPI{period}\"], label=f\"ERA5_Drought-SPI{period}\")\n",
    "    \n",
    "    # plt.plot(diff, label=f\"No time shifting Calculated-SPI{period}\")\n",
    "    # plt.plot(shifted_diff, label=f\"6-hr time shifted Calculated-SPI{period}\")\n",
    "    \n",
    "    # Add median difference lines\n",
    "    plt.axhline(median_diff_no_shift, color='blue', linestyle='--', label=f\"Median Diff No Shift: {median_diff_no_shift:.5f}\")\n",
    "    plt.axhline(median_diff_shifted, color='orange', linestyle='--', label=f\"Median Diff Shifted: {median_diff_shifted:.5f}\")\n",
    "\n",
    "    # Add median absolute difference lines (optional, different style)\n",
    "    plt.axhline(median_abs_diff_no_shift, color='blue', linestyle=':', label=f\"Median Abs Diff No Shift: {median_abs_diff_no_shift:.5f}\")\n",
    "    plt.axhline(median_abs_diff_shifted, color='orange', linestyle=':', label=f\"Median Abs Diff Shifted: {median_abs_diff_shifted:.5f}\")\n",
    "\n",
    "    # plt.plot(data_drought[f\"SPI{period}\"], label=f\"ERA5 Drought-SPI{period}\")\n",
    "\n",
    "    # plt.plot(spi_calculated[f\"SPI{period}\"],  label=f\"Calculated-SPI{period}\")\n",
    "    # plt.plot(data_drought[f\"SPI{period}\"], label=f\"ECMWF-SPI{period}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Standardised Precipitation Index Difference in SPI12\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"SPI Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Calculating SPI- Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "request = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\"48\"],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"ensemble_members\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"2023\",\n",
    "        \"2024\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drought_ens = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "data_drought_ens = data_drought_ens.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_drought_ens = data_drought_ens.sel(lat=9.25,lon=40.5, method=\"nearest\")\n",
    "data_drought_ens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Function to create the data into ensemble set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_457/4268696212.py:8: FutureWarning: ``output_sizes`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.\n",
      "  ensemble = xr.apply_ufunc(\n"
     ]
    }
   ],
   "source": [
    "# Function to reshape time into (time=24, ensemble=10)\n",
    "def make_ensemble(spi):\n",
    "    arr = np.asarray(spi)  # Convert memoryview or dask chunk to NumPy\n",
    "    reshaped = arr.reshape(24, 10)\n",
    "    return reshaped\n",
    " \n",
    "# Apply across all lat/lon\n",
    "ensemble = xr.apply_ufunc(\n",
    "    make_ensemble,\n",
    "    data_drought_ens[\"SPI48\"],  # DataArray with dims (time, lat, lon)\n",
    "    input_core_dims=[[\"time\"]],\n",
    "    output_core_dims=[[\"time\", \"ensemble\"]],\n",
    "    exclude_dims={\"time\"},\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    "    output_sizes={\"time\": 24, \"ensemble\": 10},\n",
    ")\n",
    " \n",
    "# Assign coordinates\n",
    "ensemble = ensemble.assign_coords({\n",
    "    \"time\": pd.date_range(\"2021-01-01\", periods=24, freq=\"MS\"),\n",
    "    \"ensemble\": range(1, 11)\n",
    "})\n",
    "ensemble.name = \"SPI48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean = ensemble.mean(dim=\"ensemble\", skipna=True)  # shape: (lat, lon, time)\n",
    "ensemble_std = ensemble.std(dim=\"ensemble\", skipna=True)  # shape: (lat, lon, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eg6/c3s2-eqc-quality-assessment/.venv/lib/python3.12/site-packages/dask/array/numpy_compat.py:58: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.divide(x1, x2, out)\n"
     ]
    }
   ],
   "source": [
    "ensemble_std = ensemble_std.compute()  # Load into memory for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean = ensemble_mean.compute()  # Load into memory for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one time slice and one ensemble\n",
    "from earthkit.plots.styles import Style\n",
    "SPI_STYLE = Style(cmap=plt.cm.RdBu, vmin=-3, vmax=3, normalize=False)\n",
    "SPI_SEOM_STYLE = Style(cmap=plt.cm.RdBu_r, vmin=0, vmax=1, normalize=False) # _r for reversing colorbar.\n",
    "\n",
    "global_std_map = ensemble_std.sel(time=\"2021-08-01\")  # shape (lat, lon)\n",
    "global_mean_map = ensemble_mean.sel(time=\"2021-08-01\")\n",
    "# Convert to NumPy arrays\n",
    "\n",
    "std_values = global_std_map.to_numpy()/np.sqrt(10)\n",
    "mean_values = global_mean_map.to_numpy()\n",
    "\n",
    "lat_values = global_map.lat.to_numpy()\n",
    "lon_values = global_map.lon.to_numpy()\n",
    " \n",
    "# Create meshgrid\n",
    "\n",
    "lon_grid, lat_grid = np.meshgrid(lon_values, lat_values)\n",
    " \n",
    "# # Plot with EarthKit\n",
    "\n",
    "# Create figure with 2 columns\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))  # <-- Important!\n",
    "\n",
    "# First subplot (left)\n",
    "subplot = fig.add_map(domain=\"Europe\", row=0, column=0)\n",
    "subplot.grid_cells(mean_values, x=lon_grid, y=lat_grid,style=SPI_STYLE)\n",
    "subplot.legend(location=\"right\")\n",
    "\n",
    "# Second subplot (right)\n",
    "subplot1 = fig.add_map(domain=\"Europe\", row=0, column=1)\n",
    "subplot1.grid_cells(std_values, x=lon_grid, y=lat_grid, style = SPI_SEOM_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "\n",
    "subplot.title(\"SPI at date 2021-08-01\")\n",
    "subplot1.title(\"Standard Uncertainty of the Mean (N=10)\")\n",
    "\n",
    "# Add decorations\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-3)=\n",
    "### 3. Section 3 title\n",
    "#### Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does (if code is included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cells\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-4)=\n",
    "### 4. Section 4 title\n",
    "\n",
    "#### Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does(if code is included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 3.749769,
     "end_time": "2024-03-08T17:24:00.248720",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.498951",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cell\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.245188,
     "end_time": "2024-03-08T17:39:21.277354",
     "exception": false,
     "start_time": "2024-03-08T17:39:21.032166",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-5)=\n",
    "### 5. Section 5 title \n",
    "\n",
    "#### Results Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does (if code is included). \n",
    "\n",
    "If this is the **results section**, we expect the final plots to be created here with a description of how to interpret them, and what information can be extracted for the specific use case and user question. The information in the 'quality assessment statement' should be derived here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 3.749769,
     "end_time": "2024-03-08T17:24:00.248720",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.498951",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cell\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.271597,
     "end_time": "2024-03-08T17:40:01.664067",
     "exception": false,
     "start_time": "2024-03-08T17:40:01.392470",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## â„¹ï¸ If you want to know more\n",
    "\n",
    "### Key resources\n",
    "\n",
    "List some key resources related to this assessment. E.g. CDS entries, applications, dataset documentation, external pages.\n",
    "Also list any code libraries used (if applicable).\n",
    "\n",
    "Code libraries used:\n",
    "* [C3S EQC custom functions](https://github.com/bopen/c3s-eqc-automatic-quality-control/tree/main/c3s_eqc_automatic_quality_control), `c3s_eqc_automatic_quality_control`,  prepared by [B-Open](https://www.bopen.eu/)\n",
    "\n",
    "### References\n",
    "\n",
    "List the references used in the Notebook here.\n",
    "\n",
    "E.g.\n",
    "\n",
    "[[1]](https://doi.org/10.1038/s41598-018-20628-2) Rodriguez, D., De Voil, P., Hudson, D., Brown, J. N., Hayman, P., Marrou, H., & Meinke, H. (2018). Predicting optimum crop designs using crop models and seasonal climate forecasts. Scientific reports, 8(1), 2231."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 968.520731,
   "end_time": "2024-03-08T17:40:03.783430",
   "environment_variables": {},
   "exception": null,
   "input_path": "D520.3.2.3b.SEASONAL_multimodel-bias_v5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T17:23:55.262699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
