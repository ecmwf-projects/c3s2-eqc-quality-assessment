{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Quality Assessment for ERA5 Drought Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Production date: 2026-xx-xx\n",
    "\n",
    "**Please note that this repository is used for development and review, so quality assessments should be considered work in progress until they are merged into the main branch.**\n",
    "\n",
    "Dataset version: 1.0.\n",
    "\n",
    "Produced by: Enis Gerxhalija, Olivier Burggraaff (National Physical Laboratory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸŒ Use case: Retrieving drought indicators from the ERA5-Drought dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## â“ Quality assessment question\n",
    "* **Are the drought indicators in the ERA5-Drought dataset consistent with and reproducible from ERA5 data?**\n",
    "* **Are the drought indicators in the ERA5-Drought dataset presented in a format that ensures optimal usability for users?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Human-induced climate change is likely the primary driver behind the number of increased droughts and heavy precipitation since the 1950s, per the latest assessment report by the Intergovernmental Panel on Climate Change [IPCC, 2013](https://www.ipcc.ch/report/ar6/syr/downloads/report/IPCC_AR6_SYR_LongerReport.pdf). With further global warming at 1.5Â°C and above, heavy precipitation, flooding and drought events are projected to intensify and become more frequent in most regions of Africa, Asia, North America and Europe. The environmental and societal impact of such extreme weather events are far-reaching. In the United Kingdom alone, the 2020s have seen three of the five worst harvests on record, with extreme heat and drought in 2025 causing more than Â£800mn lost revenue in harvest, [Energy & Climate Intelligence Unit](https://mcusercontent.com/8ed7ad7972fae058e8f4fb7e8/files/6d02e6e7-8639-a44d-5a8c-2313124ef699/Costs_of_climate_analysis_011225.pdf). In 2023-2024, the Amazon region in Brazil faced an 18-month drought considered the most severe since drought monitoring began in 1954. By November 2024, it left 720 health centres in drought-affected areas of Brazil to become non-operational [UNICEF, 2024](https://www.unicef.org/media/165191/file/LACR-Flash-Update-11-November-2024.pdf).\n",
    "\n",
    "A large scientific effort has gone into identifying areas more prone to drought along with monitoring areas currently experiencing drought conditions and accurately quantifying their severity [reference]. The objective quantification of drought severity remains an ongoing endeavour amongst scientists as there is not one physical variable that describes a drought. One might assume that drought severity can be measured by the total precipitation in that region, but this would overlook water-loss from the land surface through evapotranspiration, soil moisture levels, temperature anomalies and other natural variables. There is however consensus that drought indices, proxies based on long-term and shorter-term historical weather data, can accurately quantify drought severity and their impact, with studies linking the variability of drought indices to crop yields, [Vicenteâ€Serrano et al.](https://doi.org/10.1080/01431160500296032), and frequency of wildfires [reference]. Two widely-employed drought indices are the Standardised Precipitation Index (SPI) [reference](), endorsed by the World Metereological Organisation (WMO), and the more recent Standardised Precipitation-Evapotranspiration Index (SPEI) [reference](). \n",
    "\n",
    "The ERA5-Drought dataset provides a reanalysis-based dataset of the aforementioned indices using the ECMWF Reanalysis version 5 (ERA5), at a resolution of 0.25Â° globally (around 28 km) from the start of the reanalysis (in 1940) to today, [Keune et al., â€˜ERA5â€“Droughtâ€™](https://doi.org/10.1038/s41597-025-04896-y). The ERA5-Drought dataset consists of 1 deterministic and 10 ensemble drought-index members from slightly different initial conditions, enabling an estimate of the uncertainty.\n",
    "\n",
    "This notebook aims to give users much-needed confidence and transparency in the calculation of the two drought indices along with their quality flags. The C3S ERA5-Drought dataset must be consistent with and reproducible from its origins. Here, we assess this consistency and reproducibility by comparing drought-indicators retrieved from the ERA5-Drought dataset with their equivalents calculated from the origin dataset (or similar). While a full analysis and reproduction of every record within the C3S ERA5-Drought dataset is outside the scope of quality assessment (as it would require high-performance computing infrastructure), a case study with a narrower scope probes these quality attributes of the dataset and can be a jumping-off point for further analysis by the reader."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ“¢ Quality assessment statement\n",
    "\n",
    "```{admonition} These are the key outcomes of this assessment\n",
    ":class: note\n",
    "* Conclusion 1\n",
    "* Conclusion 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸ“‹ Methodology\n",
    "\n",
    "This quality assessment tests the consistency between drought indices retrieved from the [C3S ERA5-Drought dataset] and their equivalents calculated from the origin datasets, as well as the reproducibility and usability of said dataset.\n",
    "\n",
    "We will examine the SPI and SPEI drought indicators calculated from the following datasets:\n",
    "\n",
    "(include table here of the parameter, description of that parameters, and the origin dataset)\n",
    "\n",
    "The analysis and results are organised in the following steps, which are detailed in the sections below:\n",
    "\n",
    "**[](section-1)**\n",
    " * \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    " * Import all required libraries.\n",
    " * Define helper functions.\n",
    " * \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "**[](section-2)**\n",
    " * Define SPI Indicator\n",
    " * Download ERA5 precipitation\n",
    " * Accumulate\n",
    " * Calculate SPI\n",
    " * Calculate quality flags from ERA5 data\n",
    " * Download quality flags from ERA5-Drought\n",
    " * Compare quality flags\n",
    " * Download ERA5-Drought SPI\n",
    " * Comparison\n",
    " * \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "**[](section-3)**\n",
    " * Define SPEI Indicator\n",
    " * Download ERA5 potential evaporation\n",
    " * Accumulate\n",
    " * Calculate SPEI\n",
    " * Calculate quality flags from ERA5 data\n",
    " * Download quality flags from ERA5-Drought\n",
    " * Compare quality flags\n",
    " * Download ERA5-Drought SPEi\n",
    " * Comparison\n",
    " * \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "**[](section-4)**\n",
    " \n",
    "**[](section-5)** \n",
    "\n",
    "Any further notes on the method could go here (explanations, caveats or limitations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Analysis and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-1)=\n",
    "### 1. Code setup.\n",
    "\n",
    "```{note}\n",
    "This notebook uses [earthkit](https://github.com/ecmwf/earthkit) for downloading ([earthkit-data](https://github.com/ecmwf/earthkit-data)) and visualising ([earthkit-plots](https://github.com/ecmwf/earthkit-plots)) data. Because earthkit is in active development, some functionality may change after this notebook is published. If any part of the code stops functioning, please raise an issue on our GitHub repository so it can be fixed.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import required libraries\n",
    "\n",
    "In this section, we import all the relevant packages needed for running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Input / Output\n",
    "from pathlib import Path\n",
    "import earthkit.data as ekd\n",
    "import warnings\n",
    "\n",
    "# General data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "\n",
    "# Analysis\n",
    "import calendar\n",
    "from scipy import stats\n",
    "\n",
    "# Visualisation\n",
    "import earthkit.plots as ekp\n",
    "from earthkit.plots.styles import Style\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "from tqdm import tqdm  # Progress bars\n",
    "\n",
    "# Visualisation in Jupyter book -- automatically ignored otherwise\n",
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = None\n",
    "\n",
    "# Type hints\n",
    "from typing import Callable, Iterable, Optional\n",
    "from earthkit.plots.geo.domains import Domain\n",
    "AnyDomain = (Domain | str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "\n",
    "This section defines some functions and variables used in the following analysis, allowing code cells in later sections to be shorter and ensuring consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "##### Accumulation periods\n",
    "The following cells ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants such as the accumulation periods to use\n",
    "ACCUMULATION_PERIODS = [1, 3, 6, 12, 24, 36, 48]  # Months\n",
    "\n",
    "# Perform accumulation\n",
    "def accum_var(data: xr.Dataset, var: str, *,\n",
    "              accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS, time_dim: Optional[str]=None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Compute the precipitation / potential evaporation accumulation window. \n",
    "    \n",
    "    1. Convert precipitation/potential evaporation from meters to millimeters.\n",
    "    2. Compute monthly totals (accounting for days in month).\n",
    "    3. Add rolling accumulation windows on monthly totals.\n",
    "    \"\"\"\n",
    "    # Detect time dimension if not provided\n",
    "    # TO DO: Use the first dimension that contains \"time\"?\n",
    "    if time_dim is None:\n",
    "        if 'valid_time' in data.dims:\n",
    "            time_dim = 'valid_time'\n",
    "        elif 'forecast_reference_time' in data.dims:\n",
    "            time_dim = 'forecast_reference_time'\n",
    "        elif \"time\" in data.dims:\n",
    "            time_dim = \"time\"\n",
    "        else:\n",
    "            raise ValueError(\"No valid time dimension found. Expected 'valid_time' or 'forecast_reference_time'.\")\n",
    "\n",
    "    # Ensure time is sorted\n",
    "    # TO DO: Is this necessary?\n",
    "    data = data.sortby(time_dim)\n",
    "    \n",
    "    # Step 1: Convert to mm\n",
    "    data[f'{var}_mm'] = data[var] * 1000\n",
    "\n",
    "    # Step 2: Compute monthly totals\n",
    "    # TO DO: Is this necessary for the analysis (I don't think so) or only for the plot?\n",
    "    # In the latter case, you could (don't need to) either make this optional or split it out altogether\n",
    "    time_index = pd.to_datetime(data[time_dim].values)\n",
    "    \n",
    "    days_in_month = xr.DataArray(\n",
    "        time_index.days_in_month,\n",
    "        coords={time_dim: data[time_dim]},\n",
    "        dims=[time_dim]\n",
    "    )\n",
    "\n",
    "    data[f'{var}_mm_monthly_total'] = data[f'{var}_mm'] * days_in_month\n",
    "\n",
    "    # Step 3: Add rolling accumulation windows\n",
    "    for period in accumulation_periods:\n",
    "        rolling_sum = data[f\"{var}_mm_monthly_total\"].rolling({time_dim: period}, center=False).sum()\n",
    "        data[f'{var}_mm_accum_{period}m'] = rolling_sum\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### Helper functions for processing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_monthly_distributions_xr(reference_data: xr.Dataset, var: str, *,\n",
    "                                 accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS, time_dim: Optional[str]=None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Fit gamma/genlogistic distributions for each month and accumulation period using xarray.\n",
    "    Data are assumed to have been sliced to the reference period.\n",
    "    \"\"\"\n",
    "    monthly_params = {}\n",
    "\n",
    "    # Detect time dimension if not provided\n",
    "    # TO DO: Use the first dimension that contains \"time\"?\n",
    "    # TO DO: Refactor so can be re-used\n",
    "    if time_dim is None:\n",
    "        if 'valid_time' in reference_data.dims:\n",
    "            time_dim = 'valid_time'\n",
    "        elif 'forecast_reference_time' in reference_data.dims:\n",
    "            time_dim = 'forecast_reference_time'\n",
    "        else:\n",
    "            raise ValueError(\"No valid time dimension found. Expected 'valid_time' or 'forecast_reference_time'.\")\n",
    "\n",
    "    for period in accumulation_periods:\n",
    "        var_name = f'{var}_mm_accum_{period}m'\n",
    "        for month in range(1, 13):\n",
    "            # Select month subset\n",
    "            month_subset = reference_data[var_name].where(reference_data[time_dim].dt.month == month, drop=True)\n",
    "\n",
    "            # Drop NaNs and flatten\n",
    "            values = month_subset.values.flatten()\n",
    "            values = values[~np.isnan(values)]\n",
    "            if len(values) > 0:\n",
    "                # TO DO: Make the fitting distribution an argument to the function,\n",
    "                # then use functools.partial to create defaults\n",
    "                # e.g. function call: (..., var: str, distribution: Callable, ...)\n",
    "                # and the part below becomes alpha, loc, beta = distribution.fit(values)\n",
    "                # and then define\n",
    "                # fit_monthly_spi  = partial(fit_monthly_distributions_xr, distribution=stats.gamma)\n",
    "                # fit_monthly_spei = partial(fit_monthly_distributions_xr, distribution=stats.genlogistic)\n",
    "                if var == \"tp\":\n",
    "                    alpha, loc, beta = stats.gamma.fit(values)\n",
    "                elif var == \"pev\" or \"wb\":\n",
    "                    alpha, loc, beta = stats.genlogistic.fit(values) # alpha is shape parameters, loc = location, beta is scale.\n",
    "\n",
    "            monthly_params[(month, period)] = (alpha, loc, beta) # TODO: put into a xarray.\n",
    "    return monthly_params\n",
    "\n",
    "def compute_monthly_series_xr(data, accum_periods, monthly_params, time_dim=None, var_dim = None):\n",
    "    \"\"\"\n",
    "    Compute SPI time series for each accumulation period using fitted gamma parameters.\n",
    "    Returns an xarray Dataset with SPI variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Detect time dimension if not provided\n",
    "    if time_dim is None:\n",
    "        if 'valid_time' in data.dims:\n",
    "            time_dim = 'valid_time'\n",
    "        elif 'forecast_reference_time' in data.dims:\n",
    "            time_dim = 'forecast_reference_time'\n",
    "        else:\n",
    "            raise ValueError(\"No valid time dimension found. Expected 'valid_time' or 'forecast_reference_time'.\")\n",
    "\n",
    "    \n",
    "    # Detect var dimension if not provided\n",
    "    if var_dim is None:\n",
    "        if 'tp' in data.data_vars:\n",
    "            var_dim = 'tp'\n",
    "        elif 'pev' in data.data_vars:\n",
    "            var_dim = 'pev'\n",
    "        else:\n",
    "            raise ValueError(\"No valid variable dimension found. Expected 'tp' or 'pev'.\")\n",
    "            \n",
    "    spi_vars = {}\n",
    "    cdf_vars = {}\n",
    "\n",
    "    for period in accum_periods:\n",
    "        var_name = f'{var_dim}_mm_accum_{period}m'\n",
    "\n",
    "        spi_array = xr.full_like(data[var_name], np.nan)\n",
    "        cdf_array = xr.full_like(data[var_name], np.nan)\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            alpha, loc, beta = monthly_params[(month, period)]\n",
    "\n",
    "            # Select month subset\n",
    "            months = data[time_dim].dt.month\n",
    "            month_mask = (months == month)\n",
    "\n",
    "            values = data[var_name].where(month_mask)\n",
    "\n",
    "            # Compute CDF and SPI\n",
    "            if var_dim == \"tp\":\n",
    "                cdf = stats.gamma.cdf(values, a=alpha, loc=loc, scale=beta)\n",
    "                spi = stats.norm.ppf(cdf, loc=0, scale=1)\n",
    "            elif var_dim in (\"pev\", \"wb\"):\n",
    "                cdf = stats.genlogistic.cdf(values, c=alpha, loc=loc, scale=beta)\n",
    "                spi = stats.norm.ppf(cdf, loc=0, scale=1)\n",
    "            \n",
    "            # Assign values to arrays\n",
    "            spi_array = spi_array.where(~month_mask, spi)\n",
    "            cdf_array = cdf_array.where(~month_mask, cdf)\n",
    "\n",
    "        # Compute CDF and SPI\n",
    "        if var_dim == \"tp\":\n",
    "            spi_vars[f'SPI_{period}m'] = spi_array\n",
    "            cdf_vars[f'CDF_{period}m'] = cdf_array\n",
    "        elif var_dim in (\"pev\", \"wb\"):\n",
    "            spi_vars[f'SPEI_{period}m'] = spi_array\n",
    "            cdf_vars[f'CDF_{period}m'] = cdf_array\n",
    "\n",
    "    return xr.Dataset(spi_vars, coords=data.coords), xr.Dataset(cdf_vars, coords=data.coords)\n",
    "\n",
    "\n",
    "def compute_spi_dataset(data, accum_periods, n_points = 1000):\n",
    "    \"\"\"\n",
    "    Fit gamma distribution for each month and accumulation period,\n",
    "    then compute SPI, CDF, and PDF for a linspace of precipitation values.\n",
    "    \"\"\"\n",
    "    result_vars = {}\n",
    "\n",
    "    for period in accum_periods:\n",
    "        var_name = f'tp_mm_accum_{period}m'\n",
    "        \n",
    "        tp_values = data[var_name]\n",
    "\n",
    "        # Prepare storage lists\n",
    "        spi_list, cdf_list, pdf_list, precip_list = [], [], [], []\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            # Select month subset\n",
    "            month_data = tp_values.where(tp_values['valid_time.month'] == month, drop=True).values\n",
    "            month_data = month_data[month_data > 0]                \n",
    "            month_data = month_data[~np.isnan(month_data)]\n",
    "\n",
    "                \n",
    "            # Fit gamma distribution\n",
    "            alpha, loc, beta = stats.gamma.fit(month_data, floc=0)\n",
    "\n",
    "            vals = np.linspace(month_data.min(), month_data.max(), n_points)\n",
    "\n",
    "            # Compute PDF, CDF, SPI\n",
    "            cdf_vals = stats.gamma.cdf(vals, a=alpha, loc=loc, scale=beta)\n",
    "            spi_vals = stats.norm.ppf(cdf_vals)\n",
    "            pdf_vals = stats.gamma.pdf(vals, a=alpha, loc=loc, scale=beta)\n",
    "\n",
    "            # Append to lists\n",
    "            precip_list.append(vals)\n",
    "            spi_list.append(spi_vals)\n",
    "            cdf_list.append(cdf_vals)\n",
    "            pdf_list.append(pdf_vals)\n",
    "\n",
    "            \n",
    "        # Convert lists to DataArrays\n",
    "        months = np.arange(1, 13)\n",
    "        \n",
    "        result_vars[f'precip_{period}m'] = xr.DataArray(precip_list, dims=['month', 'value_index'], coords={'month': months})\n",
    "        result_vars[f'SPI_{period}m'] = xr.DataArray(spi_list, dims=['month', 'value_index'], coords={'month': months})\n",
    "        result_vars[f'CDF_{period}m'] = xr.DataArray(cdf_list, dims=['month', 'value_index'], coords={'month': months})\n",
    "        result_vars[f'PDF_{period}m'] = xr.DataArray(pdf_list, dims=['month', 'value_index'], coords={'month': months})\n",
    "\n",
    "    return xr.Dataset(result_vars)\n",
    "    \n",
    "\n",
    "def zero_precip_monthly_xr(data, cdf_spi_ds, accum_periods, start_ref=\"1991-01-01\", end_ref=\"2020-12-01\"):\n",
    "    \"\"\"\n",
    "    Adjust CDF for zero precipitation probability in xarray.\n",
    "    \"\"\"\n",
    "    # Slice reference period\n",
    "    ref_data = data.sel(valid_time=slice(start_ref, end_ref))\n",
    "    \n",
    "    adjusted_cdf_vars = {}\n",
    "    stats_summary = []\n",
    "\n",
    "    for period in accum_periods:\n",
    "        var_name = f'tp_mm_accum_{period}m'\n",
    "        cdf_var = f'CDF_{period}m'\n",
    "        adjusted_cdf = cdf_spi_ds[cdf_var].copy()\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            # Select month subset\n",
    "            month_subset = ref_data[var_name].where(ref_data['valid_time.month'] == month, drop=True)\n",
    "            n_zero = (month_subset <= 0.02).sum().compute().item() # What is the threshold value?\n",
    "            n_month = month_subset.count().compute().item()\n",
    "\n",
    "            if n_zero > 0:\n",
    "                # Probability of zero precipitation\n",
    "                p_zero = (n_zero + 1) / (2 * (n_month + 1))\n",
    "            else: # do we still need to adjust if there are no months with zero precipitation??\n",
    "                p_zero = 0\n",
    "                \n",
    "            ratio_zero = n_zero / n_month\n",
    "\n",
    "            # Adjust CDF for this month\n",
    "            mask = data['valid_time.month'] == month\n",
    "            \n",
    "            adjusted_cdf = adjusted_cdf.where(~mask, p_zero + (1 - p_zero) * cdf_spi_ds[cdf_var])\n",
    "            \n",
    "            # Append summary\n",
    "            stats_summary.append({\n",
    "                \"Month\": month,\n",
    "                \"SPI\": period,\n",
    "                \"Zero-Precip Count\": int(n_zero),\n",
    "                \"Total Months\": int(n_month),\n",
    "                \"Prob Zero Precip\": p_zero,\n",
    "                \"Historical Ratio\": ratio_zero,\n",
    "            })\n",
    "\n",
    "        adjusted_cdf_vars[cdf_var] = adjusted_cdf\n",
    "\n",
    "    stats_summary_df = pd.DataFrame(stats_summary)\n",
    "    adjusted_cdf_ds = xr.Dataset(adjusted_cdf_vars, coords=data.coords)\n",
    "\n",
    "    return stats_summary_df, adjusted_cdf_ds\n",
    "\n",
    "def shapiro_monthly_test(spi_ds, accum_periods = [1, 3, 6, 12, 24, 36, 48], months = range(1,13)):\n",
    "    results = []\n",
    "    # spi_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    spi_ref = spi_ds.sel(valid_time=slice(\"1991-01-31\", \"2020-12-31\"))\n",
    "    for period in accum_periods:\n",
    "        data = spi_ref[f\"SPI_{period}m\"]\n",
    "        for month in months:\n",
    "            month_data = data.where(data.valid_time.dt.month == month, drop=True).values\n",
    "            month_data = month_data[np.isfinite(month_data)]  # remove inf/-inf\n",
    "\n",
    "            stat, pval = stats.shapiro(month_data, nan_policy=\"omit\")\n",
    "            \n",
    "            normality = 0 if pval < 0.05 else 1\n",
    "\n",
    "            results.append((month, period,  pval,normality))\n",
    "        \n",
    "    # Convert to DataFrame\n",
    "    df_shapiro_results = pd.DataFrame(results, columns=[\"Month\", \"SPI\", \"P-Value\",\"Normality (1=Normal)\"])\n",
    "\n",
    "    return df_shapiro_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### Helper functions for reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Remove these functions after it has been replaced with modular-style requests everywhere\n",
    "def mars_request(stream, var, start_year = 1940, end_year = 2021):\n",
    "    if var == \"tp\":\n",
    "        par = \"228.128\"\n",
    "    elif var == \"pev\":\n",
    "        par = \"251.228\"\n",
    "        \n",
    "    dataset = \"reanalysis-era5-complete\"\n",
    "    request = {\n",
    "        \"class\": \"ea\",\n",
    "        \"date\": \"/\".join(f\"{year}{month:02}01\" for year in range(start_year,int(end_year)+1) for month in range(1,13)),\n",
    "        \"expver\": \"1\",\n",
    "        \"levtype\": \"sfc\",\n",
    "        \"param\": f\"{par}\",\n",
    "        \"stream\": f\"{stream}\",\n",
    "        'grid'    : '0.25/0.25',\n",
    "        \"type\": \"fc\",\n",
    "        \"data_format\": \"netcdf\" \n",
    "    }\n",
    "\n",
    "    mars_data = ekd.from_source(\"cds\", dataset, request)  # sends the request to CDS\n",
    "    mars_xarray = mars_data.to_xarray()  # convert to xarray.Dataset\n",
    "\n",
    "    return mars_xarray\n",
    "\n",
    "def era5_drought_api_multiple(indicator, var, accum_period = [1, 3, 6, 12, 24, 36, 48]):\n",
    "    ind = indicator.strip().upper()\n",
    "    if ind not in {\"SPI\", \"SPEI\"}:\n",
    "        raise ValueError(\"indicator must be 'SPI' or 'SPEI'\")\n",
    "    ind = ind.lower()\n",
    "\n",
    "    var_key = var.strip().lower()\n",
    "    if var_key not in {\"prob_zero\", \"quality\"}:\n",
    "        raise ValueError(\"var must be 'prob_zero' or 'quality'\")\n",
    "    if var_key == \"prob_zero\":\n",
    "        dataset_name = \"derived-drought-historical-monthly\"\n",
    "        request_var = f\"probability_of_zero_precipitation_{ind}\"\n",
    "        source_var_name = \"pzero\"\n",
    "        rename_prefix = \"prob_zero\"\n",
    "    else:\n",
    "        dataset_name = \"derived-drought-historical-monthly\"\n",
    "        request_var = f\"test_for_normality_{ind}\"\n",
    "        source_var_name = \"significance\"\n",
    "        rename_prefix = \"significance\"\n",
    "    \n",
    "\n",
    "    out = []\n",
    "    \n",
    "    for p in accum_period:\n",
    "        request = {\n",
    "            \"variable\": [request_var],\n",
    "            \"accumulation_period\": [str(p)],\n",
    "            \"version\": \"1_0\",\n",
    "            \"product_type\": [\"reanalysis\"],\n",
    "            \"dataset_type\": \"consolidated_dataset\",\n",
    "            \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "        }\n",
    "\n",
    "        ds = ekd.from_source(\"cds\", dataset_name, request).to_xarray(compat=\"equals\")\n",
    "        \n",
    "        if source_var_name not in ds.variables:\n",
    "            raise KeyError(\n",
    "                f\"Expected variable '{source_var_name}' not found for period {p}. \"\n",
    "                f\"Available: {list(ds.variables)}\"\n",
    "            )\n",
    "\n",
    "        new_name = f\"{rename_prefix}_{p}\"\n",
    "        ds_renamed = ds.rename({source_var_name: new_name})[[new_name]]\n",
    "        out.append(ds_renamed)\n",
    "            \n",
    "    return xr.merge(out, compat=\"override\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-2)=\n",
    "### 2. Calculate SPI at one location from ERA5 reanalysis data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Download monthly precipitation data \n",
    "First, we download the monthly-mean total precipitation data from the ERA5 reanalysis.\n",
    "Generally,\n",
    "one would use the [_ERA5 monthly averaged data on single levels from 1940 to present_ (reanalysis-era5-single-levels-monthly-means)](https://doi.org/10.24381/cds.f17050d7) dataset for this,\n",
    "which provides pre-calculated monthly means at 0.25Â° by 0.25Â° resolution.\n",
    "For this assessment,\n",
    "to be as close to the ERA5-Drought data processing pipeline as possible\n",
    "and\n",
    "to make use of some of MARS's functionalities (see [below](section-4)),\n",
    "we instead use the [_Complete ERA5 global atmospheric reanalysis_ (reanalysis-era5-complete)](https://doi.org/10.24381/cds.143582cf) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "In this assessment,\n",
    "we will calculate SPI and SPEI for each month\n",
    "(with different accumulation periods, see below)\n",
    "for the years 1940â€“2024.\n",
    "For the reference period,\n",
    "we will use the World Meteorological Organization (WMO) current standard 30-year reference period of 1991â€“2020,\n",
    "which is also used in ERA5-Drought.\n",
    "Both of these date ranges can be adjusted in the cell below when running the analysis yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preferred analysis and reference periods\n",
    "years           = (1940, 2024)  # Years for the analysis (inclusive)\n",
    "years_reference = (1991, 2020)  # Years for the reference period (inclusive)\n",
    "\n",
    "# Derived variables for convenience:\n",
    "reference_window = {\"valid_time\": slice(f\"{years_reference[0]}-01-01\", f\"{years_reference[1]}-12-01\"),}  #  Slice (1991-01-01, 2020-12-01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Having defined our target years, we can now define our CDS request.\n",
    "First, we define a template with some default parameters\n",
    "(e.g. years, data format)\n",
    "that will also be used later in the notebook.\n",
    "Additional information for specific downloads\n",
    "(e.g. variable, data stream)\n",
    "is mixed into this template where relevant.\n",
    "\n",
    "This notebook uses [earthkit-data](https://github.com/ecmwf/earthkit-data) to download files from the CDS.\n",
    "If you intend to run this notebook multiple times, it is highly recommended that you [enable caching](https://earthkit-data.readthedocs.io/en/latest/guide/caching.html) to prevent having to download the same files multiple times.\n",
    "If you prefer not to use earthkit, the following requests can also be used with the [cdsapi module](https://cds.climate.copernicus.eu/how-to-api#linux-use-client-step).\n",
    "In either case (earthkit-data or cdsapi), it is required to set up a CDS account and API key as explained [on the CDS website](https://cds.climate.copernicus.eu/how-to-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_ERA5 = \"reanalysis-era5-complete\"\n",
    "\n",
    "request_era5_template = {\n",
    "    \"class\": \"ea\",            # Default for ERA5\n",
    "    # Dates: ERA5 takes these in the format 19400101/19400201/.../20241101/20241201\n",
    "    # The following line generates a string in said format from the chosen year range\n",
    "    \"date\": \"/\".join(f\"{year}{month:02}01\"\n",
    "            for year in range(years[0] ,years[1]+1)\n",
    "            for month in range(1,13)),\n",
    "    \"expver\": \"1\",            # ERA5 consolidated data\n",
    "    \"levtype\": \"sfc\",         # Surface\n",
    "    \"grid\": \"0.25/0.25\",      # Grid: 0.25Â° by 0.25Â°\n",
    "    \"type\": \"fc\",             # Forecast\n",
    "    \"data_format\": \"netcdf\",  # NetCDF data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "In this section, we want to download \n",
    "total precipitation data (variable `228.128`)\n",
    "from the\n",
    "`moda` stream (monthly-mean reanalysis data),\n",
    "so we mix this information into the template\n",
    "and submit the request to the CDS.\n",
    "More information about formatting these requests is available in the [MARS ERA5 catalogue](https://apps.ecmwf.int/data-catalogues/era5/?class=ea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_precipitation_moda = {\n",
    "    \"param\": \"228.128\",       # Variable: Total precipitation\n",
    "    \"stream\": \"moda\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and convert to desired format\n",
    "era5_monthly_mean_reanal = ekd.from_source(\"cds\", ID_ERA5, request_era5_precipitation_moda)  # Download as field list\n",
    "era5_monthly_mean_reanal = era5_monthly_mean_reanal.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_monthly_mean_reanal  # Display in notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### 1. Calculate moving average for different accumulation periods.\n",
    "A time series of precipitation for one grid point is extracted and precipitation is accumulated over the previous $n$ months using a moving window. Since the origin precipitation data from ERA5 is from the monthly mean dataset, we calculate the total precipitation for that month by multiplying with the total number of days in that month, correcting for leap years.\n",
    "\n",
    "The different accumulation windows are used to determine the timescale of the drought. The longer the drought, typically the more severe the impact it will have.\n",
    "\n",
    "* **1-, 3-month window**: useful for soil moisture, flow in small creaks.\n",
    "* **6-, 12-month window**: looking at reservoir storage, reduced stream flow.\n",
    "* **24-, 36-, 48-month window**: groundwater recharge, reduced reservoir. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "TO DO: Some text about the user choosing their point here, which point did we choose and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preferred site for the SPI example\n",
    "example_site = {\"latitude\": 9.25, \"longitude\": 40.5,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the grid point and time slice for Ethiopia\n",
    "precipitation_example_site = era5_monthly_mean_reanal.sel(**example_site)\n",
    "\n",
    "# Perform the accumulation for each accumulation period\n",
    "precipitation_example_site = accum_var(precipitation_example_site, \"tp\")\n",
    "\n",
    "# Display result\n",
    "precipitation_example_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Refactor so the same function can be re-used for TPâ€“PEV later\n",
    "# TO DO: Make the order of the legend match the order of the lines (top to bottom),\n",
    "# e.g. by plotting in reverse acc order (because you know it always increases one way)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for p in ACCUMULATION_PERIODS:\n",
    "    var_name = f'tp_mm_accum_{p}m'\n",
    "    plt.plot(precipitation_example_site['valid_time'].values, precipitation_example_site[var_name].values, label=f'{p}-month')\n",
    "\n",
    "# Customize plot\n",
    "plt.title(f\"Precipitation Accumulation at ({example_site['latitude']} Â°N, {example_site['longitude']} Â°E)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Precipitation [mm]\")\n",
    "plt.legend(title=\"Accumulation period\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### 2. Fit gamma distribution (over calendar months) to different accumulation periods.\n",
    "\n",
    "The gamma distribution [reference] is fitted only to the data within the reference period (1991-2020). A separate distribution is fitted for each calendar month per accumulation window. \n",
    "\n",
    "For e.g. : in the 3-month accumulation window, a gamma distribution is fitted on all 30 Januaries in that reference period, all the Februaries (30 of them) and so forth... \n",
    "\n",
    "This fitting is done with the [scipy.stats.gamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html) object in Python. Three parameters are then outputted per calendar month, per accumulation period: the shape ($\\alpha$), location ($\\beta$) and scale ($\\lambda$) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_example_site_reference = precipitation_example_site.sel(**reference_window)\n",
    "\n",
    "# Fit gamma distributions\n",
    "gamma_params = fit_monthly_distributions_xr(precipitation_example_site_reference, \"tp\")\n",
    "\n",
    "# Display result\n",
    "gamma_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### 3. Compute SPI series\n",
    "\n",
    "From the parameters from the gamma distribution fitting, we calculate the cumulative distribution function or CDF, $F(x, \\alpha, \\beta, \\lambda)$, up to the accumulated precipitation $x$, from the probability distribution function or PDF, $f(u, \\alpha, \\beta, \\lambda)$:\n",
    "\n",
    "\\begin{equation}\n",
    "F(x, \\alpha, \\beta, \\lambda) = \\int_{0}^{x}f(u, \\alpha, \\beta, \\lambda) \\, du.\n",
    "\\end{equation}\n",
    "\n",
    "A one-to-one mapping of SPI-index to accumulated precipitation value is then obtained by transforming the cumulative probability values to a standard normal distribution with a mean ($\\mu$) of zero and standard deviation ($\\sigma$) of 1. This mapping is applied to the historical record of the accumulated precipitation values in that calendar month and accumulation window.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{SPI-index} = \\Phi^{-1}\\big(F(x, \\alpha, \\beta, \\lambda)\\big)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SPI series\n",
    "spi_ds, cdf_ds = compute_monthly_series_xr(point_ds, accum_periods, gamma_params)\n",
    "\n",
    "# Compute CDF, SPI, total precipitation from minimum to maximum value with 1000 points.\n",
    "# spi_ds_prec = compute_spi_dataset(point_ds,accum_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Plot of Precipitation vs CDF (for different accumulation windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "spi_month_ds = spi_ds_prec.where(spi_ds_prec.month == 5, drop=True)\n",
    "\n",
    "for p in accum_periods:\n",
    "    var1_name = f'SPI_{p}m'\n",
    "    var2_name = f'CDF_{p}m'\n",
    "    var3_name = f'precip_{p}m'\n",
    "    cs = CubicSpline(spi_month_ds[var2_name].values.ravel(), spi_month_ds[var3_name].values.ravel())\n",
    "    x_range = np.arange(-0.01, 1.01, 0.01)\n",
    "    plt.plot(cs(x_range), x_range, label=f'{p}-month')\n",
    "    # plt.scatter(spi_ds[var1_name].values, cdf_ds[var2_name].values, label=f'{p}-month')\n",
    "    # plt.scatter(spi_month_ds[var1_name].values, spi_month_ds[var3_name].values, label=f'{p}-month', s=10)\n",
    "\n",
    "# Customize plot\n",
    "plt.title('CDF vs Precipitaiton')\n",
    "plt.xlabel('Precipitation (mm)')\n",
    "plt.ylabel('CDF')\n",
    "plt.ylim([0,1])\n",
    "plt.legend(title='Accumulation Period')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of Precipitation vs SPI (for different accumulation windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "spi_month_ds = spi_ds_prec.where(spi_ds_prec.month == 5, drop=True)\n",
    "\n",
    "for p in accum_periods:\n",
    "    var1_name = f'SPI_{p}m'\n",
    "    var2_name = f'CDF_{p}m'\n",
    "    var3_name = f'precip_{p}m'\n",
    "    cs = CubicSpline(spi_month_ds[var1_name].values.ravel(), spi_month_ds[var3_name].values.ravel())\n",
    "    x_range = np.arange(-3, 3, 0.01)\n",
    "    plt.plot(x_range, cs(x_range), label=f'{p}-month')\n",
    "    # plt.scatter(spi_ds[var1_name].values, cdf_ds[var2_name].values, label=f'{p}-month')\n",
    "    # plt.scatter(spi_month_ds[var1_name].values, spi_month_ds[var3_name].values, label=f'{p}-month', s=10)\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Precipitation vs SPI')\n",
    "plt.xlabel('SPI')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.legend(title='Accumulation Period')\n",
    "# plt.xlim([-1, 1])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of SPI vs CDF (for different accumulation windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "spi_month_ds = spi_ds_prec.where(spi_ds_prec.month == 5, drop=True)\n",
    "\n",
    "for p in accum_periods:\n",
    "    var1_name = f'SPI_{p}m'\n",
    "    var2_name = f'CDF_{p}m'\n",
    "    var3_name = f'precip_{p}m'\n",
    "    cs = CubicSpline(spi_month_ds[var1_name].values.ravel(), spi_month_ds[var2_name].values.ravel())\n",
    "    x_range = np.arange(-3, 3, 0.01)\n",
    "    plt.plot(x_range, cs(x_range), label=f'{p}-month')\n",
    "    # plt.scatter(spi_ds[var1_name].values, cdf_ds[var2_name].values, label=f'{p}-month')\n",
    "    # plt.scatter(spi_month_ds[var1_name].values, spi_month_ds[var3_name].values, label=f'{p}-month', s=10)\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Precipitation vs SPI')\n",
    "plt.xlabel('SPI')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.legend(title='Accumulation Period')\n",
    "# plt.xlim([-1, 1])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### 4. Calculating historical ratio of months without precipitation and finding zero adjusted SPI (ref dataset)\n",
    "In regions of extremely low precipitation (e.g. the Sahara desert), months may have little to no accumulated precipitation. This poses a problem when fitting the gamma distribution since it is defined only for positive, real values. Furthermore, a criteria is set so that out of the 30 months in the reference period, per calendar month & accumulation window, 10 months must have non-zero accumulated precipitations. Otherwise, the calendar month is reported as having no value.\n",
    "\n",
    "Months with zero precipitation are defined as less than 0.1 mm by the [Copernicus European (EDO) and Global (GDO) Drought Observatories](https://drought.emergency.copernicus.eu/). The ERA5-Drought dataset appears to define months with zero precipitation as having an accumulated precipitation exactly equal to zero. \n",
    "\n",
    "To get around some months having zero precipitation, the CDF is adjusted with the historical occurrence $p_{0}$ of periods with zero precipitation [Stagge et al., â€˜Candidate Distributions for Climatological Drought Indices](https://doi.org/10.1002/joc.4267).\n",
    "\n",
    "\\begin{equation}\n",
    "F_{p_{0}}(x_{p>0},  \\alpha, \\beta, \\lambda) = p_{0} + \\big(1 - p_{0}\\big) \\, F(x_{p>0},  \\alpha, \\beta, \\lambda),\n",
    "\\end{equation}\n",
    "\n",
    "where $F_{p_{0}}(x_{p>0},  \\alpha, \\beta, \\lambda)$ is the CDF adjusted for zero precipitation.\n",
    "\n",
    "This simple treatment can lead to a z-distribution that is skewed, with a non-zero mean. Therefore, special care must be taken, and $p_{0}$ must be adjusted for the \"centre of probability mass\" following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{p}_{0} = \\frac{n_{p=0} + 1}{2(n + 1)},\n",
    "\\end{equation}\n",
    "\n",
    "where $n_{p=0}$ are the number of calendar months in the reference period per accumulation window, and n are the total number of calendar months (30 reference calendar months).\n",
    "\n",
    "\\begin{equation}\n",
    "F_{\\bar{p}_{0}}(x, \\alpha, \\beta, \\lambda) =\n",
    "\\begin{cases}\n",
    "\\bar{p}_{0} + (1 - \\bar{p}_{0}) \\, F(x_{p>0},  \\alpha, \\beta, \\lambda), & x > 0, \\\\[6pt]\n",
    "\\frac{n_{p=0} + 1}{2(n+1)}, & x = 0.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "This maintains the mean SPI value of zero, allowing for an objective statistical comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_sum, adjusted_cdf_ds = zero_precip_monthly_xr(point_ds, cdf_ds, accum_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "#### 5. Adjust SPI values with zero precipitation probability.\n",
    "\n",
    "The CDF adjusted for months of zero precipitation is once again transformed to the z-normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Adjusted SPI-index} = \\Phi^{-1}\\big(F_{\\bar{p}_{0}}(x,  \\alpha, \\beta, \\lambda)\\big)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_vars = {}\n",
    "\n",
    "for period in accum_periods:\n",
    "    cdf_data = adjusted_cdf_ds[f\"CDF_{period}m\"]\n",
    "    spi_vals = stats.norm.ppf(np.clip(cdf_data, 1e-16, 1 - 1e-16), loc=0, scale=1) # clipping at 8 sigma.\n",
    "    spi_data = xr.DataArray(spi_vals, coords=cdf_data.coords, attrs={\"long_name\": \"SPI\"})\n",
    "    spi_vars[f\"SPI_{period}m\"] = spi_data\n",
    "\n",
    "adjusted_spi_ds = xr.Dataset(spi_vars, coords=adjusted_cdf_ds.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Importing ERA5-Drought SPI data from dataset for comparison.\n",
    "Data request must be \"weaved\" as request too big for entire time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_dataset = \"derived-drought-historical-monthly\"\n",
    "\n",
    "spi_request1 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75]\n",
    "}\n",
    "\n",
    "spi_request2 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1981\", \"1982\", \"1983\",\n",
    "        \"1984\", \"1985\", \"1986\",\n",
    "        \"1987\", \"1988\", \"1989\",\n",
    "        \"1990\", \"1991\", \"1992\",\n",
    "        \"1993\", \"1994\", \"1995\",\n",
    "        \"1996\", \"1997\", \"1998\",\n",
    "        \"1999\", \"2000\", \"2001\",\n",
    "        \"2002\", \"2003\", \"2004\",\n",
    "        \"2005\", \"2006\", \"2007\",\n",
    "        \"2008\", \"2009\", \"2010\",\n",
    "        \"2011\", \"2012\", \"2013\",\n",
    "        \"2014\", \"2015\", \"2016\",\n",
    "        \"2017\", \"2018\", \"2019\",\n",
    "        \"2020\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75] # Ethiopia\n",
    "}\n",
    "\n",
    "era5_drought_spi = ekd.from_source(\"cds\", drought_dataset, spi_request1, spi_request2) # Sends request for this dataset to CDS.\n",
    "era5_drought_spi = era5_drought_spi.to_xarray(compat=\"equals\") # Converts to xarray.\n",
    "era5_drought_spi = era5_drought_spi.sel(lat=9.25,lon=40.5, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Comparison of the calculated SPI-index vs ERA5-Drought SPI-index (qualatitive)\n",
    "Now that we have calculated the SPI-index for one grid point, for all accumulation windows, we make a qualatitive and quantitative comparison with the corresponding data in the ERA5-Drought dataset. \n",
    "\n",
    "First, we plot the timeseries of the SPI drought index, calculated and from the derived dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x4 grid of subplots\n",
    "fig, axs = plt.subplots(4, 2, figsize=(16, 12), sharex=True, sharey=True, constrained_layout=True)\n",
    "fig.suptitle('ERA5-Drought SPI vs Calculated SPI for varying windows.', fontsize=16)\n",
    "\n",
    "# Flatten axs for easy indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "for position, period in enumerate(accum_periods):\n",
    "    ax = axs[position]\n",
    "\n",
    "    # Plot ERA5 drought SPI\n",
    "    ax.plot(era5_drought_spi[f\"SPI{period}\"].time,\n",
    "            era5_drought_spi[f\"SPI{period}\"],\n",
    "            label=f\"ERA5_Drought-SPI{period}\",\n",
    "            color=\"tab:blue\")\n",
    "\n",
    "    # Plot calculated SPI\n",
    "    ax.plot(adjusted_spi_ds[f\"SPI_{period}m\"].valid_time,\n",
    "            adjusted_spi_ds[f\"SPI_{period}m\"],\n",
    "            label=f\"Calculated SPI{period}\",\n",
    "            color=\"tab:orange\")\n",
    "\n",
    "    # Title for each subplot\n",
    "    ax.sharex(axs[0])\n",
    "    ax.set_title(f\"{period}-Month Window\", fontsize=10)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('SPI')    \n",
    "    ax.grid(True)\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(accum_periods), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "# Add a single legend for the whole figure\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper left\", ncol=2, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "#### Comparison of the calculated SPI-index vs ERA5-Drought SPI-index (quantative- plot of residuals & frequency histogram).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_spi_ds_focus = adjusted_spi_ds.rename({\n",
    "    \"valid_time\": \"time\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\",\n",
    "}) # for alignment\n",
    "\n",
    "# Create a 4x4 grid of subplots\n",
    "fig, axs = plt.subplots(4, 2, figsize=(16, 12), sharex=True, sharey=False, constrained_layout=True)\n",
    "fig.suptitle('Residual SPI for varying windows.', fontsize=16)\n",
    "\n",
    "# Flatten axs for easy indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "for position, period in enumerate(accum_periods):\n",
    "    ax = axs[position]\n",
    "    residual  = adjusted_spi_ds_focus[f\"SPI_{period}m\"] - era5_drought_spi[f\"SPI{period}\"]\n",
    "    # Plot residual between ERA5-SPI & Calculated-SPI\n",
    "    ax.plot(residual.time,\n",
    "            residual,\n",
    "            label=f\"Residual-SPI{period}\",\n",
    "            color=\"tab:blue\")\n",
    "\n",
    "    # Title for each subplot\n",
    "    ax.sharex(axs[0])\n",
    "    ax.set_title(f\"{period}-Month Window\", fontsize=10)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Residual SPI')    \n",
    "    ax.grid(True)\n",
    "    # ax.set_ylim([-1e-5, 1e-5])\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(accum_periods), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "# Add a single legend for the whole figure\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper left\", ncol=2, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_spi_ds_focus = adjusted_spi_ds.rename({\n",
    "    \"valid_time\": \"time\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\",\n",
    "}) # for alignment\n",
    "\n",
    "# Create a 4x4 grid of subplots\n",
    "fig, axs = plt.subplots(4, 2, figsize=(16, 12), sharex=False, sharey=False, constrained_layout=True)\n",
    "fig.suptitle('Residual SPI for varying windows.', fontsize=16)\n",
    "\n",
    "# Flatten axs for easy indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "for position, period in enumerate(accum_periods):\n",
    "    ax = axs[position]\n",
    "    \n",
    "    residual  = adjusted_spi_ds_focus[f\"SPI_{period}m\"] - era5_drought_spi[f\"SPI{period}\"]\n",
    "    \n",
    "    vals = (residual.values).ravel() \n",
    "    \n",
    "    finite_mask = np.isfinite(vals)\n",
    "    \n",
    "    vals = vals[finite_mask]\n",
    "\n",
    "    count = vals.size\n",
    "    mean_val = np.nanmean(vals)\n",
    "    std_val = np.nanstd(vals)\n",
    "\n",
    "    freq, bin_edges = np.histogram(vals, bins=20)\n",
    "    rel_freq = freq / count\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    # Plot relative frequency histogram\n",
    "    ax.bar(bin_centers, rel_freq, width=(bin_edges[1] - bin_edges[0]))\n",
    "    \n",
    "    ax.annotate(\n",
    "            f\"Mean residual - SPI {period}: {mean_val: .7f}\\n Std residual- SPI {period}: {std_val: .7f}\",\n",
    "            xy=(0.02, 0.95), xycoords=\"axes fraction\",  # position in axes coords\n",
    "            fontsize=10,\n",
    "            ha=\"left\", va=\"top\",\n",
    "        )\n",
    "    \n",
    "    # Title for each subplot\n",
    "    ax.sharex(axs[0])\n",
    "    ax.set_title(f\"{period}-Month Window\", fontsize=10)\n",
    "    ax.set_xlabel('Residual SPI')\n",
    "    ax.set_ylabel('Frequency')    \n",
    "    ax.grid(True)\n",
    "    \n",
    "# Remove unused axes\n",
    "for i in range(len(accum_periods), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "# Add a single legend for the whole figure\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper left\", ncol=2, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "####  6. Comparison of Probability of Zero Precipitation to that from ERA5-Drought dataset.\n",
    "You can also import the \"probability of zero precipitation\" for a given calendar month, in the reference period, per accumulation period instead of calculating it as we have done. \n",
    "\n",
    "Due to improper formatting from ERA5-Drought, you are unable to send an API request to download the data for all accumulation periods at once. If you try to do so, you will find that your xarray only contains the data for the last accumulation period (in this case, the 48-month window). \n",
    "\n",
    "We have written a simple helper function, \"era5_api_multiple\" that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_spi_zero_all = era5_drought_api_multiple(indicator = \"spi\", var = \"prob_zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust until you get probabilities right.\n",
    "def zero_precip_monthly_xr(data, cdf_spi_ds, accum_periods, start_ref=\"1991-01-01\", end_ref=\"2020-12-01\"):\n",
    "    \"\"\"\n",
    "    Adjust CDF for zero precipitation probability in xarray.\n",
    "    \"\"\"\n",
    "    # Slice reference period\n",
    "    ref_data = data.sel(valid_time=slice(start_ref, end_ref))\n",
    "    \n",
    "    adjusted_cdf_vars = {}\n",
    "    stats_summary = []\n",
    "\n",
    "    for period in accum_periods:\n",
    "        var_name = f'tp_mm_accum_{period}m'\n",
    "        cdf_var = f'CDF_{period}m'\n",
    "        adjusted_cdf = cdf_spi_ds[cdf_var].copy()\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            # Select month subset\n",
    "            month_subset = ref_data[var_name].where(ref_data['valid_time.month'] == month, drop=True)\n",
    "            n_zero = (month_subset < 0.01).sum().compute().item() # What is the threshold value? \n",
    "            n_month = month_subset.count().compute().item()\n",
    "\n",
    "            if n_zero > 0:\n",
    "                # Probability of zero precipitation\n",
    "                p_zero = (n_zero + 1) / (2 * (n_month + 1))\n",
    "            else: # do we still need to adjust if there are no months with zero precipitation??\n",
    "                p_zero = 0\n",
    "                \n",
    "            ratio_zero = n_zero / n_month\n",
    "\n",
    "            # Adjust CDF for this month\n",
    "            mask = data['valid_time.month'] == month\n",
    "            \n",
    "            adjusted_cdf = adjusted_cdf.where(~mask, p_zero + (1 - p_zero) * cdf_spi_ds[cdf_var])\n",
    "            \n",
    "            # Append summary\n",
    "            stats_summary.append({\n",
    "                \"Month\": month,\n",
    "                \"SPI\": period,\n",
    "                \"Zero-Precip Count\": int(n_zero),\n",
    "                \"Total Months\": int(n_month),\n",
    "                \"Prob Zero Precip\": p_zero,\n",
    "                \"Historical Ratio\": ratio_zero,\n",
    "            })\n",
    "\n",
    "        adjusted_cdf_vars[cdf_var] = adjusted_cdf\n",
    "\n",
    "    stats_summary_df = pd.DataFrame(stats_summary)\n",
    "    adjusted_cdf_ds = xr.Dataset(adjusted_cdf_vars, coords=data.coords)\n",
    "\n",
    "    return stats_summary_df, adjusted_cdf_ds\n",
    "\n",
    "\n",
    "stats_sum, adjusted_cdf_ds = zero_precip_monthly_xr(point_ds, cdf_ds, accum_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_sum[stats_sum[\"Month\"]==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_sum[stats_sum[\"Month\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_spi_zero_all_ds = prob_spi_zero_all.sel(lat=9.25,lon=40.5, method=\"nearest\")\n",
    "# prob_spi_zero_all_ds[\"prob_zero_1\"]\n",
    "\n",
    "prob_spi_zero_all_ds[\"prob_zero_1\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "##### Comparing Calculated Probability of Zero values with that from ERA5-Drought dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### 7. Quality control using the Shapiro-Wilks test on calculated SPI data.\n",
    "Quality control is performed by the ERA5-Drought team over the entire dataset. This is done by testing if the calculated distribution of the estimated drought indices over the reference period follows a normal distribution with mean 0 and standard deviation 1. \n",
    "\n",
    "This test is performed using the Shapiro-Wilks test for normality [S. S. SHAPIRO, M. B. WILK, An analysis of variance test for normality (complete samples), Biometrika, Volume 52, Issue 3-4, December 1965, Pages 591â€“611](https://doi.org/10.1093/biomet/52.3-4.591), with a $\\alpha$ = 0.05 on the data in the reference period (1991-2020). \n",
    "\n",
    "If the resultant p-value is less than $\\alpha$ = 0.05, the corresponding quality parameter is set to 0 (bad), otherwise set to 1 (good).\n",
    "\n",
    "We perform this test over the calculated SPI values, that are adjusted for zero-precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shapiro_results = shapiro_monthly_test(spi_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shapiro_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "##### Importing Shapiro-Wilks SPI Significance Test results from ERA5-Drought dataset.\n",
    "You can also import the quality flags from ERA5-Drought, for every calendar month, in the reference period, per accumulation period. \n",
    "\n",
    "Due to improper formatting from ERA5-Drought, you are unable to send an API request to download this data for all accumulation periods at once. If you try to do so, you will find that your xarray only contains the data for the last accumulation period (in this case, the 48-month window). \n",
    "\n",
    "We have written a simple script that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi_all = era5_drought_api_multiple(indicator = \"spi\", var = \"quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Comparison of significance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select quality flags at one location and compare that with dataset.\n",
    "quality_spi_all_ds = quality_spi_all.sel(lat=9.25,lon=40.5, method=\"nearest\")\n",
    "\n",
    "for p in accum_periods:\n",
    "    quality_accum  = quality_spi_all_ds[f\"significance_{p}\"]\n",
    "    calculated_accum = df_shapiro_results[df_shapiro_results[\"SPI\"]==p]\n",
    "    for m in range(1,13):\n",
    "        quality_month = quality_accum.where(quality_accum.time.dt.month.isin(m), drop=True)\n",
    "        calculated_month = calculated_accum[calculated_accum[\"Month\"]== m]\n",
    "\n",
    "        if quality_month.values.item() != calculated_month[\"Normality (1=Normal)\"].iloc[0]:\n",
    "            print(\"Discrepancy in quality\")\n",
    "        else:\n",
    "            pass\n",
    "    print(f\"Quality indicators for SPI-{p} match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Calculating ERA5-Drought SPI (across a region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_periods = [48]\n",
    "\n",
    "# Select latitude and longitude box (keeping small for smaller compute time, recommended 10 x 10)\n",
    "lon_min, lon_max = 19.0, 21.5   \n",
    "lat_min, lat_max = 41.0, 43.5\n",
    "\n",
    "# Subset (latitude is descending in ERA5)\n",
    "ds_loc = point_ds.sel(\n",
    "    latitude=slice(lat_max, lat_min),\n",
    "    longitude=slice(lon_min, lon_max),\n",
    ")\n",
    "\n",
    "ds_loc, days_in_month = accum_var(ds_loc, var = \"tp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [48]\n",
    "\n",
    "start_ref, end_ref  = \"1991-01-01\", \"2020-12-01\"\n",
    "\n",
    "ds_loc_ref = ds_loc.sel({\"valid_time\": slice(start_ref, end_ref)})\n",
    "\n",
    "# Prepare coords\n",
    "months = np.arange(1, 13)\n",
    "lat_vals = ds_loc_ref[\"latitude\"].values\n",
    "lon_vals = ds_loc_ref[\"longitude\"].values\n",
    "\n",
    "# Create empty parameter arrays (NaNs), with desired dims/coords\n",
    "alpha = xr.DataArray(\n",
    "    np.nan,\n",
    "    dims=(\"period\", \"month\", \"latitude\", \"longitude\"),\n",
    "    coords={\n",
    "        \"period\": accum_periods,\n",
    "        \"month\": months,\n",
    "        \"latitude\": lat_vals,\n",
    "        \"longitude\": lon_vals,\n",
    "    },\n",
    ")\n",
    "loc = alpha.copy()\n",
    "beta = alpha.copy()\n",
    "\n",
    "# # Helper: position of each period for quick indexing\n",
    "# period_pos = {p: i for i, p in enumerate(accum_periods)}\n",
    "\n",
    "# Loop over all grid cells\n",
    "for lat in lat_vals:\n",
    "    for lon in lon_vals:\n",
    "        pt = ds_loc_ref.sel(latitude=lat, longitude=lon)\n",
    "\n",
    "        monthly_params = fit_monthly_distributions_xr(\n",
    "            pt, accum_periods, start_ref, end_ref, var=\"tp\"\n",
    "        )\n",
    "\n",
    "        # Write results into the parameter maps\n",
    "        for (month, period), (a, l, b) in monthly_params.items():\n",
    "            alpha.loc[dict(period=period, month=month, latitude=lat, longitude=lon)] = a\n",
    "            loc.loc[dict(period=period, month=month, latitude=lat, longitude=lon)] = l\n",
    "            beta.loc[dict(period=period, month=month, latitude=lat, longitude=lon)] = b\n",
    "\n",
    "# Pack into a Dataset\n",
    "gamma_params = xr.Dataset({\"alpha\": alpha, \"loc\": loc, \"beta\": beta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect dim names\n",
    "time_dim = \"valid_time\" if \"valid_time\" in ds_loc_ref.dims else \"forecast_reference_time\"\n",
    "lat_name = \"latitude\"\n",
    "lon_name = \"longitude\"\n",
    "\n",
    "spi_vars = {}\n",
    "\n",
    "for period in accum_periods:\n",
    "    var_name = f\"tp_mm_accum_{period}m\"\n",
    "    vals = ds_loc[var_name]  # (time, lat, lon)\n",
    "\n",
    "    spi = xr.full_like(vals, np.nan, dtype=float) # allocate matching xarray.\n",
    "\n",
    "    # Group by month from the values\n",
    "    gb = vals.groupby(f\"{time_dim}.month\")\n",
    "\n",
    "    for m, vals_m in gb:  # vals_m: (time_m, lat, lon)\n",
    "        # Parameter fields (lat, lon)\n",
    "        alpha_m = gamma_params[\"alpha\"].sel(period=period, month=m)\n",
    "        loc_m   = gamma_params[\"loc\"].sel(period=period, month=m)\n",
    "        beta_m  = gamma_params[\"beta\"].sel(period=period, month=m)\n",
    "\n",
    "        # Core dim = time only; broadcast lat/lon params\n",
    "        cdf_m = xr.apply_ufunc(\n",
    "            stats.gamma.cdf,\n",
    "            vals_m, alpha_m, loc_m, beta_m,\n",
    "            input_core_dims=[[time_dim], [], [], []],   # âœ… only time is core\n",
    "            output_core_dims=[[time_dim]],\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "            output_dtypes=[float],\n",
    "        ).clip(min=1e-16, max= 1 - 1e-16)                # avoid ppf inf\n",
    "\n",
    "        spi_m = xr.apply_ufunc(\n",
    "            stats.norm.ppf,\n",
    "            cdf_m,\n",
    "            input_core_dims=[[time_dim]],\n",
    "            output_core_dims=[[time_dim]],\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "            output_dtypes=[float],\n",
    "        )\n",
    "\n",
    "        # Assign to positions for month m, built from vals (the full series)\n",
    "        month_idx = vals[time_dim].dt.month == m\n",
    "        target_times_m = vals[time_dim].where(month_idx, drop=True)\n",
    "    \n",
    "        # Align to exact timestamps before assignment\n",
    "        spi.loc[{time_dim: target_times_m}] = spi_m.sel({time_dim: target_times_m})\n",
    "    \n",
    "    # Store variables\n",
    "    spi_vars[f\"SPI_{period}m\"] = spi\n",
    "\n",
    "# Final datasets\n",
    "spi_ds = xr.Dataset(spi_vars, coords=ds_loc.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "\n",
    "request1 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ], \n",
    "    \"area\": [43.5, 19.0, 41.0, 21.5]  # lat, lon \n",
    "}\n",
    "\n",
    "request2 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1981\", \"1982\", \"1983\",\n",
    "        \"1984\", \"1985\", \"1986\",\n",
    "        \"1987\", \"1988\", \"1989\",\n",
    "        \"1990\", \"1991\", \"1992\",\n",
    "        \"1993\", \"1994\", \"1995\",\n",
    "        \"1996\", \"1997\", \"1998\",\n",
    "        \"1999\", \"2000\", \"2001\",\n",
    "        \"2002\", \"2003\", \"2004\",\n",
    "        \"2005\", \"2006\", \"2007\",\n",
    "        \"2008\", \"2009\", \"2010\",\n",
    "        \"2011\", \"2012\", \"2013\",\n",
    "        \"2014\", \"2015\", \"2016\",\n",
    "        \"2017\", \"2018\", \"2019\",\n",
    "        \"2020\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [43.5, 19.0, 41.0, 21.5] # lat, lon \n",
    "}\n",
    "\n",
    "spi_drought = ekd.from_source(\"cds\", dataset, request1,request2) # Sends request for this dataset to CDS.\n",
    "spi_drought = spi_drought.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_ds = spi_ds.rename({\n",
    "    \"valid_time\": \"time\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\"})\n",
    "\n",
    "# spi_ds = spi_ds.drop_vars(\"number\")   # removes as a coordinate\n",
    "\n",
    "# spi_ds = spi_ds.drop_vars(\"expver\")   # removes as a coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_drought, spi_ds = xr.align(spi_drought, spi_ds, join=\"inner\")   # only overlapping coords\n",
    "spi_diff = (spi_drought[\"SPI48\"] - spi_ds[\"SPI_48m\"]).rename(\"diff\")\n",
    "spi_diff_mean = spi_diff.mean(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D case\n",
    "spi_diff_mean.plot(cmap=\"viridis\", robust=True)\n",
    "plt.title(\"SPI Mean Diff across all time\")\n",
    "plt.show()\n",
    "\n",
    "# # 3D: pick a time slice\n",
    "# spi_diff.isel(time=0).plot(cmap=\"viridis\", robust=True)\n",
    "# plt.title(f\"SPI diff at {str(spi_diff.time.values[0])}\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-3)=\n",
    "###  Calculating ERA5-Drought SPEI from reanalysis origin data.\n",
    "The steps to calculating the SPEI-index are exactly the same as calculating the SPI index, with the only modification being that the SPEI integrates both the precipitation and potential evapotranspiration (PET) data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "#### 0. Importing monthly-average potential evaporation data.\n",
    "\n",
    "We import the monthly-mean potential evaporation data from the \"ERA5 monthly-averaged data on single levels from 1940 to present\" dataset.(https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means?tab=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pev_rean = mars_request(stream=\"moda\", var = \"pev\") # edmo == ensemble, moda == reanalysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### 1. Calculate moving average for different accumulation periods (PET/PEV & TP).\n",
    "\n",
    "A time series of both precipitation & potential evaporation (PET/PEV) from one grid point are extracted and the precipitation & potential evaporation are accumulated over the previous $n$ months using a moving window, analogous to the SPI.\n",
    "\n",
    "Note, [ECMWF convention](https://codes.ecmwf.int/grib/param-db/182) is that negative values for PEV/PET indicate evaporation, whereas positive values indicate condensation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select precipitation data.\n",
    "point_ds = era5_monthly_mean_reanal.sel(valid_time=slice(\"1940-01-01\", \"2020-12-31\"))\n",
    "point_ds = point_ds.sel(latitude= 9.25, longitude= 40.5)\n",
    "monthly_sum_tp_reanal = accum_var(point_ds, var = \"tp\")\n",
    "\n",
    "# Select potential evaporation data.\n",
    "pev_rean = pev_rean.sel(\n",
    "    latitude=9.25, longitude=40.5, method='nearest'\n",
    ").sel(valid_time=slice(\"1940-01-01\", \"2020-12-31\"))\n",
    "\n",
    "monthly_sum_pev_reanal = accum_var(pev_rean, var=\"pev\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for p in accum_periods:\n",
    "    var_name = f'pev_mm_accum_{p}m'\n",
    "    plt.plot(monthly_sum_pev_reanal['valid_time'].values, monthly_sum_pev_reanal[var_name].values, label=f'{p}-month')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Potential Evaporation Accumulation at (9.25Â°N, 40.5Â°E)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PEV (mm)')\n",
    "plt.legend(title='Accumulation Period')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "#### Plot of the accumulated water balance (P-PET).\n",
    "Since the ECMWF convention is that a negative value of PET indicates evaporation, care must be taken when subtracting the precipitation by the potential evaporation (PEV/PET).\n",
    "\n",
    "The negative of PEV/PET must be applied, meaning that $P âˆ’ (âˆ’PET) = P + PET$ in this case. \n",
    "\n",
    "This approach aligns with the definition of the water balance, where a negative water balance value indicates that more water is potentially being transferred to the atmosphere. It also makes sense when interpreting the accumulated plots of both potential evaporation and water balance in a country like Ethiopia (below), where one might expect more evaporation and in fact has a negative water balance (as is the case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_p = monthly_sum_tp_reanal\n",
    "ds_e = monthly_sum_pev_reanal\n",
    "\n",
    "# Map matching period variable names between the two datasets\n",
    "pairs = {\n",
    "    '1m':  ('tp_mm_accum_1m',  'pev_mm_accum_1m'),\n",
    "    '3m':  ('tp_mm_accum_3m',  'pev_mm_accum_3m'),\n",
    "    '6m':  ('tp_mm_accum_6m',  'pev_mm_accum_6m'),\n",
    "    '12m': ('tp_mm_accum_12m', 'pev_mm_accum_12m'),\n",
    "    '24m': ('tp_mm_accum_24m', 'pev_mm_accum_24m'),\n",
    "    '36m': ('tp_mm_accum_36m', 'pev_mm_accum_36m'),\n",
    "    '48m': ('tp_mm_accum_48m', 'pev_mm_accum_48m'),\n",
    "}\n",
    "\n",
    "wb_vars = {}\n",
    "\n",
    "for k, (p_name, e_name) in pairs.items():\n",
    "    if (p_name in ds_p) and (e_name in ds_e):\n",
    "        da_p = ds_p[p_name]\n",
    "        da_e = ds_e[e_name]           # negative totals\n",
    "\n",
    "        # Align by labels to avoid mismatches\n",
    "        da_p, da_e = xr.align(da_p, da_e, join='inner')\n",
    "\n",
    "        # Convert PEV to positive PET and compute water balance\n",
    "        pet_pos = -da_e\n",
    "        wb      = (da_p - pet_pos).rename(f'wb_mm_accum_{k}')\n",
    "\n",
    "        wb_vars[wb.name] = wb\n",
    "\n",
    "# Build a dataset of water-balance variables\n",
    "wb_ds = xr.Dataset(wb_vars, coords=da_p.coords)\n",
    "    \n",
    "# # Select the grid point and time slice for Ethiopia\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for p in accum_periods:\n",
    "    var_name = f'wb_mm_accum_{p}m'\n",
    "    plt.plot(wb_ds['valid_time'].values, wb_ds[var_name].values, label=f'{p}-month')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Total Precipitation - (- Potential Evaporation) at (9.25Â°N, 40.5Â°E)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('TP - PEV (mm)')\n",
    "plt.legend(title='Accumulation Period')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### 2. Fit Generalised Log-Logistic Distribution to PET.\n",
    "\n",
    "The general log-logistic distribution [reference] is fitted only to the data within the reference period (1991-2020), similar to the gamma distribution being fitted in the calculation of the SPI-index. A separate distribution is fitted for each calendar month per accumulation window.\n",
    "\n",
    "This fitting is done with the [scipy.stats.genlogistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.genlogistic.html) object in Python. Three parameters are then outputted per calendar month, per accumulation period: the shape (), location () and scale () parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit log logistic distributions\n",
    "log_params = fit_monthly_distributions_xr(wb_ds, accum_periods, start_ref, end_ref, var =  \"wb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "#### 3. Compute SPEI series.\n",
    "\n",
    "Similar to the calculation for the SPI index, the three parameters from the fitted general log-logistic distribution are taken and the cumulative distribution function (CDF) is calculated up to the accumulated water balance, from the probability distribution function (PDF).\n",
    "\n",
    "As $ P âˆ’ PET $ is barely identical to 0, no modifications analogous to the SPI such as adjusting for zero precipitation, are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SPI series\n",
    "spei_ds, cdf_ds = compute_monthly_series_xr(wb_ds, accum_periods, log_params, var_dim = \"wb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### 4. Quality control using the Shapiro-Wilks test on calculated SPEI data.\n",
    "\n",
    "Shapiro-Wilks test is performed on the calculated SPEI index below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "spei_shapiro_results = shapiro_monthly_test(spei_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "##### Importing Shapiro-Wilks SPI Significance Test results from ERA5-Drought dataset.\n",
    "Quality flags ar imported from ERA5-Drought, for every calendar month, in the reference period, per accumulation period for the SPEI index.\n",
    "\n",
    "Due to improper formatting from ERA5-Drought, you are unable to send an API request to download this data for all accumulation periods at once. If you try to do so, you will find that your xarray only contains the data for the last accumulation period (in this case, the 48-month window). \n",
    "\n",
    "We have written a simple script that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spei_all = era5_drought_api_multiple(indicator = \"spei\", var = \"quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "##### Comparing Calculated Shapiro-Wilks SPI Significance Test with that from ERA5-Drought dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select quality flags at one location and compare that with dataset.\n",
    "quality_spei_all_ds = quality_spei_all.sel(lat=9.25,lon=40.5, method=\"nearest\")\n",
    "\n",
    "for p in accum_periods:\n",
    "    quality_accum  = quality_spei_all_ds[f\"significance_{p}\"]\n",
    "    calculated_accum = spei_shapiro_results[spei_shapiro_results[\"SPI\"]==p]\n",
    "    for m in range(1,13):\n",
    "        quality_month = quality_accum.where(quality_accum.time.dt.month.isin(m), drop=True)\n",
    "        calculated_month = calculated_accum[calculated_accum[\"Month\"]== m]\n",
    "\n",
    "        if quality_month.values.item() != calculated_month[\"Normality (1=Normal)\"].iloc[0]:\n",
    "            print(\"Discrepancy in quality\")\n",
    "        else:\n",
    "            pass\n",
    "    print(f\"Quality indicators for SPI-{p} match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "#### Comparison of the calculated SPEI-index vs ERA5-Drought SPEI-index (qualatitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "request1 = {\n",
    "    \"variable\": [\"standardised_precipitation_evapotranspiration_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\", \"1981\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75]\n",
    "}\n",
    "\n",
    "request2 = {\n",
    "    \"variable\": [\"standardised_precipitation_evapotranspiration_index\"],\n",
    "    \"accumulation_period\": [\n",
    "        \"1\",\n",
    "        \"3\",\n",
    "        \"6\",\n",
    "        \"12\",\n",
    "        \"24\",\n",
    "        \"36\",\n",
    "        \"48\"\n",
    "    ],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1982\", \"1983\", \"1984\",\n",
    "        \"1985\", \"1986\", \"1987\",\n",
    "        \"1988\", \"1989\", \"1990\",\n",
    "        \"1991\", \"1992\", \"1993\",\n",
    "        \"1994\", \"1995\", \"1996\",\n",
    "        \"1997\", \"1998\", \"1999\",\n",
    "        \"2000\", \"2001\", \"2002\",\n",
    "        \"2003\", \"2004\", \"2005\",\n",
    "        \"2006\", \"2007\", \"2008\",\n",
    "        \"2009\", \"2010\", \"2011\",\n",
    "        \"2012\", \"2013\", \"2014\",\n",
    "        \"2015\", \"2016\", \"2017\",\n",
    "        \"2018\", \"2019\", \"2020\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\": [9.45, 40.25, 8.95, 40.75]\n",
    "}\n",
    "\n",
    "data_spei = ekd.from_source(\"cds\", dataset, request1,request2) # Sends request for this dataset to CDS.\n",
    "data_spei = data_spei.to_xarray(compat=\"equals\") # Converts to xarray.\n",
    "data_spei = data_spei.sel(lat=9.25,lon=40.5, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x4 grid of subplots\n",
    "fig, axs = plt.subplots(4, 2, figsize=(16, 12), sharex=True, sharey=True, constrained_layout=True)\n",
    "fig.suptitle('ERA5-Drought SPEI vs Calculated SPEI for varying windows.', fontsize=16)\n",
    "\n",
    "data_spei_focus, spei_ds_focus = xr.align(data_spei, spei_ds, join = \"override\")   # only overlapping coords\n",
    "\n",
    "# Flatten axs for easy indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "for position, period in enumerate(accum_periods):\n",
    "    ax = axs[position]\n",
    "\n",
    "    # Plot ERA5 drought SPI\n",
    "    ax.plot(data_spei_focus[f\"SPEI{period}\"].time,\n",
    "            data_spei_focus[f\"SPEI{period}\"],\n",
    "            label=f\"ERA5_Drought-SPEI{period}\",\n",
    "            color=\"tab:blue\")\n",
    "\n",
    "    # Plot calculated SPI\n",
    "    ax.plot(spei_ds_focus[f\"SPEI_{period}m\"].valid_time,\n",
    "            spei_ds_focus[f\"SPEI_{period}m\"],\n",
    "            label=f\"Calculated SPEI{period}\",\n",
    "            color=\"tab:orange\")\n",
    "\n",
    "    # Title for each subplot\n",
    "    ax.sharex(axs[0])\n",
    "    ax.set_title(f\"{period}-Month Window\", fontsize=10)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('SPEI')    \n",
    "    ax.grid(True)\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(accum_periods), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "# Add a single legend for the whole figure\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper left\", ncol=2, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### Comparison of the calculated SPEI-index vs ERA5-Drought SPEI-index (quantatitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "spei_ds_focus = spei_ds.rename({\n",
    "    \"valid_time\": \"time\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\",\n",
    "}) # for alignment\n",
    "\n",
    "data_spei_focus, spei_ds_focus = xr.align(data_spei_focus, spei_ds_focus, join = \"override\")   # only overlapping coords\n",
    "\n",
    "# Create a 4x4 grid of subplots\n",
    "fig, axs = plt.subplots(4, 2, figsize=(16, 12), sharex=True, sharey=False, constrained_layout=True)\n",
    "fig.suptitle('Residual SPEI for varying windows.', fontsize=16)\n",
    "\n",
    "# Flatten axs for easy indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "for position, period in enumerate(accum_periods):\n",
    "    ax = axs[position]\n",
    "    residual  = spei_ds_focus[f\"SPEI_{period}m\"] - data_spei_focus[f\"SPEI{period}\"]\n",
    "    # Plot residual between ERA5-SPI & Calculated-SPI\n",
    "    ax.plot(residual.time,\n",
    "            residual,\n",
    "            label=f\"Residual-SPEI\",\n",
    "            color=\"tab:blue\")\n",
    "\n",
    "    # Title for each subplot\n",
    "    ax.sharex(axs[0])\n",
    "    ax.set_title(f\"{period}-Month Window\", fontsize=10)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Residual SPEI')    \n",
    "    ax.grid(True)\n",
    "    # ax.set_ylim([-1e-5, 1e-5])\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(accum_periods), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "# Add a single legend for the whole figure\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper left\", ncol=2, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### Comparison of the calculated SPEI-index vs ERA5-Drought SPEI-index with quality flags (quantatitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "for period in accum_periods:\n",
    "    drought_spi = data_spei_focus[f\"SPEI{period}\"].copy()\n",
    "    calc_spi = spei_ds_focus[f\"SPEI_{period}m\"].copy() # copy so it doesn't change.\n",
    "    \n",
    "    for month in range(1,13):        \n",
    "        significance = quality_spei_all_ds[f\"significance_{period}\"].sel(time=f\"2020-{month:02d}-01\").compute().item()\n",
    "        if significance == 0:\n",
    "            drought_spi = drought_spi.where(drought_spi.time.dt.month != month, other=np.nan) # keeps EVERY other month- the selected one becomes nan. \n",
    "            calc_spi = calc_spi.where(calc_spi.time.dt.month != month, other=np.nan)\n",
    "        else:\n",
    "            pass \n",
    "    \n",
    "    data_spei_focus[f\"SPEI{period}\"] = drought_spi # bring it back.\n",
    "    adjusted_spi_ds[f\"SPEI_{period}m\"] = calc_spi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_periods = [1, 3, 6, 12, 24, 36, 48]\n",
    "\n",
    "# data_spei_focus, spei_ds_focus = xr.align(data_spei_focus, spei_ds_focus, join = \"override\")   # only overlapping coords\n",
    "\n",
    "# Create a 4x4 grid of subplots\n",
    "fig, axs = plt.subplots(4, 2, figsize=(16, 12), sharex=True, sharey=False, constrained_layout=True)\n",
    "fig.suptitle('Residual SPEI for varying windows w/ quality flags.', fontsize=16)\n",
    "\n",
    "# Flatten axs for easy indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "for position, period in enumerate(accum_periods):\n",
    "    ax = axs[position]\n",
    "    residual  = adjusted_spi_ds[f\"SPEI_{period}m\"] - data_spei_focus[f\"SPEI{period}\"]\n",
    "    # Plot residual between ERA5-SPI & Calculated-SPI\n",
    "    ax.plot(residual.time,\n",
    "            residual,\n",
    "            label=f\"Residual-SPEI\",\n",
    "            color=\"tab:blue\")\n",
    "\n",
    "    # Title for each subplot\n",
    "    ax.sharex(axs[0])\n",
    "    ax.set_title(f\"{period}-Month Window\", fontsize=10)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Residual SPEI')    \n",
    "    ax.grid(True)\n",
    "    # ax.set_ylim([-1e-5, 1e-5])\n",
    "\n",
    "# Remove unused axes\n",
    "for i in range(len(accum_periods), len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "# Add a single legend for the whole figure\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper left\", ncol=2, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-4)=\n",
    "### 4. Playing with ERA5 SPI - Ensemble \n",
    "\n",
    "#### Subsections\n",
    "Describe what is done in this step/section and what the `code` in the cell does(if code is included)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Importing SPI-12 Index from ERA5 ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_ens_dataset = \"derived-drought-historical-monthly\"\n",
    "spi_ens_request = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\"12\"], # 1 is also already downloaded.\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"ensemble_members\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\", \"1981\",\n",
    "        \"1982\", \"1983\", \"1984\",\n",
    "        \"1985\", \"1986\", \"1987\",\n",
    "        \"1988\", \"1989\", \"1990\",\n",
    "        \"1991\", \"1992\", \"1993\",\n",
    "        \"1994\", \"1995\", \"1996\",\n",
    "        \"1997\", \"1998\", \"1999\",\n",
    "        \"2000\", \"2001\", \"2002\",\n",
    "        \"2003\", \"2004\", \"2005\",\n",
    "        \"2006\", \"2007\", \"2008\",\n",
    "        \"2009\", \"2010\", \"2011\",\n",
    "        \"2012\", \"2013\", \"2014\",\n",
    "        \"2015\", \"2016\", \"2017\",\n",
    "        \"2018\", \"2019\", \"2020\",\n",
    "        \"2021\", \"2022\", \"2023\",\n",
    "        \"2024\", \"2025\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"area\":[10, 40, 9, 41]\n",
    "}\n",
    "\n",
    "point_drought_ens_ = ekd.from_source(\"cds\", spi_ens_dataset, spi_ens_request) # Sends request for this dataset to CDS.\n",
    "point_drought_ens_ = point_drought_ens_.to_xarray(compat=\"equals\") # Converts to xarray.\n",
    "point_drought_ens_ = point_drought_ens_.sel(lat = 9.25, lon = 40.5) # note there are duplicate timestamps. each one is a ensemble member."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Give each ensemble member unique identifier (one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ens(dataset, no_ens = 10):\n",
    "    _, index = np.unique(dataset['time'], return_index=True)\n",
    "    \n",
    "    ens_dataset = []\n",
    "    \n",
    "    for i in range(1,no_ens+1):\n",
    "        ens_member = dataset.isel(time = index + i)    \n",
    "        ens_dataset.append(ens_member)\n",
    "        \n",
    "    drought_ens = xr.concat(ens_dataset, dim=\"number\")\n",
    "    drought_ens = drought_ens.assign_coords(number=np.arange(1,11))  # or 1..10 if you prefer\n",
    "\n",
    "    return drought_ens\n",
    "    \n",
    "drought_ens = create_ens(point_drought_ens_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_ens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plotting each ensemble member vs time (one location, SPI-12) w/ thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = drought_ens['SPI12']  # select the DataArray\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "da.transpose('time', 'number').plot.line(x='time', hue='number', add_legend=True,\n",
    "    figsize=(12, 5),\n",
    ")\n",
    "plt.hlines(y=[-1.5, 1.5], xmin=drought_ens_mean['time'].values.min(), xmax=drought_ens_mean['time'].values.max(), colors='r', linestyles='--', \n",
    "           label = \"Thresholds\")\n",
    "# Customize plot\n",
    "plt.title('SPI12 â€” Ensemble members over time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('SPI-12')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-complete\"\n",
    "request = {\n",
    "    \"class\": \"ea\",\n",
    "    \"date\": \"20000101\",\n",
    "    \"expver\": \"1\",\n",
    "    \"levtype\": \"sfc\",\n",
    "    \"param\": \"228.128\",\n",
    "    \"stream\": \"moda\",\n",
    "    'grid'    : '0.25/0.25',\n",
    "    \"type\": \"fc\",\n",
    "    \"data_format\": \"netcdf\" \n",
    "}\n",
    "\n",
    "mars_pev_rean = ekd.from_source(\"cds\", dataset, request)  # sends the request to CDS\n",
    "mars_pev_rean = mars_pev_rean.to_xarray()  # convert to xarray.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_pev_rean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-complete\"\n",
    "request = {\n",
    "    \"class\": \"ea\",\n",
    "    \"date\": \"/\".join(f\"{year}{month:02}01\" for year in range(1940,2021) for month in range(1,13)),\n",
    "    \"expver\": \"1\",\n",
    "    \"levtype\": \"sfc\",\n",
    "    \"param\": \"228.128\",\n",
    "    \"stream\": \"moda\",\n",
    "    'grid'    : '0.25/0.25',\n",
    "    \"type\": \"fc\",\n",
    "    \"data_format\": \"netcdf\" \n",
    "}\n",
    "\n",
    "mars_pev_rean = ekd.from_source(\"cds\", dataset, request)  # sends the request to CDS\n",
    "mars_pev_rean = mars_pev_rean.to_xarray()  # convert to xarray.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_pev_rean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/\".join(f\"{year}{month:02}01\" for year in range(1940,2021) for month in range(1,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_pev_rean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_pev_rean[\"tp\"].sel(longitude = slice(40,41), latitude = slice(10,9), valid_time=\"2000-01-01T06:00:00\").plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot ensemble mean & standard dev (one location, SPI-1) w/ Thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_ens_mean = drought_ens.mean(dim=\"number\")\n",
    "drought_ens_std = drought_ens.std(dim=\"number\")/np.sqrt(10-1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(drought_ens_mean['time'].values, drought_ens_mean[\"SPI12\"].values, label=f'SPI12')\n",
    "\n",
    "# Two threshold lines\n",
    "plt.hlines(y=[-1.5, 1.5],\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='r', linestyles='--', label='Extreme Wet/Dry Thresholds')\n",
    "\n",
    "# Mean line\n",
    "mean_spi12 = drought_ens_mean[\"SPI12\"].mean().values\n",
    "plt.hlines(y=mean_spi12,\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='b', linestyles='--', label='Mean SPI12 across time.')\n",
    "\n",
    "\n",
    "plt.title('Ensemble mean of SPI12 vs Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('SPI-12 Time Series')\n",
    "plt.legend(title='12-month accumulation period')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Number of SPI greater than threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = drought_ens[\"SPI12\"]  # dims: ('time',)\n",
    "\n",
    "threshold_mag = 2\n",
    "\n",
    "exceeds = np.abs(da) > threshold_mag # boolean xarray with all values that are \"severely\" wet/drier than usual.\n",
    "\n",
    "count_per_time = exceeds.sum(dim='number') # count number that exceed along the ensemble dimension.\n",
    "\n",
    "fractional_count_per_time = count_per_time / 10\n",
    "\n",
    "percent_per_time = fractional_count_per_time*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of Mean, Stdev & P( > Severe Threshold ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# --- Top panel: % members exceeding ---\n",
    "percent_per_time.plot(ax=ax1, color='tab:purple')\n",
    "ax1.set_title(f'% ensemble members with |SPI12| > {threshold_mag} per time')\n",
    "ax1.set_ylabel('Percent (%)')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- 2nd panel: Ensemble band ---\n",
    "ci = drought_ens_std['SPI12'].values*np.sqrt(10-1) # confidence interval\n",
    "y = drought_ens_mean['SPI12'].values\n",
    "t = drought_ens_mean['time'].values\n",
    "\n",
    "ax2.plot(t,y,label='SPI12 Mean', color='tab:blue')\n",
    "\n",
    "# Horizontal thresholds Â±2\n",
    "ax2.hlines(y=[-threshold_mag, threshold_mag],\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='r', linestyles='--', label='Extreme Wet/Dry Thresholds')\n",
    "\n",
    "ax2.set_title('Mean SPI12 vs Time')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Mean SPI-12')\n",
    "ax2.legend(title='12-month accumulation period')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# --- 3nd panel: Ensemble band ---\n",
    "ci = drought_ens_std['SPI12'].values*np.sqrt(10-1) # confidence interval\n",
    "y = drought_ens_mean['SPI12'].values\n",
    "t = drought_ens_mean['time'].values\n",
    "\n",
    "# ax2.plot(t,y.values,label='SPI1 Mean', color='tab:blue')\n",
    "ax3.fill_between(t, (y-ci), (y+ci), color='b')\n",
    "\n",
    "# Horizontal thresholds Â±1.5\n",
    "ax3.hlines(y=[-threshold_mag, threshold_mag],\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='r', linestyles='--', label='Extreme Wet/Dry Thresholds')\n",
    "\n",
    "ax3.set_title('Mean SPI12 vs Time with St.dev Band')\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Mean SPI-12')\n",
    "ax3.legend(title='12-month accumulation period')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Heat map of probability exceeding threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "t = percent_per_time['time'].values\n",
    "y = percent_per_time.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2.8))\n",
    "\n",
    "# Create a 2D array with one \"row\" so imshow can display it\n",
    "heat = y[None, :]  # shape (1, T)\n",
    "\n",
    "# Use extent to align x with datetime range\n",
    "t_num = mdates.date2num(t)\n",
    "extent = (t_num.min(), t_num.max(), 0, 1)\n",
    "\n",
    "im = ax.imshow(\n",
    "    heat,\n",
    "    aspect='auto',\n",
    "    cmap='RdBu_r',\n",
    "    vmin=0, vmax=100,\n",
    "    extent=extent\n",
    ")\n",
    "\n",
    "ax.set_yticks([])  # hide vertical axis (it's a strip)\n",
    "ax.set_title('% ensemble members with |SPI12| > 1.5 (timeline color strip)')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "# Ticks on time\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax, pad=0.02)\n",
    "cbar.set_label('Percent exceeding (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of |Mean(SPI)| > 1.5  & P( |SPI| > 1.5 ) > 0.5 \n",
    "\n",
    "####  P( |SPI| > 1.5 )  calculated from # of ensemble members greater than threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = drought_ens_mean[\"SPI12\"]  # dims: ('time',)\n",
    "\n",
    "probability_mag = 0.5 \n",
    "\n",
    "cond_both = (np.abs(da > threshold_mag) & (fractional_count_per_time > probability_mag))\n",
    "\n",
    "da_hit = da.sel(time=cond_both)\n",
    "\n",
    "timestamps = da_hit.time.values\n",
    "\n",
    "print(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Histogram ensemble standard dev vs # quality flag (one location, one window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drought_ens_std[\"SPI12\"]\n",
    "sig = quality_spi_all[\"significance_12\"]\n",
    "\n",
    "thresholds = np.arange(0.1, 1.4, 0.1)  # 0.1 .. 1.5\n",
    "bad_counts_cum = np.zeros_like(thresholds, dtype=int)\n",
    "good_counts_cum = np.zeros_like(thresholds, dtype=int)\n",
    "\n",
    "for month in range(1,13):\n",
    "        significance = quality_spi_all[f\"significance_1\"].sel(time=f\"2020-{month:02d}-01\").compute().item()\n",
    "        if significance == 0:\n",
    "            vals = data.where(data.time.dt.month == month, drop=True).values\n",
    "            bad_counts_cum += (vals[:, None] < thresholds[None, :]).sum(axis=0) # then sum along column.\n",
    "            # bad_counts_cum[j] = number of vals that are < thresholds[j]\n",
    "            # B[i, j] = (vals[i] < thresholds[j])\n",
    "        else:\n",
    "            vals = data.where(data.time.dt.month == month, drop=True).values\n",
    "            good_counts_cum += (vals[:, None] < thresholds[None, :]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_bins  = np.diff(np.concatenate(([0], bad_counts_cum)))\n",
    "good_bins = np.diff(np.concatenate(([0], good_counts_cum)))\n",
    "total_bins = bad_bins + good_bins\n",
    "good_norm = good_bins / total_bins\n",
    "bad_norm = bad_bins / total_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(thresholds, good_norm, width = 0.1, color='blue', alpha=0.8, label='Quality = 1 (good)')\n",
    "ax.bar(thresholds, bad_norm, width = 0.1,  color='red', bottom = good_norm, alpha=0.9, label='Quality = 0 (bad)')\n",
    "\n",
    "ax.set_xlabel('Uncertainty in SPI1 (0.1 bin)')\n",
    "ax.set_ylabel('% count of good/bad months (stacked)')\n",
    "ax.set_title('% counts (stacked) for each uncertainty bin')\n",
    "ax.set_xticks(thresholds)\n",
    "ax.set_xticklabels([f'{t:.1f}' for t in thresholds])\n",
    "ax.grid(True, linestyle='--', alpha=0.35)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Reading in ERA5 SPI- Ensemble (global, 2 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "request = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\"48\"],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"ensemble_members\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"2023\",\n",
    "        \"2024\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drought_ens = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "data_drought_ens = data_drought_ens.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Function to create the data into ensemble set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reshape time into (time=24, ensemble=10)\n",
    "def make_ensemble(spi):\n",
    "    arr = np.asarray(spi)  # Convert memoryview or dask chunk to NumPy\n",
    "    reshaped = arr.reshape(24, 10)\n",
    "    return reshaped\n",
    " \n",
    "# Apply across all lat/lon\n",
    "ensemble = xr.apply_ufunc(\n",
    "    make_ensemble,\n",
    "    data_drought_ens[\"SPI48\"],  # DataArray with dims (time, lat, lon)\n",
    "    input_core_dims=[[\"time\"]],\n",
    "    output_core_dims=[[\"time\", \"ensemble\"]],\n",
    "    exclude_dims={\"time\"},\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    "    output_sizes={\"time\": 24, \"ensemble\": 10},\n",
    ")\n",
    " \n",
    "# Assign coordinates\n",
    "ensemble = ensemble.assign_coords({\n",
    "    \"time\": pd.date_range(\"2023-01-01\", periods=24, freq=\"MS\"),\n",
    "    \"ensemble\": range(1, 11)\n",
    "})\n",
    "ensemble.name = \"SPI48\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Selecting ensemble member 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.sel(ensemble=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_values = ensemble.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculating mean and stdev over all ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean = ensemble.mean(dim=\"ensemble\", skipna=True)  # shape: (lat, lon, time)\n",
    "ensemble_std = ensemble.std(dim=\"ensemble\", skipna=True)  # shape: (lat, lon, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean_time = abs(ensemble_mean).mean(dim=\"time\", skipna=True)  # shape: (lat, lon, time)\n",
    "ensemble_std_time = ensemble_std.std(dim=\"time\", skipna=True)  # shape: (lat, lon, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_std = ensemble_std.compute()  # Load into memory for plotting\n",
    "ensemble_std_time = ensemble_std_time.compute() \n",
    "\n",
    "ensemble_mean = ensemble_mean.compute()  # Load into memory for plotting\n",
    "ensemble_mean_time = ensemble_mean_time.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of mean vs stdev over all ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one time slice and one ensemble\n",
    "from earthkit.plots.styles import Style\n",
    "\n",
    "global_std_map = ensemble_std.sel(time=\"2024-05-01\")  # shape (lat, lon)\n",
    "global_mean_map = ensemble_mean.sel(time=\"2024-05-01\")\n",
    "\n",
    "# global_std_map = ensemble_std_time\n",
    "# global_mean_map = ensemble_mean_time\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "\n",
    "std_values = global_std_map.to_numpy()/np.sqrt(10)\n",
    "mean_values = global_mean_map.to_numpy()\n",
    "\n",
    "lat_values = global_mean_map.lat.to_numpy()\n",
    "lon_values = global_mean_map.lon.to_numpy()\n",
    " \n",
    "# Create meshgrid\n",
    "\n",
    "lon_grid, lat_grid = np.meshgrid(lon_values, lat_values)\n",
    " \n",
    "# # Plot with EarthKit\n",
    "\n",
    "SPI_STYLE = Style(cmap='RdBu', vmin = -3, vmax = 3, normalize=False)\n",
    "SPI_SEOM_STYLE = Style(cmap='cividis', vmin = 0, vmax = 0.5, normalize=False) # _r for reversing colorbar.\n",
    "\n",
    "# Create figure with 2 columns\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))  # <-- Important!\n",
    "\n",
    "# First subplot (left)\n",
    "subplot = fig.add_map(domain=\"Australia\", row=0, column=0)\n",
    "subplot.grid_cells(mean_values, x=lon_grid, y=lat_grid,style=SPI_STYLE)\n",
    "subplot.legend(location=\"right\")\n",
    "\n",
    "# Second subplot (right)\n",
    "subplot1 = fig.add_map(domain=\"Australia\", row=0, column=1)\n",
    "subplot1.grid_cells(std_values, x=lon_grid, y=lat_grid, style = SPI_SEOM_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "\n",
    "subplot.title(\"Mean | SPI | \")\n",
    "subplot1.title(\"SPI SEOM\")\n",
    "\n",
    "# Add decorations\n",
    "fig.title(\"SPI Ensemble (N=10) averaged across 2023-2024 month, 1 month accumulation period.\")\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of P( |SPI48| >= 2 ) over entire region, for one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ensemble  # dims: ('time',)\n",
    "\n",
    "threshold_mag = 2\n",
    "\n",
    "exceeds = np.abs(da) >= threshold_mag # boolean xarray with all values that are \"severely\" wet/drier than usual.\n",
    "\n",
    "count_per_time = exceeds.sum(dim='ensemble') # count number that exceed along the ensemble dimension.\n",
    "\n",
    "fractional_count_per_time = count_per_time / 10\n",
    "\n",
    "percent_per_time = fractional_count_per_time*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from earthkit.plots.styles import Style\n",
    "\n",
    "# --- Define mask FIRST ---\n",
    "extreme_mask = (mean_values >= 2) | (mean_values <= -2)\n",
    "\n",
    "# Build a constant-valued mask for the grey layer:\n",
    "# 1 where non-extreme (|SPI| â‰¤ 2), NaN where extreme (so extremes won't be drawn in grey)\n",
    "nonextreme_const = np.where(~extreme_mask, 1.0, np.nan)\n",
    "\n",
    "# For the colored overlay, keep only extreme values (NaN elsewhere)\n",
    "mean_extremes = np.where(extreme_mask, mean_values, np.nan)\n",
    "\n",
    "# --- Styles ---\n",
    "# Single-colour style for non-extremes: ensure a tight 0â€“1 range\n",
    "GREY_STYLE = Style(\n",
    "    cmap=ListedColormap([\"#d9d9d9\"]),\n",
    "    vmin=0, vmax=1,\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# SPI style for extremes; use a symmetric range that suits your data\n",
    "SPI_STYLE = Style(\n",
    "    cmap='RdBu',\n",
    "    vmin=-4, vmax=4,    # adjust to your expected SPI range\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# --- Plot ---\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))\n",
    "\n",
    "# Left subplot: mean SPI48 with grey background for non-extremes\n",
    "subplot = fig.add_map(domain=\"Australia\", row=0, column=0)\n",
    "\n",
    "# Draw grey background first\n",
    "subplot.grid_cells(\n",
    "    nonextreme_const, x=lon_grid, y=lat_grid,\n",
    "    style=GREY_STYLE, zorder=1, alpha=0.8  # alpha to ensure visibility\n",
    ")\n",
    "\n",
    "# Overlay extremes with color\n",
    "subplot.grid_cells(\n",
    "    mean_extremes, x=lon_grid, y=lat_grid,\n",
    "    style=SPI_STYLE, zorder=2\n",
    ")\n",
    "\n",
    "subplot.legend(location=\"right\")\n",
    "\n",
    "\n",
    "subplot.title(\"|Mean SPI48| â‰¥ 2 | (grey = |Mean SPI| â‰¤ 2)\")\n",
    "\n",
    "longitude = [142]\n",
    "latitude = [-29.25]\n",
    "\n",
    "# Plot the point as a scatter plot\n",
    "subplot.scatter(x=longitude, y=latitude, color='red', marker='o', s=100, label = \"Tibooburra\")\n",
    "\n",
    "# Right subplot (unchanged example)\n",
    "SPI_SEOM_STYLE = Style(cmap='cividis', normalize=False)\n",
    "subplot1 = fig.add_map(domain=\"Australia\", row=0, column=1)\n",
    "subplot1.grid_cells(percent_values, x=lon_grid, y=lat_grid, style=SPI_SEOM_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "subplot1.title(\"P(| SPI48 | > 2)\")\n",
    "subplot1.scatter(x=longitude, y=latitude, color='red', marker='o', s=100, label = \"Tibooburra\")\n",
    "\n",
    "fig.title(\"SPI48 Ensemble (N=10) with Tibooburra pointed out.\")\n",
    "fig.land(); fig.coastlines(); fig.borders(); fig.gridlines()\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Analysing uncertainty correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_mean = ensemble_mean_time\n",
    "spi_seom = ensemble_std_time / np.sqrt(10)\n",
    "\n",
    "spi_mean = np.where((np.abs(spi_mean) <= 3) , spi_mean, np.nan)\n",
    "spi_seom = np.where((np.abs(spi_mean) <= 3), spi_seom, np.nan)\n",
    "\n",
    "# Flatten & mask\n",
    "x = spi_mean.ravel()          # SPI mean\n",
    "y = spi_seom.ravel()   # SPI standard uncertainty (N=10)\n",
    "mask = np.isfinite(x) & np.isfinite(y)\n",
    "x = x[mask]; y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 6), dpi=160)\n",
    "plt.scatter(x, y, s=8, alpha=0.5, color=\"#1f77b4\", edgecolors=\"none\")\n",
    "plt.xlabel(\"Mean | SPI-1 |\")\n",
    "plt.ylabel(\"SPI-1 Standard Uncertainty of the mean (N=10)\")\n",
    "plt.title(\"Mean | SPI-1 | vs SPI-1 SUOM for SPI <= 3 across 2023-2024\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Masks (ignore NaNs in x or y)\n",
    "mask_neg = (x < 0) & np.isfinite(x) & np.isfinite(y)\n",
    "mask_pos = (x >= 0) & np.isfinite(x) & np.isfinite(y)\n",
    "\n",
    "n_neg = np.count_nonzero(mask_neg)\n",
    "n_pos = np.count_nonzero(mask_pos)\n",
    "\n",
    "# Compute means; if group empty, set to None\n",
    "mean_y_neg = float(np.nanmean(y[mask_neg])) if n_neg > 0 else None\n",
    "mean_y_pos = float(np.nanmean(y[mask_pos])) if n_pos > 0 else None\n",
    "\n",
    "# Helper for formatting\n",
    "def fmt_mean(m):\n",
    "    return f\"{m:.4f}\" if m is not None and np.isfinite(m) else \"NA\"\n",
    "\n",
    "print(\"Group summaries:\")\n",
    "print(f\"  x < 0    -> n = {n_neg:4d}, mean(y) = {fmt_mean(mean_y_neg)}\")\n",
    "print(f\"  x >= 0   -> n = {n_pos:4d}, mean(y) = {fmt_mean(mean_y_pos)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- your scatter (unchanged) ---\n",
    "# Assign colors based on sign of x\n",
    "colors = np.where(x < 0, 'red', 'blue')\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=160)\n",
    "plt.scatter(\n",
    "    x, y,\n",
    "    s=8,\n",
    "    alpha=0.5,\n",
    "    color=colors,\n",
    "    edgecolors='none'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Mean SPI-1\")\n",
    "plt.ylabel(\"SPI-1 Standard Uncertainty of the mean (N=10)\")\n",
    "plt.title(\"Mean SPI-1 vs SPI-1 SUOM for SPI <= 3 across 2023-2024\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- compute means (ignore NaNs in x or y) ---\n",
    "mask_neg = (x < 0) & np.isfinite(x) & np.isfinite(y)\n",
    "mask_pos = (x >= 0) & np.isfinite(x) & np.isfinite(y)\n",
    "\n",
    "n_neg = int(np.count_nonzero(mask_neg))\n",
    "n_pos = int(np.count_nonzero(mask_pos))\n",
    "\n",
    "mean_y_neg = float(np.nanmean(y[mask_neg])) if n_neg > 0 else None\n",
    "mean_y_pos = float(np.nanmean(y[mask_pos])) if n_pos > 0 else None\n",
    "\n",
    "def fmt_mean(m):\n",
    "    return f\"{m:.4f}\" if (m is not None and np.isfinite(m)) else \"NA\"\n",
    "\n",
    "# --- print on the plot as text boxes ---\n",
    "ax = plt.gca()\n",
    "\n",
    "# Left text box (x < 0)\n",
    "left_text = f\"SPI-1 < 0:\\n  n = {n_neg}\\n  mean(SPI SUOM) = {fmt_mean(mean_y_neg)}\"\n",
    "ax.text(\n",
    "    0.02, 0.98, left_text,\n",
    "    transform=ax.transAxes,\n",
    "    va='top', ha='left',\n",
    "    color='red',\n",
    "    fontsize=10,\n",
    "    bbox=dict(facecolor='white', edgecolor='red', alpha=0.9, boxstyle='round,pad=0.3')\n",
    ")\n",
    "\n",
    "# Right text box (x >= 0)\n",
    "right_text = f\"SPI-1 >= 0:\\n  n = {n_pos}\\n  mean(SPI SUOM) = {fmt_mean(mean_y_pos)}\"\n",
    "ax.text(\n",
    "    0.98, 0.98, right_text,\n",
    "    transform=ax.transAxes,\n",
    "    va='top', ha='right',\n",
    "    color='blue',\n",
    "    fontsize=10,\n",
    "    bbox=dict(facecolor='white', edgecolor='blue', alpha=0.9, boxstyle='round,pad=0.3')\n",
    ")\n",
    "\n",
    "# Optional: vertical reference line at x = 0\n",
    "ax.axvline(0, color='k', linestyle=':', linewidth=1.2, alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of mean ensemble Standard Uncertainty of the Mean vs Time (Australia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# 1) Global mean time series\n",
    "# -----------------------\n",
    "# Simple average over lat/lon for each time\n",
    "global_ts = ensemble_std.mean(dim=(\"lat\", \"lon\"), skipna=True)\n",
    "global_ts.name = \"global_mean_std\"\n",
    "\n",
    "# -----------------------\n",
    "# 2) Australia domain mean time series\n",
    "# -----------------------\n",
    "# Define a rectangular box that covers mainland Australia\n",
    "# Adjust if you want to include territories or use a different extent.\n",
    "AU_LAT_MIN, AU_LAT_MAX = -45.0, -5.0\n",
    "AU_LON_MIN, AU_LON_MAX = 100.0, 160.0\n",
    "\n",
    "# If your longitude coordinate is 0..360, these bounds already work.\n",
    "# If it's -180..180, they also work (Australia is at positive longitudes).\n",
    "\n",
    "lat_sel = (ensemble_std[\"lat\"] >= AU_LAT_MIN) & (ensemble_std[\"lat\"] <= AU_LAT_MAX)\n",
    "lon_sel = (ensemble_std[\"lon\"] >= AU_LON_MIN) & (ensemble_std[\"lon\"] <= AU_LON_MAX)\n",
    "\n",
    "ensemble_std_au = ensemble_std.where(lat_sel & lon_sel, drop=True)\n",
    "aus_ts = ensemble_std_au.mean(dim=(\"lat\", \"lon\"), skipna=True)\n",
    "aus_ts.name = \"australia_mean_std\"\n",
    "\n",
    "# -----------------------\n",
    "# 3) Plot\n",
    "# -----------------------\n",
    "plt.figure(figsize=(10, 5), dpi=140)\n",
    "plt.plot(global_ts[\"time\"], global_ts, label=\"Global mean std\", lw=1.8)\n",
    "plt.plot(aus_ts[\"time\"], aus_ts, label=\"Australia mean std\", lw=1.8)\n",
    "plt.title(\"Mean Ensemble Standard Deviation â€” Global vs Australia (unweighted)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Standard deviation\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "pltplt.show()\n",
    "\n",
    "# -----------------------\n",
    "# 4) Optional: Export as DataFrame\n",
    "# -----------------------\n",
    "df = xr.merge([global_ts, aus_ts]).to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.245188,
     "end_time": "2024-03-08T17:39:21.277354",
     "exception": false,
     "start_time": "2024-03-08T17:39:21.032166",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-5)=\n",
    "### 5. Calculating the SPI/SPEI Ensemble Data\n",
    "\n",
    "#### Results Subsections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Reading in ensemble precipitation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-single-levels-monthly-means\"\n",
    "request = {\n",
    "    \"product_type\": [\"monthly_averaged_ensemble_members\"],\n",
    "    \"variable\": [\"total_precipitation\"],\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\", \"1981\",\n",
    "        \"1982\", \"1983\", \"1984\",\n",
    "        \"1985\", \"1986\", \"1987\",\n",
    "        \"1988\", \"1989\", \"1990\",\n",
    "        \"1991\", \"1992\", \"1993\",\n",
    "        \"1994\", \"1995\", \"1996\",\n",
    "        \"1997\", \"1998\", \"1999\",\n",
    "        \"2000\", \"2001\", \"2002\",\n",
    "        \"2003\", \"2004\", \"2005\",\n",
    "        \"2006\", \"2007\", \"2008\",\n",
    "        \"2009\", \"2010\", \"2011\",\n",
    "        \"2012\", \"2013\", \"2014\",\n",
    "        \"2015\", \"2016\", \"2017\",\n",
    "        \"2018\", \"2019\", \"2020\",\n",
    "        \"2021\", \"2022\", \"2023\",\n",
    "        \"2024\", \"2025\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"time\": [\"00:00\"],\n",
    "    \"data_format\": \"grib\",\n",
    "    \"download_format\": \"unarchived\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_ens = mars_request(stream=\"edmo\", var = \"tp\") # edmo == ensemble, moda == reanalysis\n",
    "# tp_ens = tp_ens.assign_coords(number=tp_ens.number + 1)\n",
    "# tp_ens_sel = tp_ens.sel(latitude=9.5, longitude=40.5, method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "pev_ens = mars_request(stream=\"moda\", var = \"pev\") # edmo == reanalysis, moda == ensemble\n",
    "# tp_ens = tp_ens.assign_coords(number=tp_ens.number + 1)\n",
    "# tp_ens_sel = tp_ens.sel(latitude=9.5, longitude=40.5, method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Selecting drought index from ensemble at one location (lon=9.5, lat = 40.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_drought_ens_ = point_drought_ens_.sel(lon = 9.5, lat = 40.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble(da, time_dim=\"time\", member_dim=\"member\"):\n",
    "    times = pd.Index(da[time_dim].values)\n",
    "    member_ids = pd.Series(times).groupby(times).cumcount().to_numpy()\n",
    "    \n",
    "    # Create MultiIndex\n",
    "    mi = pd.MultiIndex.from_arrays([times, member_ids], names=[time_dim, member_dim])\n",
    "    \n",
    "    # Assign MultiIndex and rename the dimension to something temporary\n",
    "    da = da.rename({time_dim: \"tmp\"}).assign_coords(tmp=mi)\n",
    "    \n",
    "    # Unstack to get (time, member)\n",
    "    return da.unstack()\n",
    "\n",
    "ens = make_ensemble(point_drought_ens_[\"SPI1\"])  # or pass the whole Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Ensemble spread along with 48-month rolling average in SPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ens_std = ens.std(dim = \"member\") / np.sqrt(10)\n",
    "point_ens_mean = ens.mean(dim = \"member\")\n",
    "\n",
    "point_ens_std_rolling = point_ens_mean.rolling(\n",
    "    time=48, center=False, min_periods=48\n",
    ").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a clean time vs stdev plot\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(point_ens_mean.time.values, point_ens_mean.values,\n",
    "         color='tab:blue', lw=1.3, alpha=0.45, label='Mean (monthly)')\n",
    "plt.plot(point_ens_std_rolling.time.values, point_ens_std_rolling.values,\n",
    "         color='tab:green', lw=2.4, marker='o', label='4-year mean (calendar)')\n",
    "\n",
    "plt.xlabel(\"Forecast reference time\")\n",
    "plt.ylabel(\"Mean across ensemble (SPI)\")\n",
    "plt.title(\"Ensemble mean of SPI over time (yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute rolling mean for the ensemble mean SPI (same window as stdev)\n",
    "point_ens_mean_rolling = point_ens_mean.rolling(\n",
    "    time=3, center=False, min_periods=3\n",
    ").mean()\n",
    "\n",
    "# Create side-by-side subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 5), sharex=True)\n",
    "\n",
    "# --- Left: Std. dev over time ---\n",
    "axes[0].plot(\n",
    "    point_ens_std.time.values, point_ens_std.values,\n",
    "    color='tab:blue', lw=1.3, alpha=0.45, label='Std. dev (monthly)'\n",
    ")\n",
    "axes[0].plot(\n",
    "    point_ens_std_rolling.time.values, point_ens_std_rolling.values,\n",
    "    color='tab:green', lw=2.4, marker='o', label='4-year stdev mean (calendar)'\n",
    ")\n",
    "axes[0].set_xlabel(\"Forecast reference time\")\n",
    "axes[0].set_ylabel(\"Std. dev across ensemble (SPI)\")\n",
    "axes[0].set_title(\"Ensemble spread of SPI over time\\n(yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# --- Right: Mean SPI over time ---\n",
    "axes[1].plot(\n",
    "    point_ens_mean.time.values, point_ens_mean.values,\n",
    "    color='tab:orange', lw=1.3, alpha=0.55, label='Mean SPI (monthly)'\n",
    ")\n",
    "axes[1].plot(\n",
    "    point_ens_mean_rolling.time.values, point_ens_mean_rolling.values,\n",
    "    color='tab:red', lw=2.4, marker='o', label='4-year mean (calendar)'\n",
    ")\n",
    "axes[1].set_xlabel(\"Forecast reference time\")\n",
    "axes[1].set_ylabel(\"Mean SPI\")\n",
    "axes[1].set_title(\"Ensemble mean SPI over time\\n(yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rolling mean for ensemble mean SPI (same window as stdev)\n",
    "point_ens_mean_rolling = point_ens_mean.rolling(\n",
    "    time=3, center=False, min_periods=3\n",
    ").mean()\n",
    "\n",
    "fig, ax_left = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# --- Left y-axis: Std. dev ---\n",
    "lns1 = ax_left.plot(\n",
    "    point_ens_std.time.values, point_ens_std.values,\n",
    "    color='tab:blue', lw=1.3, alpha=0.45, label='Std. dev (monthly)'\n",
    ")\n",
    "lns2 = ax_left.plot(\n",
    "    point_ens_std_rolling.time.values, point_ens_std_rolling.values,\n",
    "    color='tab:green', lw=2.0, marker='o', label='Std. dev 4-year mean'\n",
    ")\n",
    "ax_left.set_ylabel(\"Std. dev across ensemble (SPI)\", color='tab:blue')\n",
    "ax_left.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax_left.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Right y-axis: Mean SPI ---\n",
    "ax_right = ax_left.twinx()\n",
    "lns3 = ax_right.plot(\n",
    "    point_ens_mean.time.values, point_ens_mean.values,\n",
    "    color='tab:orange', lw=1.3, alpha=0.55, label='Mean SPI (monthly)'\n",
    ")\n",
    "lns4 = ax_right.plot(\n",
    "    point_ens_mean_rolling.time.values, point_ens_mean_rolling.values,\n",
    "    color='tab:red', lw=2.0, marker='o', label='Mean SPI 4-year mean'\n",
    ")\n",
    "ax_right.set_ylabel(\"Mean SPI\", color='tab:orange')\n",
    "ax_right.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "# --- Shared x-axis and title ---\n",
    "ax_left.set_xlabel(\"Forecast reference time\")\n",
    "ax_left.set_title(\"Std. dev vs Mean SPI over time (two y-axes)\\n(lon, lat) : (40.5, 9.5)\")\n",
    "\n",
    "# --- Unified legend combining both axes ---\n",
    "lines = lns1 + lns2 + lns3 + lns4\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax_left.legend(lines, labels, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ens_mean = ens.mean(dim=\"member\")\n",
    "point_ens_stdev = ens.std(dim=\"member\")  # replace with SEM if preferred\n",
    "\n",
    "t = point_ens_mean.time.values\n",
    "y = point_ens_mean.values\n",
    "spread = point_ens_stdev.values  # or SEM\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Mean line\n",
    "plt.plot(t, y, color='tab:orange', lw=1.8, label='Mean SPI')\n",
    "\n",
    "# Shaded band for uncertainty\n",
    "plt.fill_between(\n",
    "    t, y - spread, y + spread,\n",
    "    color='tab:blue', alpha=0.2, label='Â± stdev across ensemble'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Forecast reference time\")\n",
    "plt.ylabel(\"Mean SPI\")\n",
    "plt.title(\"Ensemble mean SPI with Â± stdev band over time\\n(lon, lat): (40.5, 9.5)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([-3, 3])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Ensemble spread of total precipitation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stdev across ensemble members (dimension: 'number')\n",
    "tp_ens_sel = tp_ens_sel.sel(forecast_reference_time = slice('1940-01-31T18:00:00.000000000','2025-09-30T18:00:00.000000000'))\n",
    "tp_std = tp_ens_sel['tp'].std(dim='number')  # result dims: (forecast_reference_time)\n",
    "tp_mean = tp_ens_sel['tp'].mean(dim='number')  # result dims: (forecast_reference_time)\n",
    "\n",
    "# Make a clean time vs stdev plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(point_ens_mean.values, point_ens_std.values, color='tab:blue')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "#### Ensemble spread along with 48-month rolling average in tp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stdev across ensemble members\n",
    "tp_std = tp_ens_sel['tp'].mean(dim='number')\n",
    "\n",
    "tp_std_monthly = tp_std.rolling(\n",
    "    forecast_reference_time=48, center=False, min_periods=48\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(tp_std['forecast_reference_time'].values, tp_std.values,\n",
    "         color='tab:blue', lw=1.3, alpha=0.45, label='Std. dev (monthly)')\n",
    "plt.plot(tp_std_monthly['forecast_reference_time'].values, tp_std_monthly.values,\n",
    "         color='tab:green', lw=2.4, marker='o', label='4-year mean (calendar)')\n",
    "\n",
    "plt.xlabel(\"Forecast reference time\")\n",
    "plt.ylabel(\"Mean across ensemble (tp)\")\n",
    "plt.title(\"Ensemble mean of tp over time (yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 3.749769,
     "end_time": "2024-03-08T17:24:00.248720",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.498951",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cell\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.271597,
     "end_time": "2024-03-08T17:40:01.664067",
     "exception": false,
     "start_time": "2024-03-08T17:40:01.392470",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## â„¹ï¸ If you want to know more\n",
    "\n",
    "### Key resources\n",
    "The CDS catalogue entries for the data used were:\n",
    "* Complete ERA5 global atmospheric reanalysis: [reanalysis-era5-complete](https://doi.org/10.24381/cds.143582cf)\n",
    "* Monthly drought indices from 1940 to present derived from ERA5 reanalysis: [derived-drought-historical-monthly](https://doi.org/10.24381/9bea5e16)\n",
    "\n",
    "Code libraries used:\n",
    "* [earthkit](https://github.com/ecmwf/earthkit)\n",
    "  * [earthkit-data](https://github.com/ecmwf/earthkit-data)\n",
    "  * [earthkit-plots](https://github.com/ecmwf/earthkit-plots)\n",
    "\n",
    "### References\n",
    "\n",
    "List the references used in the Notebook here.\n",
    "\n",
    "E.g.\n",
    "\n",
    "[[1]](https://doi.org/10.1038/s41598-018-20628-2) Rodriguez, D., De Voil, P., Hudson, D., Brown, J. N., Hayman, P., Marrou, H., & Meinke, H. (2018). Predicting optimum crop designs using crop models and seasonal climate forecasts. Scientific reports, 8(1), 2231."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 968.520731,
   "end_time": "2024-03-08T17:40:03.783430",
   "environment_variables": {},
   "exception": null,
   "input_path": "D520.3.2.3b.SEASONAL_multimodel-bias_v5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T17:23:55.262699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
