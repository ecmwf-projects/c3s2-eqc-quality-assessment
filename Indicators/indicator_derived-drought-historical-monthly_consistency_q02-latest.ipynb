{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Reproducibility of drought indicators in the ERA5â€“Drought dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Production date: 2026-02-xx\n",
    "\n",
    "**Please note that this repository is used for development and review, so quality assessments should be considered work in progress until they are merged into the main branch.**\n",
    "\n",
    "Dataset version: 1.0.\n",
    "\n",
    "Produced by: Enis Gerxhalija, Olivier Burggraaff (National Physical Laboratory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸŒ Use case: Retrieving drought indicators from the ERA5-Drought dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## â“ Quality assessment question\n",
    "* **Are the drought indicators in the ERA5-Drought dataset consistent with and reproducible from ERA5 data?**\n",
    "* **Are the drought indicators in the ERA5-Drought dataset presented in a format that ensures optimal usability for users?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Drought and extreme precipitation have far-reaching environmental, societal, and economic impacts.\n",
    "In the United Kingdom, the record-breaking hot and dry spring and summer of 2025 caused harvest losses worth more than Â£800 million [[ECIU+25](https://eciu.net/analysis/reports/2025/estimated-financial-losses-faced-by-uk-farmers-due-dry-weather-impacts-on-key-arable-crops)].\n",
    "An 18-month drought in Brazil in 2023â€“24,\n",
    "the most severe since monitoring began in 1954,\n",
    "led to 720 health centres in affected areas becoming non-operational [[UNICEF+24](https://www.unicef.org/media/165191/file/LACR-Flash-Update-11-November-2024.pdf)].\n",
    "Finally, extreme rainfall killed hundreds of people and caused billions of â‚¬ of damages in Spain in 2024 [[Franch-Pardo+25](https://doi.org/10.48088/ejg.i.fra.16.2.286.297)].\n",
    "Climate change is thought to be the primary driver behind the increase in drought and extreme precipitation events since the 1950s [[IPCC+23](https://doi.org/10.59327/IPCC/AR6-9789291691647)],\n",
    "and this trend is expected to continue into the future.\n",
    "\n",
    "Given these impacts,\n",
    "monitoring drought and extreme precipitation is vital.\n",
    "Several quantitative proxies have been developed to aid in this monitoring,\n",
    "generally based on\n",
    "(a combination of)\n",
    "total precipitation, soil moisture, evapotranspiration, and surface (air) temperature.\n",
    "Two widely-employed indices are the \n",
    "_Standardised Precipitation Index (SPI)_ [[McKee+93](https://climate.colostate.edu/pdfs/relationshipofdroughtfrequency.pdf)]\n",
    "and\n",
    "_Standardised Precipitation-Evapotranspiration Index (SPEI)_ [[Vicente-Serrano+10](https://doi.org/10.1175/2009JCLI2909.1)].\n",
    "Both operate on the same principle,\n",
    "namely quantifying the amount of precipitation\n",
    "over a given time frame at a given location\n",
    "relative to its historical climatology.\n",
    "For example,\n",
    "an SPI value of +1 corresponds to a precipitation that is 1 standard deviation above the mean,\n",
    "for that site and time frame.\n",
    "This probabilistic approach lends itself to statements on the occurrence rate of extreme events [[McKee+93](https://climate.colostate.edu/pdfs/relationshipofdroughtfrequency.pdf)].\n",
    "SPEI expands on SPI by including not just gains from precipitation, but also losses due to evapo(transpi)ration.\n",
    "SPI and SPEI can be evaluated over different _accumulation periods_ to probe phenomena at different time scales:\n",
    "* 1, 3 months: soil moisture, flow in small creeks.\n",
    "* 6, 12 months: reservoir storage, stream flow.\n",
    "* 24, 36, 48 months: groundwater recharge, reduced reservoir.\n",
    "**Reference for these?**\n",
    "\n",
    "Of course,\n",
    "proxies like SPI and SPEI depend on reliable input data in the form of historical meteorological time series.\n",
    "Weather stations can provide these data with high accuracy and precision for specific sites,\n",
    "but their coverage is sparse and their data are not always interoperable.\n",
    "Reanalyses,\n",
    "which integrate observations and forecast modelling,\n",
    "can provide similar data consistently with long-term global coverage.\n",
    "For example, ECMWF's fifth-generation reanalysis _ERA5_ provides meteorological data on a global ~31 km grid\n",
    "going back to 1940 [[Soci+24](https://doi.org/10.1002/qj.4803), [Hersbach+20](https://doi.org/10.1002/qj.3803)].\n",
    "\n",
    "The Copernicus Climate Change Service (C3S)\n",
    "now provides pre-calculated SPI and SPEI indices derived from ERA5\n",
    "in the ERA5â€“Drought dataset [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)],\n",
    "[available from the Climate Data Store](https://doi.org/10.24381/9bea5e16).\n",
    "This derived dataset can be a valuable resource for applications in many sectors,\n",
    "since it can be used out of the box,\n",
    "freeing users from the need to find and process the underpinning meteorological data themselves.\n",
    "ERA5â€“Drought provides SPI and SPEI for 7 different accumulation periods,\n",
    "interpolated to a 0.25Â° Ã— 0.25Â° grid.\n",
    "It includes ERA5's deterministic reanalysis as well as 10 propagated members of the ERA5-EDA ensemble.\n",
    "The latter can be used to quantify the uncertainty in SPI or SPEI resulting from uncertainty in the input data.\n",
    "Moreover, ERA5â€“Drought provides multiple quality flags that can be used to filter data that do not meet the requirements for SPI or SPEI to be applicable.\n",
    "\n",
    "This quality assessment tests the consistency of ERA5â€“Drought with the underpinning ERA5 data in terms of its reproducibility.\n",
    "By manually reproducing SPI, SPEI, and the associated quality indicators from ERA5 data and comparing the results,\n",
    "we assess whether ERA5â€“Drought can indeed be used as a more convenient alternative to ERA5 for monitoring drought and extreme precipitation.\n",
    "Furthermore,\n",
    "any discrepancies found in the comparison shed light on caveats in the use of ERA5â€“Drought specifically and drought indicators broadly,\n",
    "such as the underlying assumptions.\n",
    "This notebook provides code for reproducing SPI and SPEI from ERA5 data and can be a jumping-off point for further analysis by the reader."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ“¢ Quality assessment statement\n",
    "\n",
    "```{admonition} These are the key outcomes of this assessment\n",
    ":class: note\n",
    "* Conclusion 1\n",
    "* Conclusion 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸ“‹ Methodology\n",
    "\n",
    "This quality assessment tests the consistency between drought indices retrieved from the [C3S ERA5-Drought dataset] and their equivalents calculated from the origin datasets, as well as the reproducibility and usability of said dataset.\n",
    "\n",
    "We will examine the SPI and SPEI drought indicators calculated from the following datasets:\n",
    "\n",
    "(include table here of the parameter, description of that parameters, and the origin dataset)\n",
    "\n",
    "The analysis and results are organised in the following steps, which are detailed in the sections below:\n",
    "\n",
    "**[](section-code_setup)**\n",
    " * Import all required libraries.\n",
    " * Define helper functions.\n",
    "\n",
    "**[](section-general_setup)**\n",
    " * Define scope of analysis (time, location).\n",
    " * Set up CDS downloads for ERA5 and ERA5-Drought.\n",
    " * Download land-sea mask.\n",
    "\n",
    "**[](section-spi)**\n",
    " * Define SPI Indicator\n",
    " * Download ERA5 precipitation\n",
    " * Accumulate\n",
    " * Calculate SPI\n",
    " * Calculate quality flags from ERA5 data\n",
    " * Download quality flags from ERA5-Drought\n",
    " * Compare quality flags\n",
    " * Download ERA5-Drought SPI\n",
    " * Comparison\n",
    "\n",
    "**[](section-spei)**\n",
    " * Define SPEI Indicator\n",
    " * Download ERA5 potential evaporation\n",
    " * Accumulate\n",
    " * Calculate SPEI\n",
    " * Calculate quality flags from ERA5 data\n",
    " * Download quality flags from ERA5-Drought\n",
    " * Compare quality flags\n",
    " * Download ERA5-Drought SPEI\n",
    " * Comparison\n",
    "\n",
    "**[](section-ensemble)**\n",
    "\n",
    "**[](section-conclusion)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Analysis and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-code_setup)=\n",
    "### 1. Code setup\n",
    "```{note}\n",
    "This notebook uses [earthkit](https://github.com/ecmwf/earthkit) for downloading ([earthkit-data](https://github.com/ecmwf/earthkit-data)) and visualising ([earthkit-plots](https://github.com/ecmwf/earthkit-plots)) data. Because earthkit is in active development, some functionality may change after this notebook is published. If any part of the code stops functioning, please raise an issue on our GitHub repository so it can be fixed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import required libraries\n",
    "In this section, we import all the relevant packages needed for running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Input / Output\n",
    "from pathlib import Path\n",
    "import earthkit.data as ekd\n",
    "import warnings\n",
    "\n",
    "# General data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "from itertools import batched\n",
    "\n",
    "# Analysis\n",
    "from dask.array import nanmedian, ravel\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Visualisation\n",
    "import earthkit.plots as ekp\n",
    "from earthkit.plots.styles import Style\n",
    "from cartopy import crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm, ListedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "from tqdm import tqdm  # Progress bars\n",
    "\n",
    "# Visualisation in Jupyter book -- automatically ignored otherwise\n",
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = None\n",
    "\n",
    "# Type hints\n",
    "from typing import Any, Callable, Iterable, Optional\n",
    "from scipy.stats import rv_continuous as Distribution\n",
    "from pandas.io.formats.style import Styler as pdStyler\n",
    "from earthkit.plots.geo.domains import Domain\n",
    "AnyDomain = (Domain | str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "This section defines some functions and variables used in the following analysis, allowing code cells in later sections to be shorter and ensuring consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "##### Data downloading & (pre-)processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The following functions handle downloading data in specific circumstances, e.g. a geographical or temporal subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographical\n",
    "def request_data_for_one_site(*, lat, lon, half_width=0.25):\n",
    "    \"\"\" Return a CDS request for a bounding box around a point (lon, lat). \"\"\"\n",
    "    north, south = lat + half_width, lat - half_width\n",
    "    east , west  = lon + half_width, lon - half_width\n",
    "    box = [north, west, south, east]\n",
    "    box = [round(x, 2) for x in box]  # Round all coordinates to 2 digits\n",
    "    request_box = {\"area\": box}\n",
    "    return request_box\n",
    "\n",
    "# Handling CDS size limits\n",
    "def batch_requests(main_request: dict, *, batch_key: str=\"year\", n: int=20) -> list[dict]:\n",
    "    \"\"\" Take a big request (e.g. ERA5â€“Drought for all years) and separate it into smaller ones (size `n`). \"\"\"\n",
    "    full_range = main_request[batch_key]  # e.g. [1940, 1941, ..., 2024]\n",
    "    batched_range = batched(full_range, n)  # e.g. [1940, ..., 1959], [1960, ..., 1979], ...\n",
    "    subrequests = [main_request | {batch_key: batch} for batch in batched_range]  # create corresponding CDS requests\n",
    "    return subrequests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The following functions ensure consistency in dimensions, which have inconsistent names (e.g. `valid_time` vs. `time`) and definitions (e.g. longitude 0 to 360 or â€“180 to 180) between ERA5 and ERA5-Drought:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_era5_dimensions(data: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Rename dimensions\n",
    "    from ERA5 (valid_time, latitude, longitude)\n",
    "    to ERA5â€“Drought (time, lat, lon)\n",
    "    format.\n",
    "    \"\"\"\n",
    "    data = data.rename({\"valid_time\": \"time\",\n",
    "                        \"latitude\": \"lat\",\n",
    "                        \"longitude\": \"lon\",\n",
    "                       })\n",
    "    return data\n",
    "\n",
    "def longitude_360_to_180(data: xr.Dataset, longitude_dimension=\"lon\") -> xr.Dataset:\n",
    "    \"\"\" Convert longitude from 0..360 to -180..180 format. \"\"\"\n",
    "    longitude_new = ((data[longitude_dimension] - 180) % 360) - 180\n",
    "    data = data.assign_coords({longitude_dimension: longitude_new})\n",
    "    data = data.sortby(longitude_dimension)#.squeeze()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The following functions handle unit conversions, e.g. m to mm precipitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit conversion\n",
    "def convert_unit(dataset: xr.Dataset, key: str, conversion: Callable, new_unit: str) -> None:\n",
    "    \"\"\" Convert the units of dataset[key] to new_unit using a conversion function (e.g. lambda x: x*1000 for m to mm), in-place. \"\"\"\n",
    "    # Metadata handling\n",
    "    metadata_old = dataset[key].attrs\n",
    "    metadata_new = metadata_old | {\"units\": new_unit}\n",
    "\n",
    "    # Apply changes\n",
    "    dataset[key] = conversion(dataset[key]).assign_attrs(**metadata_new)\n",
    "\n",
    "    return dataset  # Redundant because modified in-place\n",
    "\n",
    "convert_m2mm = partial(convert_unit, conversion=(lambda x: x*1000), new_unit=\"mm\")  # meter -> millimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The following function handles the overall pre-processing of ERA5 data: renaming dimensions, converting units, and adjusting longitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_era5(data: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\" Pre-process ERA5 data (precipitation and/or evapotranspiration) into the right format for analysis. \"\"\"\n",
    "    # Dimensions\n",
    "    data = rename_era5_dimensions(data)\n",
    "    data = longitude_360_to_180(data, \"lon\")  # Convert longitude from 0..360 to â€“180..180\n",
    "\n",
    "    # Units\n",
    "    for var in data.data_vars:  # Assume all data_vars need to be converted to mm\n",
    "        data = convert_m2mm(data, var)  # Convert from m to mm\n",
    "\n",
    "    # Re-chunk for speed gain in fitting\n",
    "    data = data.chunk({\"time\": -1, \"lat\": 103, \"lon\": 360,})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "The following function calculates the water balance `wb` from the total precipitation and potential evaporation ([](section-spei)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_waterbalance(data: xr.Dataset, *,\n",
    "                           precipitation_variable: str=\"tp\", evaporation_variable: str=\"pev\",\n",
    "                           waterbalance_variable: str=\"wb\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Calculate the water balance from precipitation and evaporation.\n",
    "    Assumes the ECMWF sign convention where downward flux is positive and upward flux is negative,\n",
    "    i.e. WB = TP + PEV.\n",
    "    \"\"\"\n",
    "    # Calculate water balance\n",
    "    waterbalance = data[precipitation_variable] + data[evaporation_variable]\n",
    "\n",
    "    # Apply variable name, metadata, and convert to dataset\n",
    "    waterbalance = waterbalance.rename(waterbalance_variable)\n",
    "    waterbalance = waterbalance.assign_attrs({\"units\": data[precipitation_variable].units,\n",
    "                                              \"long_name\": \"Water balance\",\n",
    "                                             })\n",
    "    waterbalance = waterbalance.to_dataset()\n",
    "    \n",
    "    return waterbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The following functions aid in processing quality flag data from ERA5â€“Drought into a standard format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_era5drought_qualityflag(data: dict[Any, ekd.FieldList], variable_out: Optional[str]=None, *,\n",
    "                                       month_dimension: str=\"month\", time_dimension: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\" \n",
    "    Turn a dict of earthkit FieldLists with ERA5â€“Drought quality flags, organised by accumulation period, into one big xr.Dataset.\n",
    "    Rename the variables according to the dict keys; optionally using `variable_out` (e.g. SPEI -> SPEI1, SPEI3, etc).\n",
    "    Convert a 12-month time dimension (2020-01-01, 2020-02-01, ...) into a month dimension (1, 2, ...).\n",
    "    \"\"\"\n",
    "    # Convert field lists to xarray Datasets\n",
    "    data = {key: ds.to_xarray(compat=\"equals\")\n",
    "            for key, ds in data.items()}\n",
    "\n",
    "    # Rename variables to include keys (e.g. accumulation periods)\n",
    "    first_fieldlist = next(iter(data.values()))\n",
    "    data_var = next(iter(first_fieldlist.data_vars))\n",
    "    if not variable_out:  # If not manually specified, append keys to existing variable name\n",
    "        variable_out = data_var\n",
    "    \n",
    "    data = [ds.rename_vars({data_var: f\"{variable_out}{key}\"})\n",
    "            for key, ds in data.items()]\n",
    "\n",
    "    # Merge into one\n",
    "    data = xr.merge(data, compat=\"equals\")\n",
    "\n",
    "    # Change time dimension to months\n",
    "    data = data.assign_coords({month_dimension: data[time_dimension].dt.month})\n",
    "    data = data.swap_dims({time_dimension: month_dimension})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "The following functions restructure the ensemble members in the ERA5-Drought dataset along a new dimension, as in ERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_number_dimension(data: xr.Dataset, *, n_members: int=10,\n",
    "                         time_dimension: str=\"time\", ensemble_dimension: str=\"number\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Convert a dataset\n",
    "    from ERA5-Drought format (10-duplicate `time` dimension)\n",
    "    to ERA5 format (`number` dimension for ensemble members)\n",
    "    \"\"\"\n",
    "    # Find unique times and use these to generate datasets for successive members\n",
    "    member_numbers = np.arange(n_members)\n",
    "    _, index = np.unique(data[time_dimension], return_index=True)\n",
    "    data = [data.isel({time_dimension: index + i}) for i in member_numbers]\n",
    "\n",
    "    # Combine into one dataset\n",
    "    data = xr.concat(data, dim=ensemble_dimension).assign_coords(number=member_numbers)\n",
    "\n",
    "    # Rechunk for memory efficiency\n",
    "    data = data.chunk({ensemble_dimension: n_members, time_dimension: 48, \"lat\": 360, \"lon\": 103,})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "##### Accumulation periods\n",
    "The following cells contain constants and functions used in accumulating variables (e.g. precipitation) over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants such as the accumulation periods to use\n",
    "ACCUMULATION_PERIODS = [1, 3, 6, 12, 24, 36, 48]  # Months\n",
    "MONTHS = range(1, 13)  # January to December (inclusive)\n",
    "MONTHS_NAMED = {\n",
    "     1:  \"January\", 2:  \"February\", 3:  \"March\",\n",
    "     4:  \"April\",   5:  \"May\",      6:  \"June\",\n",
    "     7:  \"July\",    8:  \"August\",   9:  \"September\",\n",
    "    10: \"October\", 11: \"November\", 12:  \"December\",\n",
    "}\n",
    "\n",
    "# Perform accumulation\n",
    "def accumulate(data: xr.Dataset, var: str, *,\n",
    "               accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS, time_dimension: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Accumulate the given variable `var` (e.g. total precipitation) over different accumulation periods.\n",
    "    \"\"\"\n",
    "    # Find the number of days in each month\n",
    "    time_index = pd.to_datetime(data[time_dimension])  # Convert time dimension to datetime format\n",
    "    days_in_month = xr.DataArray(time_index.days_in_month, coords={time_dimension: data[time_dimension]})  # Aligned with `data` coordinates\n",
    "\n",
    "    # Monthly mean `var` â†’ Monthly total `var`\n",
    "    monthly_total = data[var] * days_in_month\n",
    "\n",
    "    ## Accumulate over periods (rolling windows)\n",
    "    result = data.copy()\n",
    "    for period in accumulation_periods:\n",
    "        rolling_sum = monthly_total.rolling({time_dimension: period}, center=False).sum()  # Rolling sum\n",
    "        result[f\"{var}{period}\"] = rolling_sum  # Assign variable in same format as ERA5â€“Drought (e.g. SPI6)\n",
    "\n",
    "    # Reorganise\n",
    "    result = result.drop_vars([var])  # Drop original var\n",
    "    result = result.chunk(data.chunksizes)  # Rechunk like input data\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "The following functions calculate the probability\n",
    "(regular and accounting for the centre of mass)\n",
    "of months with zero (accumulated) precipitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_months_with_zero_precipitation(data: xr.Dataset, *,\n",
    "                                             round_to: int=2, threshold: float=0.,\n",
    "                                             time_dimension: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Find the number of months with zero precipitation (and total months) in a given dataset.\n",
    "    Rounds to `round_to` decimals (default 2, e.g. 0.01 mm).\n",
    "    Zero precipitation means <= `threshold` (0. by default).\n",
    "    Applied per calendar month and to all variables (e.g. accumulation periods).\n",
    "    \"\"\"\n",
    "    # Round data, to 2 decimals by default\n",
    "    data = data.round(round_to)\n",
    "\n",
    "    # Find months below threshold\n",
    "    is_zero = (data <= threshold)\n",
    "\n",
    "    # Group by calendar month and find total number\n",
    "    data_by_calendar_month = is_zero.groupby(is_zero[time_dimension].dt.month)\n",
    "    n_zero  = data_by_calendar_month.sum(dim=time_dimension).astype(np.uint8)\n",
    "    n_total = data_by_calendar_month.count(dim=time_dimension).astype(np.uint8)\n",
    "\n",
    "    return n_total, n_zero\n",
    "\n",
    "def probability_of_zero_precipitation(data: xr.Dataset, **kwargs) -> xr.Dataset:\n",
    "    \"\"\" Calculate the simple probability of zero precipitation in the given dataset, per calendar month. \"\"\"\n",
    "    # Find number of months (total / zero precipitation)\n",
    "    n_total, n_zero = number_of_months_with_zero_precipitation(data, **kwargs)\n",
    "\n",
    "    # Calculate simple probability\n",
    "    p_zero = n_zero / (n_total + 1)\n",
    "\n",
    "    return p_zero\n",
    "   \n",
    "def probability_of_zero_precipitation_weighted(data: xr.Dataset, **kwargs) -> xr.Dataset:\n",
    "    \"\"\" Calculate the weighted probability of zero precipitation in the given dataset, per calendar month. \"\"\"\n",
    "    # Find number of months (total / zero precipitation)\n",
    "    n_total, n_zero = number_of_months_with_zero_precipitation(data, **kwargs)    \n",
    "\n",
    "    # Calculate weighted probability\n",
    "    p_zero = xr.where(\n",
    "        n_zero > 0,                          # Condition: More than 0 months with zero precipitation\n",
    "        (n_zero + 1) / (2 * (n_total + 1)),  # If condition met: weighted probability\n",
    "        0                                    # If condition not met: just 0\n",
    "    )\n",
    "\n",
    "    return p_zero\n",
    "\n",
    "def zero_precip_rejection(prob_ds: xr.Dataset, ls_mask):\n",
    "    \"\"\"\n",
    "    Compute zero-precipitation rejection masks from monthly probabilities and\n",
    "    apply a landâ€“sea mask.\n",
    "\n",
    "    Logic:\n",
    "      - Mark a month as 'rejected' if probability >= 0.33 (1 = reject, 0 = keep).\n",
    "      - Aggregate across the 'month' dimension to get the count per grid cell.\n",
    "      - Apply a landâ€“sea mask so ocean pixels are set to NaN.\n",
    "      - Return both the monthly rejection field and the aggregated counts across time.\n",
    "    \"\"\"\n",
    "    \n",
    "    rejection = xr.where(\n",
    "            prob_ds >= 0.33,\n",
    "            1,\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # Aggregate across time \n",
    "    rejection_agg = rejection.sum(dim=[d for d in [\"month\", \"time\"] if d in rejection.dims])\n",
    "\n",
    "    # Apply land-sea mask to monthly\n",
    "    rejection_ds = rejection.where(ls_mask[\"lsm\"])\n",
    "    \n",
    "    # Apply land-sea mask to aggregated\n",
    "    rejection_agg_ds = rejection_agg.where(ls_mask[\"lsm\"])\n",
    "\n",
    "    return rejection_ds, rejection_agg_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "##### Calculating SPI and SPEI\n",
    "The following functions fit the appropriate distributions\n",
    "(gamma, generalised logistic)\n",
    "and calculate corresponding CDF and SPI / SPEI values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General fitting function\n",
    "def fit_distributions(reference_data: xr.Dataset, distribution: Distribution, *,\n",
    "                      time_dimension: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Fit distributions (e.g. gamma) for each month and accumulation period using xarray parallelisation.\n",
    "    Data are assumed to have been sliced to the reference period.\n",
    "    \"\"\"\n",
    "    # Define fitting function for the given distribution\n",
    "    def fit(y):\n",
    "        y = y[np.isfinite(y)]\n",
    "        try:\n",
    "            params = distribution.fit(y)\n",
    "        except:  # If fitting fails, return NaN\n",
    "            # Should only catch a specific Exception type\n",
    "            params = [np.nan]*(distribution.numargs+2)\n",
    "        finally:  # Always convert result (values or NaN) to desired format\n",
    "            params = np.stack(params, axis=-1)  # Extend with axis for stats (alpha, loc, scale, ...)\n",
    "        return params\n",
    "\n",
    "    # Split dataset by month\n",
    "    reference_data_by_month = reference_data.groupby(reference_data[time_dimension].dt.month)\n",
    "\n",
    "    # Apply fitting function by month\n",
    "    params = xr.apply_ufunc(fit, reference_data_by_month,\n",
    "                            input_core_dims=[[time_dimension]], output_core_dims=[[\"stat\"]],\n",
    "                            vectorize=True, dask=\"parallelized\",\n",
    "                            dask_gufunc_kwargs={\"output_sizes\": {\"stat\": distribution.numargs+2}},  # e.g. 3 for gamma (alpha, loc, scale)\n",
    "                            output_dtypes=[np.float64],\n",
    "                           )\n",
    "    params = params.chunk({\"month\": -1})\n",
    "\n",
    "    return params\n",
    "\n",
    "# Fitting functions for SPI, SPEI distributions specifically\n",
    "fit_monthly_spi  = partial(fit_distributions, distribution=stats.gamma)\n",
    "fit_monthly_spei = partial(fit_distributions, distribution=stats.genlogistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "The following functions apply the fitted distributions to calculate CDF values for observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Clip the CDF to avoid infinities\n",
    "def clip_cdf(cdf: xr.Dataset, threshold: float=1e-16) -> xr.Dataset:\n",
    "    \"\"\" Clip CDF values to `threshold` on both sides. \"\"\"\n",
    "    cdf_clipped = cdf.clip(threshold, 1 - threshold)\n",
    "    return cdf_clipped\n",
    "\n",
    "# Main function: Compute CDF values for observed data\n",
    "def compute_cdf(data: xr.Dataset, parameters: xr.Dataset, distribution: Distribution, *,\n",
    "                month_dimension: str=\"month\", time_dimension: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    For observed `data`, compute the corresponding cumulative distribution function (CDF)\n",
    "    values for the given `distribution` and `parameters`.\n",
    "\n",
    "    Assumes that `data` and `parameters` have compatible:\n",
    "        Coordinates (except time)\n",
    "        Variables (e.g. corresponding to accumulation periods)\n",
    "    \"\"\"\n",
    "    # Create a month dimension for broadcasting with the one in parameters\n",
    "    data_month = data[time_dimension].dt.month.rename(month_dimension)\n",
    "\n",
    "    # Extract parameters\n",
    "    nr_parameters = parameters.sizes[\"stat\"]  # 3 for gamma / genlogistic\n",
    "    parameters_extracted = [parameters.sel(stat=j).sel({month_dimension: data_month}) for j in range(nr_parameters)]\n",
    "\n",
    "    # Calculate CDF values by month\n",
    "    cdf = xr.apply_ufunc(distribution.cdf, data, *parameters_extracted,\n",
    "                         input_core_dims=[[], [], [], []], output_core_dims=[[]],\n",
    "                         vectorize=True, dask=\"parallelized\",\n",
    "                         output_dtypes=[np.float64],\n",
    "                         keep_attrs=True\n",
    "                        )\n",
    "    cdf = clip_cdf(cdf)  # Clip extreme values to avoid infinities\n",
    "    cdf = cdf.chunk(data.chunksizes) # Rechunk like input data\n",
    "\n",
    "    return cdf\n",
    "\n",
    "# Compute SPI, SPEI with preset distributions\n",
    "compute_cdf_spi  = partial(compute_cdf, distribution=stats.gamma)\n",
    "compute_cdf_spei = partial(compute_cdf, distribution=stats.genlogistic)\n",
    "\n",
    "def adjust_cdf_for_zero_precipitation(cdf: xr.Dataset, data: xr.Dataset, *,\n",
    "                                      time_dimension: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Adjust CDF for months with zero precipitation, following Stagge+15 (doi:10.1002/joc.4267) method.\n",
    "    `cdf` can be for any time range, `data` should be for the reference period only.\n",
    "    \"\"\" \n",
    "    # Calculate probability of zero precipitation\n",
    "    p_zero = probability_of_zero_precipitation_weighted(data)\n",
    "    p_zero = p_zero.chunk({\"month\": -1})  # Rechunk for efficiency\n",
    "\n",
    "    # Adjust CDF by calendar month\n",
    "    def adjust_cdf_by_calendar_month(cdf_one_month: xr.Dataset) -> xr.Dataset:\n",
    "        this_month = cdf_one_month[time_dimension][0].month.values # Cannot use GroupBy label inside .map() unfortunately\n",
    "        p_zero_this_month = p_zero.sel({\"month\": this_month})\n",
    "        cdf_adjusted = p_zero_this_month + (1 - p_zero_this_month) * cdf_one_month\n",
    "        return cdf_adjusted\n",
    "\n",
    "    cdf_by_calendar_month = cdf.groupby(cdf[time_dimension].dt.month)\n",
    "    cdf_adjusted = cdf_by_calendar_month.map(adjust_cdf_by_calendar_month)  \n",
    "    # Alternative approach: Broadcast p_zero against cdf instead of using GroupBy (not implemented here)\n",
    "\n",
    "    # Reorganise for efficiency\n",
    "    cdf_adjusted = cdf_adjusted.chunk(cdf.chunksizes)\n",
    "\n",
    "    return cdf_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "The following functions calculate the SPI/SPEI index based on CDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function: Convert CDF values to indexes\n",
    "def cdf_to_index(cdf: xr.Dataset, *, var: Optional[str]=None, index_name: Optional[str]=None) -> xr.Dataset:\n",
    "    \"\"\" Transform CDF values to a standard normal distribution. \"\"\"\n",
    "    cdf = clip_cdf(cdf)  # Clip extreme values to avoid infinities\n",
    "\n",
    "    # Convert CDF to index\n",
    "    index_values = xr.apply_ufunc(stats.norm.ppf, cdf, 0.0, 1.0,  \n",
    "                                  input_core_dims=[[], [], []], output_core_dims=[[]],\n",
    "                                  vectorize=True, dask=\"parallelized\",\n",
    "                                  output_dtypes=[np.float64],\n",
    "                                  keep_attrs=True,\n",
    "                                 )\n",
    "\n",
    "    # Adjust variable names to ERA5â€“Drought format (e.g. SPI6) if desired\n",
    "    if var and index_name:\n",
    "        rename_variables = {datavar: datavar.replace(var, index_name) for datavar in cdf.data_vars if var in datavar}\n",
    "        index_values = index_values.rename_vars(rename_variables)\n",
    "\n",
    "    return index_values\n",
    "\n",
    "# Compute SPI, SPEI with preset names\n",
    "cdf_to_spi  = partial(cdf_to_index, var=\"tp\", index_name=\"SPI\")\n",
    "cdf_to_spei = partial(cdf_to_index, var=\"wb\", index_name=\"SPEI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "For convenience, we also provide the following functions that wrap the entire process from data to SPI/SPEI into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full SPI pipeline\n",
    "def calculate_spi_from_era5(data: xr.Dataset, *, reference_window: dict[str, slice],\n",
    "                            do_preprocessing=False, evaluation_window: Optional[dict[str, slice]]=None,\n",
    "                            precipitation_variable: str=\"tp\",\n",
    "                            accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS,\n",
    "                           ) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Calculate SPI from ERA5 precipitation data, including all steps.\n",
    "    Input should be ERA5 data in xarray format, either\n",
    "        freshly downloaded (`do_preprocessing=True`)\n",
    "        or\n",
    "        pre-processed (`do_preprocessing=False`).\n",
    "    Some parameters can be adjusted,\n",
    "        e.g. the name of the precipitation variable (\"tp\" by default).\n",
    "        This should make the function easier to transfer to other datasets.\n",
    "    \"\"\"\n",
    "    # Optional: pre-process to desired format\n",
    "    if do_preprocessing:\n",
    "        data = preprocess_era5(data)\n",
    "\n",
    "    # Accumulate total precipitation\n",
    "    data = accumulate(data, precipitation_variable, accumulation_periods=accumulation_periods)\n",
    "\n",
    "    # Select data in reference window\n",
    "    data_reference = data.sel(**reference_window)\n",
    "    \n",
    "    # Fit gamma distribution\n",
    "    spi_parameters = fit_monthly_spi(data_reference)\n",
    "\n",
    "    # Optional: Only calculate index in evaluation window\n",
    "    if evaluation_window:\n",
    "        data = data.sel(**evaluation_window)\n",
    "\n",
    "    # Compute CDF time series\n",
    "    cdf = compute_cdf_spi(data, spi_parameters)\n",
    "\n",
    "    # Adjust CDF\n",
    "    cdf_adjusted = adjust_cdf_for_zero_precipitation(cdf, data_reference)\n",
    "\n",
    "    # Calculate SPI from adjusted CDF\n",
    "    spi = cdf_to_spi(cdf_adjusted)\n",
    "    spi = spi.chunk(data.chunksizes)\n",
    "\n",
    "    return spi\n",
    "\n",
    "# Full SPEI pipeline\n",
    "def calculate_spei_from_era5(data: xr.Dataset, *, reference_window: dict[str, slice],\n",
    "                             do_preprocessing=False, evaluation_window: Optional[dict[str, slice]]=None,\n",
    "                             precipitation_variable: str=\"tp\", evaporation_variable: str=\"pev\",\n",
    "                             accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS,\n",
    "                            ) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Calculate SPEI from ERA5 precipitation and potential evaporation data, including all steps.\n",
    "    Input should be ERA5 data in xarray format, either\n",
    "        freshly downloaded (`do_preprocessing=True`)\n",
    "        or\n",
    "        pre-processed (`do_preprocessing=False`).\n",
    "    Some parameters can be adjusted,\n",
    "        e.g. the name of the precipitation variable (\"tp\" by default).\n",
    "        This should make the function easier to transfer to other datasets.\n",
    "    \"\"\"\n",
    "    # Optional: pre-process to desired format\n",
    "    if do_preprocessing:\n",
    "        data = preprocess_era5(data)\n",
    "\n",
    "    # Calculate water balance\n",
    "    data = calculate_waterbalance(data)\n",
    "\n",
    "    # Accumulate water balance\n",
    "    data = accumulate(data, \"wb\")\n",
    "\n",
    "    # Select data in reference window\n",
    "    data_reference = data.sel(**reference_window)\n",
    "\n",
    "    # Fit generalised logistic distribution\n",
    "    spei_parameters = fit_monthly_spei(data_reference)\n",
    "\n",
    "    # Optional: Only calculate index in evaluation window\n",
    "    if evaluation_window:\n",
    "        data = data.sel(**evaluation_window)\n",
    "\n",
    "    # Compute CDF time series\n",
    "    cdf = compute_cdf_spei(data, spei_parameters)\n",
    "\n",
    "    # Calculate SPI from CDF\n",
    "    spei = cdf_to_spei(cdf)\n",
    "    spei = spei.chunk(data.chunksizes)\n",
    "\n",
    "    return spei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Categorising SPI and SPEI\n",
    "The following cell defines categories for SPI and SPEI values, e.g. \"severe drought\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category format: [lower limit, upper limit, label, colour]\n",
    "# Colours are picked from Fig. 4 in Keune+25 and labelled for convenience\n",
    "# Limits are left-inclusive: [lower, upper)\n",
    "# Extreme values of 100 / -100 should be treated as infinities but play nicer with some code than np.inf\n",
    "\n",
    "CATEGORIES_SPI = [  # Approximates the GDO scheme\n",
    "    (   2.00, 100,     \"Extremely wet\",            \"#7a007b\"),  # deepest purple \n",
    "    (   1.50,   2.00,  \"Severely wet\",             \"#af51c3\"),  # dark purple\n",
    "    (   1.00,   1.50,  \"Moderately wet\",           \"#eaccf8\"),  # medium purple\n",
    "    (   0.00,   1.00,  \"Nearâ€‘normal / mildly wet\", \"#ffffff\"),  # white\n",
    "    (  -1.00,   0.00,  \"Nearâ€‘normal / mildly dry\", \"#ffffff\"),  # white\n",
    "    (  -1.50,  -1.00,  \"Moderately dry\",           \"#fffc03\"),  # yellow\n",
    "    (  -2.00,  -1.50,  \"Severely dry\",             \"#feaa00\"),  # orange\n",
    "    (-100,     -2.00,  \"Extremely dry\",            \"#ff0100\"),  # red\n",
    "]\n",
    "\n",
    "CATEGORIES_SPEI = [\n",
    "    (   2.33, 100,    \"Extremely wet\",  \"#01148b\"),  # very dark navy\n",
    "    (   1.65,   2.33, \"Severely wet\",   \"#1871de\"),  # strong blue\n",
    "    (   1.28,   1.65, \"Moderately wet\", \"#14acf4\"),  # medium blue\n",
    "    (   0.84,   1.28, \"Mildly wet\",     \"#00f2fe\"),  # cyan\n",
    "    (  -0.84,   0.84, \"Near-normal\",    \"#9afa93\"),  # light green\n",
    "    (  -1.28,  -0.84, \"Mildly dry\",     \"#fdc403\"),  # yellow\n",
    "    (  -1.65,  -1.28, \"Moderately dry\", \"#f2631d\"),  # orange\n",
    "    (  -2.33,  -1.65, \"Severely dry\",   \"#df2929\"),  # red\n",
    "    (-100,     -2.33, \"Extremely dry\",  \"#8c1b1a\"),  # dark red\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "The following functions categorise SPI / SPEI values into said categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _digitise_dataset(data: xr.Dataset, bin_edges: list[float], *, persist=True) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Digitise all variables in `data` according to a list of left bin edges, e.g. SPI categories (\"extremely dry\").\n",
    "    In this notebook, do not call this function directly, but use the categorise_dataset wrapper.\n",
    "    \"\"\"\n",
    "    # Perform digitisation\n",
    "    data_digitised = xr.apply_ufunc(np.digitize, data,\n",
    "                                    kwargs={\"bins\": bin_edges, \"right\": False}, \n",
    "                                    input_core_dims=[[]],\n",
    "                                    vectorize=True, dask=\"parallelized\",\n",
    "                                    output_dtypes=[np.uint8],  # uint8 is small; not suitable for >256 categories\n",
    "    )\n",
    "\n",
    "    # Persist in memory if desired (default True because uint8s are small)\n",
    "    if persist:\n",
    "        data_digitised = data_digitised.persist()\n",
    "\n",
    "    return data_digitised\n",
    "\n",
    "def categorise_dataset(data: xr.Dataset, categories: Iterable[Iterable], **kwargs) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Categorise a dataset into bins defined by category thresholds.\n",
    "    Extracts bin edges from `categories` and then digitises `data` accordingly.\n",
    "    Handles NaNs by adding a dummy category (extremely high bin edge).\n",
    "    \"\"\"\n",
    "    # Extract bin edges from categories\n",
    "    # Last element is dropped to extend range to infinity (although values should be clipped anyway)\n",
    "    bin_edges = [category[0] for category in categories[:-1]]\n",
    "\n",
    "    # Add dummy category to catch NaNs\n",
    "    bin_edges = [100000] + bin_edges\n",
    "\n",
    "    # Apply digitisation\n",
    "    data_categorised = _digitise_dataset(data, bin_edges, **kwargs)\n",
    "\n",
    "    return data_categorised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "##### Quality flags\n",
    "The following functions create and apply quality flag tests, such as the probability of zero precipitation and the Shapiroâ€“Wilk normality test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_shapiro_test(spi_ds: xr.Dataset, reference_window: dict[str, slice],\n",
    "                    accum_periods: Iterable[int]=ACCUMULATION_PERIODS,\n",
    "                    months = range(1,13), index_name: Optional[str]=None ):\n",
    "    \n",
    "    \"\"\" Function to perform Shapiro-Wilks test on data within reference period\n",
    "    Returns Shapiro-Wilks statistic along with probability of falling under normal distribution, along with significance value (1/0).\"\"\"\n",
    "    \n",
    "    spi_ref = spi_ds.sel(**reference_window)  \n",
    "\n",
    "    time_dim = find_time_dim(spi_ds) # find time dimension.\n",
    "        \n",
    "    spi_ref = spi_ref.chunk({time_dim: -1}) \n",
    "    \n",
    "    spi_ref = spi_ref.where(np.isfinite(spi_ref))  # Mask non-finite values before applying shapiro-wilks.\n",
    "\n",
    "    spi_ref_by_month = spi_ref.groupby(f\"{time_dim}.month\")\n",
    "\n",
    "    # Perform shapiro on xarray\n",
    "    _ , pval = xr.apply_ufunc(stats.shapiro, spi_ref_by_month,\n",
    "                                input_core_dims=[[time_dim]], output_core_dims=[[],[]],\n",
    "                                vectorize=True, dask=\"parallelized\",\n",
    "                                output_dtypes=[np.float64, np.float64],\n",
    "                                keep_attrs=True,\n",
    "                               )\n",
    "    \n",
    "    normality = xr.where(pval < 0.05, 0, 1)  # Values < 0.05 â†’ 0\n",
    "\n",
    "    for p in accum_periods:\n",
    "        normality = normality.rename({f\"{index_name}{p}\": f\"significance_{p}\"})\n",
    "\n",
    "    month_ts = pd.to_datetime([f\"2020-{int(m):02d}-01\" for m in normality[\"month\"].values])\n",
    "    \n",
    "    # Assign those timestamps to the existing 'month' dimension,\n",
    "    # then rename the dimension from 'month' -> 'time'\n",
    "    normality = (\n",
    "        normality\n",
    "        .assign_coords(month=(\"month\", month_ts))\n",
    "        .rename(month=\"time\")\n",
    "    )\n",
    "\n",
    "    return pval, normality\n",
    "\n",
    "def apply_sw_quality_mask(era5_quality: xr.Dataset, index_ds: xr.Dataset, indicator: str):\n",
    "    \"\"\"\n",
    "    Apply significance-based quality masks to drought indicator datasets.\n",
    "    \"\"\"\n",
    "    index_ds = safe_rename(index_ds)\n",
    "    for period in ACCUMULATION_PERIODS:\n",
    "        sig = era5_quality[f\"significance_{period}\"]\n",
    "        sig = sig.assign_coords(time=sig.time.dt.month)\n",
    "        mask = sig.sel(time=index_ds[f\"{indicator}{period}\"].time.dt.month)\n",
    "        index_ds[f\"{indicator}{period}\"] = index_ds[f\"{indicator}{period}\"].where(mask.values == 1)\n",
    "    return index_ds\n",
    "\n",
    "\n",
    "def count_flagged_months(data: xr.Dataset, *, invert=True) -> xr.Dataset:\n",
    "    \"\"\" For a dataset with quality flags for 12 months, return a new dataset with the number of months failing that flag. \"\"\"\n",
    "    sum_by_month = data.sum(dim=\"month\").astype(np.uint8)  # Number of months *passing* the flag\n",
    "    if invert:  # Make False for e.g. mismatched months\n",
    "        sum_by_month = 12 - sum_by_month                   # Number of months *failing* the flag\n",
    "    return sum_by_month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "##### Statistics\n",
    "The following functions calculate and display the difference (by month) between datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: Calculate statistics\n",
    "def align_data_variables(data1: xr.Dataset, data2: xr.Dataset, var: Optional[str]=\"\") -> tuple[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Only look at variables that are in both datasets, following the order in `data2`.\n",
    "    Optional: variables have to include `var`, e.g. \"SPI\".\n",
    "    \"\"\"\n",
    "    matching_variables = [data_var for data_var in data2.data_vars\n",
    "                          if data_var in data1.data_vars and var in data_var]\n",
    "    data1, data2 = [data[matching_variables] for data in (data1, data2)]\n",
    "    return data1, data2\n",
    "\n",
    "def median_by_month(data: xr.Dataset, *, time_dimension: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\" Calculate the median by month, flattening all other dimensions (time, lat, lon, etc.) \"\"\"\n",
    "    data_by_month = data.groupby(data[time_dimension].dt.month)\n",
    "    median = data_by_month.quantile(0.5, dim=..., skipna=True)  # .quantile function has better dask support than .median\n",
    "    median = median.drop_vars(\"quantile\")  # Drop unneeded coordinate\n",
    "    return median\n",
    "\n",
    "# Main function: Calculate statistics\n",
    "def comparison_monthly_statistics(data1: xr.Dataset, data2: xr.Dataset, *,\n",
    "                                  time_dimension: str=\"time\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given two datasets, calculate a number of statistics for each variable and return the result in a table.\n",
    "    \"\"\"\n",
    "    # Align data variables\n",
    "    data1, data2 = align_data_variables(data1, data2)\n",
    "\n",
    "    # Calculate difference\n",
    "    difference = data2 - data1\n",
    "    difference_absolute = xr.ufuncs.abs(difference)\n",
    "\n",
    "    # Calculate monthly medians\n",
    "    md  = median_by_month(difference,          time_dimension=time_dimension).to_pandas()\n",
    "    mad = median_by_month(difference_absolute, time_dimension=time_dimension).to_pandas()\n",
    "\n",
    "    # Combine into one DataFrame\n",
    "    stats = pd.concat({\"Î”\": md, \"|Î”|\": mad}, axis=1)  # Combine\n",
    "    stats = stats.swaplevel(0, 1, axis=1)             # Accumulation period on top, statistic below\n",
    "    stats = stats[data2.data_vars]                    # Same order as inputs\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Styling for statistics\n",
    "def _add_matching_vertical_separators(styler: pdStyler, *,\n",
    "                                      colour=\"#bbbbbb\", width=\"1px\") -> pdStyler:\n",
    "    \"\"\"\n",
    "    Add vertical separators corresponding to the top level columns,\n",
    "        e.g. SPI1 â€“ SPI3 etc. but not Î” â€“ |Î”| in comparison_monthly_statistics outputs.\n",
    "\n",
    "    Note: This function is largely AI-generated, with manual edits.\n",
    "      - Adds a left border to the first subcolumn of each top-level group\n",
    "        in the BODY (data cells),\n",
    "      - Adds the same border to the HEADER cells for both level 0 and level 1\n",
    "        at the same positions (using .col{i} classes).\n",
    "    \"\"\"\n",
    "    # Get data, check applicability\n",
    "    df = styler.data\n",
    "    if not isinstance(df.columns, pd.MultiIndex) or df.columns.nlevels < 2:\n",
    "        raise ValueError(\"Expected MultiIndex columns with â‰¥2 levels.\")\n",
    "\n",
    "    # Group boundaries (indices of first subcolumn of each new top-level label)\n",
    "    lvl0 = df.columns.get_level_values(0).to_numpy()\n",
    "    breaks = np.flatnonzero(lvl0[1:] != lvl0[:-1]) + 1\n",
    "    breaks = np.r_[0, breaks]  # Include first element\n",
    "\n",
    "    # Define common styles\n",
    "    border_left = f\"{width} solid {colour}\"\n",
    "\n",
    "    # 1) BODY: draw the vertical rule down through the data\n",
    "    for i in breaks:\n",
    "        styler = styler.set_properties(\n",
    "            subset=pd.IndexSlice[:, df.columns[i]],\n",
    "            **{\"border-left\": border_left}\n",
    "        )\n",
    "\n",
    "    # 2) HEADER: mirror the exact same left border on header cells\n",
    "    # Each header cell at position i has classes: th.col_heading.level{0|1}.col{i}\n",
    "    header_rules = []\n",
    "    for i in breaks:\n",
    "        header_rules.extend([\n",
    "            {\"selector\": f\"th.col_heading.level0.col{i}\",\n",
    "             \"props\": [(\"border-left\", border_left)]}\n",
    "            ,\n",
    "            {\"selector\": f\"th.col_heading.level1.col{i}\",\n",
    "             \"props\": [(\"border-left\", border_left)]}\n",
    "        ])\n",
    "\n",
    "    # Optional: normalize other header borders so only our verticals stand out\n",
    "    header_rules.extend([\n",
    "        {\"selector\": \"th.col_heading\", \"props\": [(\"border-bottom\", \"0\")]},\n",
    "        # keep overall layout tight and consistent\n",
    "        {\"selector\": \"table\",\n",
    "         \"props\": [(\"border-collapse\", \"separate\"), (\"border-spacing\", \"0\")]}\n",
    "    ])\n",
    "\n",
    "    styler = styler.set_table_styles((styler.table_styles or []) + header_rules)\n",
    "\n",
    "    return styler\n",
    "\n",
    "# Main function: Display statistics\n",
    "def display_monthly_statistics(comparison_stats: pd.DataFrame, *,\n",
    "                               label1: str=\"ERA5â€“Drought\", label2: str=\"Reproduced\", title_suffix: Optional[str]=\"\") -> pdStyler:\n",
    "    \"\"\"\n",
    "    Display the statistics calculated in comparison_monthly_statistics with more style.\n",
    "    Note: does NOT do the actual calculations, unlike in other notebooks.\n",
    "    \"\"\"\n",
    "    # Use words for month names; cannot be done in styler\n",
    "    comparison_stats = comparison_stats.rename(MONTHS_NAMED).rename_axis(None, axis=0)\n",
    "\n",
    "    # Apply styles:\n",
    "    # Number of digits\n",
    "    # Caption\n",
    "    # No sticky index â€“ does not play nice with jupyter-book\n",
    "    # Apply vertical lines to separate columns\n",
    "    formatted = comparison_stats.style \\\n",
    "                                .format(precision=4)  \\\n",
    "                                .set_caption(f\"{label2} â€“ {label1}{title_suffix}\")  \\\n",
    "                                .pipe(_add_matching_vertical_separators)\n",
    "    \n",
    "    # Center headers ; AI-generated\n",
    "    formatted = formatted.set_table_styles(\n",
    "        (formatted.table_styles or []) + [\n",
    "            {'selector': 'th.col_heading',\n",
    "             'props': [('text-align', 'center'),\n",
    "                       ('vertical-align', 'bottom'),\n",
    "                      ],\n",
    "             },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "##### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "The following cell defines [earthkit-plots styles](https://earthkit-plots.readthedocs.io/en/latest/_api/plots/styles/index.html) for the variables in the datasets.\n",
    "These styles define the colour maps and colour bar ranges for each quantity. Earthkit-plots styles are explained further in the [corresponding documentation](https://earthkit-plots.readthedocs.io/en/latest/examples/examples/examples.html#Styles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Translate tabulated categories to Style\n",
    "def categories_to_earthkit_style(var_categories: Iterable[Iterable]) -> Style:  # e.g. CATEGORIES_SPI\n",
    "    \"\"\" From a list of categories, create an earthkit.plots Style. \"\"\"\n",
    "    cmap = ListedColormap([category[3] for category in var_categories[::-1]])  # Colours in reverse (ascending) order\n",
    "    norm =   BoundaryNorm([category[0] for category in var_categories[-2::-1]], cmap.N, extend=\"both\")\n",
    "    style = {\"cmap\": cmap, \"norm\": norm, \"extend\": \"both\"}\n",
    "    return style\n",
    "\n",
    "# Styles for SPI, SPEI based on Keune+25 colours\n",
    "_style_spi  = categories_to_earthkit_style(CATEGORIES_SPI)\n",
    "_style_spei = categories_to_earthkit_style(CATEGORIES_SPEI)\n",
    "_style_spi_diff  = {\"cmap\": plt.cm.RdBu.resampled(11), \"vmin\": -1, \"vmax\": 1, \"extend\": \"both\"}\n",
    "_style_spei_diff = _style_spi_diff\n",
    "\n",
    "# Styles for quality flags\n",
    "_style_probability = {\"cmap\": plt.cm.magma.resampled(30), \"vmin\": 0, \"vmax\": 1}\n",
    "_style_probability_diff = {\"cmap\": plt.cm.RdBu.resampled(15), \"vmin\": -1, \"vmax\": 1}\n",
    "_style_flag_summed = {\"cmap\": ListedColormap([\"#000000\", \"#ffffcd\", \"#f4eabb\", \"#ebd4ac\", \"#e2bc99\", \"#d5a888\", \"#c8947a\",\n",
    "                                                         \"#bf7e6b\", \"#b4685d\", \"#a7544e\", \"#9b3d3f\", \"#8d2733\", \"#800029\",]),\n",
    "                      \"norm\": BoundaryNorm(np.arange(-0.5, 13, 1), 13),\n",
    "                      \"ticks\": np.arange(0, 13, 1),\n",
    "                     }\n",
    "\n",
    "# Individual styles\n",
    "# Set up like this so they can still be edited individually\n",
    "styles = {\n",
    "    # Indexes\n",
    "    \"SPI\":  Style(**_style_spi),  \"SPI_diff\":  Style(**_style_spi_diff),\n",
    "    \"SPEI\": Style(**_style_spei), \"SPEI_diff\": Style(**_style_spei_diff),\n",
    "\n",
    "    # Quality flags\n",
    "    \"probability\": Style(**_style_probability), \"probability_diff\": Style(**_style_probability_diff),\n",
    "    \"flag_summed\": Style(**_style_flag_summed),\n",
    "}\n",
    "\n",
    "# Apply general settings\n",
    "for style in styles.values():\n",
    "    style.normalize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "The following cell contains some base helper functions (e.g. displaying in Jupyter Notebook or Jupyter Book style, adding textboxes with consistent formatting, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation: Helper functions, general\n",
    "def _glue_or_show(fig, glue_label=None):\n",
    "    try:\n",
    "        glue(glue_label, fig, display=False)\n",
    "    except TypeError:\n",
    "        plt.show()\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def _add_textbox_to_subplots(text: str, *axs: plt.Axes | ekp.Subplot, right=False) -> None:\n",
    "    \"\"\" Add a text box to each of the specified subplots. \"\"\"\n",
    "    # Get the plt.Axes for each ekp.Subplot\n",
    "    axs = [subplot.ax if isinstance(subplot, ekp.Subplot) else subplot for subplot in axs]\n",
    "\n",
    "    # Set up location\n",
    "    x = 0.95 if right else 0.05\n",
    "    horizontalalignment = \"right\" if right else \"left\"\n",
    "\n",
    "    # Add the text\n",
    "    for ax in axs:\n",
    "        ax.text(x, 0.95, text, transform=ax.transAxes,\n",
    "        horizontalalignment=horizontalalignment, verticalalignment=\"top\",\n",
    "        bbox={\"facecolor\": \"white\", \"edgecolor\": \"black\", \"boxstyle\": \"round\",\n",
    "              \"alpha\": 1})\n",
    "\n",
    "def plot_zero_line(*axs: plt.Axes) -> None:\n",
    "    \"\"\" Plot the y=0 line with consistent styling. \"\"\"\n",
    "    for ax in axs:\n",
    "        ax.axhline(0, color=\"black\", zorder=1.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "The following functions are also base helper functions, but specific to geospatial plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default geospatial domain\n",
    "# To do: Use Robinson CRS but maintain aspect ratio\n",
    "DOMAIN_GLOBAL = ekp.geo.domains.Domain.from_string(\"global\")\n",
    "# DOMAIN_GLOBAL = ekp.geo.domains.Domain.from_string(\"global\", crs=ccrs.Robinson())\n",
    "\n",
    "# Visualisation: Helper functions for geospatial plots\n",
    "def _spatial_plot_append_subplots(fig: ekp.Figure, *data: xr.Dataset, domain: Optional[AnyDomain]=None, **kwargs) -> list[ekp.Subplot]:\n",
    "    \"\"\" Plot any number of datasets into new subplots in an existing earthkit figure. \"\"\"\n",
    "    # Create subplots\n",
    "    subplots = [fig.add_map(domain=domain) for d in data]\n",
    "\n",
    "    # Plot\n",
    "    for subplot, d in zip(subplots, data):\n",
    "        subplot.grid_cells(d, **kwargs)\n",
    "\n",
    "    return subplots\n",
    "\n",
    "def decorate_fig(fig: ekp.Figure, *, title: Optional[str]=\"\") -> None:\n",
    "    \"\"\" Decorate an earthkit figure with land, coastlines, etc. \"\"\"\n",
    "    # Add progress bar because individual steps can be very slow for large plots\n",
    "    with tqdm(total=5, desc=\"Decorating\", leave=False) as progressbar:\n",
    "        fig.land()\n",
    "        progressbar.update()\n",
    "        fig.coastlines()\n",
    "        progressbar.update()\n",
    "        fig.borders()\n",
    "        progressbar.update()\n",
    "        fig.gridlines(linestyle=plt.rcParams[\"grid.linestyle\"])\n",
    "        progressbar.update()\n",
    "        fig.title(title)\n",
    "        progressbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "The following functions handle visualisation of accumulated variables such as total precipitation and water balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function: Plot any accumulated variable\n",
    "def plot_accumulated_variable(data: xr.Dataset, site: dict[str, slice], var: str, *,\n",
    "                              accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS,\n",
    "                              start_at_zero=False, var_label: Optional[str]=None,\n",
    "                              glue_label: Optional[str]=None) -> None:\n",
    "    \"\"\" Plot accumulated time series for variable `var` in multiple accumulation periods, in one `site`. \"\"\"\n",
    "    # Select data in site\n",
    "    data_site = data.sel(**site)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6), constrained_layout=True)\n",
    "    \n",
    "    # Plot data for each accumulation period\n",
    "    for period in accumulation_periods:\n",
    "        period_var = f\"{var}{period}\"\n",
    "        data_site[period_var].plot(ax=ax, label=f\"{period:2d} months\")\n",
    "    \n",
    "    # Decorate figure\n",
    "    ax.set_title(f\"{var_label} at ({site['lat']} Â°N, {site['lon']} Â°E) accumulated over different periods\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(f\"Accumulated {var_label}\")\n",
    "    ax.legend(title=\"Accumulation\\nperiod\", reverse=True, loc=\"best\")\n",
    "    if start_at_zero:  # Start y-axis at 0, e.g. for total precipitation\n",
    "        ax.set_ylim(ymin=0)\n",
    "    else:              # If values can be + or -, show the 0-line clearly\n",
    "        plot_zero_line(ax)      \n",
    "\n",
    "    # Show result\n",
    "    _glue_or_show(fig, glue_label)\n",
    "\n",
    "\n",
    "# Plot tp, wb with preset names\n",
    "plot_accumulated_precipitation = partial(plot_accumulated_variable, var=\"tp\", start_at_zero=True,  var_label=\"Total precipitation [mm]\")\n",
    "plot_accumulated_waterbalance  = partial(plot_accumulated_variable, var=\"wb\", start_at_zero=False, var_label=\"Water balance [mm]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "The following functions produce time series comparisons between datasets, such as reproduced vs. ERA5â€“Drought SPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Display SPI / SPEI categories\n",
    "def plot_index_categories(ax: plt.Axes, categories: Iterable[Iterable]) -> None:\n",
    "    \"\"\" Display SPI / SPEI categories (e.g. \"extreme drought\") on an Axes panel. \"\"\"\n",
    "    for low, high, label, colour in categories:\n",
    "        ax.axhspan(low, high, facecolor=colour, edgecolor=None, alpha=0.25)\n",
    "\n",
    "\n",
    "# Main function: Plot time series of absolute values (left) and differences (right)\n",
    "def plot_time_series_comparison(data1: xr.Dataset, data2: xr.Dataset, var: str, *,\n",
    "                                accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS,\n",
    "                                var_categories: Optional[Iterable[Iterable]]=None,  # e.g. CATEGORIES_SPI\n",
    "                                label1: str=\"ERA5â€“Drought\", label2: str=\"Reproduced\", title_suffix: Optional[str]=\"\",\n",
    "                                glue_label: Optional[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Plot time series of one variable (e.g. SPI) in two datasets.\n",
    "    Left column: Overlapping time series.\n",
    "    Right column: Differences (data2 â€“ data1).\n",
    "\n",
    "    One row for each accumulation period.\n",
    "    `var` and `accumulation_periods` are provided manually, rather than inferred,\n",
    "        to allow plotting of individual time series.\n",
    "    \"\"\"\n",
    "    # Calculate difference\n",
    "    difference = data2 - data1\n",
    "    difference_label = f\"{label2} â€“ {label1}\"\n",
    "\n",
    "    # Setup: Figure\n",
    "    ncols = 2\n",
    "    nrows = len(accumulation_periods)  # Note: not checking if accumulation periods are actually present in data1, data2\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=\"col\", squeeze=False,\n",
    "                            layout=\"constrained\", gridspec_kw={\"wspace\": 0.07}, figsize=(10, 2.7*nrows))\n",
    "\n",
    "    # Setup: Styles for line plots\n",
    "    timeseries_kwargs = [{\"color\": \"tab:blue\", \"linestyle\": \"-\", \"label\": label1},\n",
    "                         {\"color\": \"tab:orange\", \"linestyle\": \"dotted\", \"label\": label2},\n",
    "                        ]\n",
    "\n",
    "    # Plot data\n",
    "    for ax_row, period in zip(axs, accumulation_periods):\n",
    "        var_period = f\"{var}{period}\"\n",
    "\n",
    "        # Plot absolute time series\n",
    "        for data, style in zip([data1, data2], timeseries_kwargs):\n",
    "            data[var_period].plot(ax=ax_row[0], **style, zorder=2)\n",
    "\n",
    "        # Display categories and 0 line\n",
    "        plot_index_categories(ax_row[0], var_categories)\n",
    "        plot_zero_line(*ax_row)\n",
    "\n",
    "        # Plot difference\n",
    "        difference[var_period].plot(ax=ax_row[1], color=\"#004488\", zorder=2)\n",
    "        md  = nanmedian(ravel(              difference[var_period]),  axis=0).compute()\n",
    "        mad = nanmedian(ravel(xr.ufuncs.abs(difference[var_period])), axis=0).compute()\n",
    "        _add_textbox_to_subplots(f\"Median Î”: {md:+.4f}\\nMedian |Î”|: {mad:.4f}\", ax_row[1], right=True)\n",
    "        \n",
    "\n",
    "    # Decorate figure\n",
    "    for ax in axs.ravel():\n",
    "        ax.set_title(None)\n",
    "    axs[0, 0].set_title(f\"{var} values\")\n",
    "    axs[0, 1].set_title(f\"Difference ({difference_label})\")\n",
    "\n",
    "    axs[0, 0].set_ylim(-8, 8)\n",
    "    axs[0, 1].set_ylim(-1, 1)\n",
    "\n",
    "    for ax in axs[:, 0]:\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "    for ax in axs[:-1].ravel():\n",
    "        ax.set_xlabel(None)\n",
    "    for ax in axs[-1]:\n",
    "        ax.set_xlabel(\"Time\")\n",
    "    for ax_row, period in zip(axs, accumulation_periods):\n",
    "        ax_row[0].set_ylabel(f\"{var}-{period}\")\n",
    "        ax_row[1].set_ylabel(f\"{var}-{period} difference\")\n",
    "\n",
    "    fig.suptitle(f\"{label1} vs. {label2}\\nTime series for {var}{title_suffix}\")\n",
    "    fig.align_ylabels()\n",
    "\n",
    "    # Show result\n",
    "    _glue_or_show(fig, glue_label)\n",
    "\n",
    "\n",
    "# Plot SPI, SPEI with preset names\n",
    "plot_time_series_comparison_spi  = partial(plot_time_series_comparison, var=\"SPI\",  var_categories=CATEGORIES_SPI)\n",
    "plot_time_series_comparison_spei = partial(plot_time_series_comparison, var=\"SPEI\", var_categories=CATEGORIES_SPEI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "The following functions produce confusion matrices for the SPI / SPEI categories, e.g. how many \"extremely dry\" points in the reproduced dataset are also classified as \"extremely dry\" in ERA5â€“Drought:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function: Plot confusion matrices with counts (text) and fractions (colour)\n",
    "def plot_confusion_matrices(data1: xr.Dataset, data2: xr.Dataset, var: str, var_categories: Iterable[Iterable], *,  # e.g. CATEGORIES_SPI\n",
    "                            label1: str=\"ERA5â€“Drought\", label2: str=\"Reproduced\", title_suffix: Optional[str]=\"\",\n",
    "                            glue_label: Optional[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Plot confusion matrices of categories for one variable (e.g. SPI \"extremely dry\") in two datasets.\n",
    "    One panel for each data_variable that occurs in both datasets and matches the variable `var`,\n",
    "        e.g. different accumulation periods.\n",
    "    \"\"\"\n",
    "    # Align datasets\n",
    "    data1, data2 = align_data_variables(data1, data2, var=var)  # Ensure only matching variables\n",
    "    data1, data2 = xr.align(data1, data2)  # Ensure only matching points\n",
    "    # data1 = data1.transpose(*[dim for dim in data2.dims])  # Ensure same order of dimensions\n",
    "    data1, data2 = [data.stack(point=data2.dims) for data in (data1, data2)]  # Make 1D list\n",
    "\n",
    "    # Apply categorisation\n",
    "    categories1, categories2 = [categorise_dataset(data, var_categories) for data in (data1, data2)]\n",
    "\n",
    "    # Create figure\n",
    "    n_categories = len(var_categories)\n",
    "    category_index = np.arange(n_categories)\n",
    "    n_datavars = len(data2.data_vars)\n",
    "    ncols = 2\n",
    "    nrows = (n_datavars // ncols) + 1\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5*ncols, 4*nrows),\n",
    "                            gridspec_kw={\"hspace\": 0.01, \"wspace\": 0.01}, layout=\"constrained\")\n",
    "\n",
    "    # Loop over data variables\n",
    "    for ax, data_var in zip(axs.ravel(), data2.data_vars):\n",
    "        # Create confusion matrix from categorisation values\n",
    "        matrix_absolute = confusion_matrix(categories1[data_var], categories2[data_var], labels=category_index+1)\n",
    "        matrix_relative = confusion_matrix(categories1[data_var], categories2[data_var], labels=category_index+1, normalize=\"true\")  # Normalised by rows (data1)\n",
    "        # For operational code, you would add an assert here to check that the sum of elements in matrix_absolute\n",
    "        # corresponds to the number of not-NaN elements of the input data\n",
    "\n",
    "        # Plot fractions for color background\n",
    "        image = ax.imshow(matrix_relative, cmap=plt.cm.Blues.resampled(10), vmin=0.0, vmax=1.0)\n",
    "\n",
    "        # Annotate with absolute counts\n",
    "        for i in category_index:\n",
    "            for j in category_index:\n",
    "                counts_abs, counts_rel = matrix_absolute[i, j], matrix_relative[i, j]\n",
    "                ax.text(j, i, f\"{counts_abs:.0f}\", ha=\"center\", va=\"center\",  # Add text\n",
    "                        color=\"white\" if counts_rel > 0.5 else \"black\",       # Colour based on background\n",
    "                        fontsize=9, clip_on=True)                             # Text settings\n",
    "\n",
    "        # Label based on data variable, adding a hyphen\n",
    "        data_var_hyphenated = data_var.replace(var, var+\"-\")\n",
    "        ax.set_title(data_var_hyphenated)\n",
    "\n",
    "    # Add category labels\n",
    "    category_labels = [category[2] for category in var_categories]\n",
    "    for ax in axs.ravel():\n",
    "        ax.set_xticks(category_index, labels=category_labels, rotation=30, ha=\"right\", fontsize=9)\n",
    "        ax.set_yticks(category_index, labels=category_labels, fontsize=9)\n",
    "        ax.grid(False)\n",
    "        ax.xaxis.set_inverted(True)\n",
    "\n",
    "    # Add a single shared colorbar showing row-wise fraction\n",
    "    cbar = fig.colorbar(image, ax=axs, location=\"right\", fraction=0.05, pad=0.05)# shrink=0.6)\n",
    "    cbar.set_label(f\"Fraction of {label1} data (rows)\")\n",
    "\n",
    "    # Hide extra panels if odd number of variables\n",
    "    for ax in axs.ravel()[n_datavars:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    # Decorate figure\n",
    "    fig.suptitle(f\"{label1} vs. {label2}\\nConfusion matrices for {var}{title_suffix}\")\n",
    "    fig.supxlabel(label2, fontweight=\"bold\", fontsize=16)\n",
    "    fig.supylabel(label1, fontweight=\"bold\", fontsize=16)\n",
    "\n",
    "    # Show result\n",
    "    _glue_or_show(fig, glue_label)\n",
    "\n",
    "\n",
    "# Plot SPI, SPEI with preset names\n",
    "plot_confusion_matrices_spi  = partial(plot_confusion_matrices, var=\"SPI\",  var_categories=CATEGORIES_SPI)\n",
    "plot_confusion_matrices_spei = partial(plot_confusion_matrices, var=\"SPEI\", var_categories=CATEGORIES_SPEI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "The following functions display geospatial comparisons between index values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function: Plot maps of absolute values (left, middle) and differences (right)\n",
    "def plot_geospatial_comparison(data1: xr.Dataset, data2: xr.Dataset, var: str, time: str, *,\n",
    "                               label1: str=\"ERA5â€“Drought\", label2: str=\"Reproduced\", title_suffix: Optional[str]=\"\",\n",
    "                               domain: AnyDomain=DOMAIN_GLOBAL,\n",
    "                               time_dimension: str=\"time\",\n",
    "                               glue_label: Optional[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Plot geospatial views of categories for one variable (e.g. SPI \"extremely dry\") in two datasets.\n",
    "    One panel for each data_variable that occurs in both datasets and matches the variable `var`,\n",
    "        e.g. different accumulation periods.\n",
    "    \"\"\"\n",
    "    # Select data for one timestamp\n",
    "    data1, data2 = [data.sel({time_dimension: time})\n",
    "                    for data in (data1, data2)]\n",
    "\n",
    "    # Only look at variables that are in both datasets\n",
    "    # Note: Not checking for matching coordinates / xr.align\n",
    "    data1, data2 = align_data_variables(data1, data2, var=var)\n",
    "\n",
    "    # Calculate difference\n",
    "    difference = data2 - data1\n",
    "    difference_label = f\"{label2} â€“ {label1}\"\n",
    "\n",
    "    # Setup: Figure\n",
    "    nrows = len(data2.data_vars)\n",
    "    ncols = 3  # data1, data2, diff\n",
    "    fig = ekp.Figure(rows=nrows, columns=ncols, size=(4*ncols, 3.7*nrows))\n",
    "\n",
    "    # Loop over variables (e.g. accumulation periods)\n",
    "    for data_var in data2.data_vars:\n",
    "        # Plot index\n",
    "        for data in (data1, data2):\n",
    "            subplot = fig.add_map(domain=domain)\n",
    "            subplot.grid_cells(data, z=data_var, style=styles[var])\n",
    "\n",
    "        # Plot difference\n",
    "        subplot_diff = fig.add_map(domain=domain)\n",
    "        subplot_diff.grid_cells(difference, z=data_var, style=styles[var+\"_diff\"])\n",
    "\n",
    "    # Label dataset at the top\n",
    "    for subplot, label in zip(fig.subplots[:ncols], [label1, label2, difference_label]):\n",
    "        subplot.title(label)\n",
    "\n",
    "    # Label indicator in the corner\n",
    "    for subplots, data_var in zip(batched(fig.subplots, ncols), data2.data_vars):\n",
    "        _add_textbox_to_subplots(data_var, *subplots)\n",
    "\n",
    "    # Legend at the bottom\n",
    "    for subplot in fig.subplots[-ncols:]:\n",
    "        subplot.legend(label=var)\n",
    "\n",
    "    # Decorate figure\n",
    "    time_for_title = time[:7] # Could be changed to proper datetime format, e.g. %B %Y\n",
    "    title = f\"{label1} vs. {label2}\\nGeospatial comparison for {var} ({domain.name}, {time_for_title}){title_suffix}\"\n",
    "    decorate_fig(fig, title=title)\n",
    "\n",
    "    # Show result\n",
    "    _glue_or_show(fig.fig, glue_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "The following functions display geospatial comparisons between quality flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function: Plot maps of absolute values (left, middle) and differences (right)\n",
    "def plot_geospatial_comparison_quality_flags(probability1: xr.Dataset, probability2: xr.Dataset, var: str, example_month: int, *,\n",
    "                                             masks: Optional[dict[str, Iterable[xr.Dataset]]]=None,  # e.g. p0 < 0.33 or alpha > 0.05\n",
    "                                             label1: str=\"ERA5â€“Drought\", label2: str=\"Reproduced\", title_suffix: Optional[str]=\"\",\n",
    "                                             flag_label: Optional[str]=None,\n",
    "                                             domain: AnyDomain=DOMAIN_GLOBAL, shared_mask: Optional[xr.DataArray]=True,\n",
    "                                             month_dimension: str=\"month\",\n",
    "                                             glue_label: Optional[str]=None) -> None:\n",
    "    \"\"\"\n",
    "    Plot geospatial views of pre-calculated quality flags in two datasets.\n",
    "    probability1, probability2 are datasets with quality flag probabilities.\n",
    "        These are displayed for variable `var` (e.g. tp3, SPEI3) in `example_month` (e.g. 12).\n",
    "    Additional masks created from the quality flags can be included, e.g. where p0 <= 0.33.\n",
    "        These are ingested as a dict with str keys (the label for that mask) and an Iterable of xr.Datasets,\n",
    "        corresponding to the mask in probability1 and in probability2.\n",
    "        Following Keune+25 Fig 3, masks are displayed as their sum over all 12 months (left, middle)\n",
    "        as well as the number of mismatched months (right).\n",
    "    An additional `shared_mask`, e.g. a land-sea mask, can be applied to all panels.\n",
    "    \"\"\"\n",
    "    # Select data for one month\n",
    "    probability1, probability2 = [prob.sel({month_dimension: example_month})\n",
    "                                  for prob in (probability1, probability2)]\n",
    "\n",
    "    # Calculate difference\n",
    "    probability_difference = probability2 - probability1\n",
    "    difference_label = f\"{label2} â€“ {label1}\"\n",
    "\n",
    "    # Setup: Figure\n",
    "    nrows = 1 + len(masks)  # 1 for probabilities + 1 each per mask\n",
    "    ncols = 3  # data1, data2, diff\n",
    "    figsize = (4*ncols, 3*nrows) if domain.name == \"Global\" else (4*ncols, 3.7*nrows)  # Adjust aspect ratio for global plots\n",
    "    fig = ekp.Figure(rows=nrows, columns=ncols, size=figsize)\n",
    "\n",
    "    # Plot probabilities\n",
    "    for prob in (probability1, probability2):\n",
    "        subplot = fig.add_map(domain=domain)\n",
    "        subplot.grid_cells(prob.where(shared_mask), z=var, style=styles[\"probability\"])\n",
    "\n",
    "    subplot = fig.add_map(domain=domain)\n",
    "    subplot.grid_cells(probability_difference.where(shared_mask), z=var, style=styles[\"probability_diff\"])\n",
    "\n",
    "    # Plot masks (if any)\n",
    "    for key, list_of_datasets in masks.items():\n",
    "        # Plot masks\n",
    "        for mask in list_of_datasets:\n",
    "            # Add up per month and invert (to get number of *flagged* rather than *allowed* points)\n",
    "            flagged_months = count_flagged_months(mask)\n",
    "\n",
    "            # Plot\n",
    "            subplot = fig.add_map(domain=domain)\n",
    "            subplot.grid_cells(flagged_months.where(shared_mask), z=var, style=styles[\"flag_summed\"])\n",
    "\n",
    "        # Find mismatched months\n",
    "        mask1, mask2 = list_of_datasets\n",
    "        masks_not_matching = (mask1 != mask2)\n",
    "        months_not_matching = count_flagged_months(masks_not_matching, invert=False)\n",
    "\n",
    "        # Plot number of mismatched months\n",
    "        subplot = fig.add_map(domain=domain)\n",
    "        subplot.grid_cells(months_not_matching.where(shared_mask), z=var, style=styles[\"flag_summed\"])\n",
    "\n",
    "    # Label dataset at the top\n",
    "    for subplot, label in zip(fig.subplots[:ncols], [label1, label2, difference_label]):\n",
    "        subplot.title(label)\n",
    "\n",
    "    # Label masks in the corner\n",
    "    for subplots, key in zip(batched(fig.subplots[ncols:], ncols), masks.keys()):\n",
    "        _add_textbox_to_subplots(key, *subplots)\n",
    "\n",
    "    # Legend at the sides\n",
    "    fig.subplots[0].legend(label=flag_label, location=\"left\")\n",
    "    fig.subplots[ncols-1].legend(label=\"Difference\", location=\"right\")\n",
    "    for subplot in fig.subplots[ncols::ncols]:\n",
    "        subplot.legend(label=\"Flagged months\", location=\"left\")\n",
    "    for subplot in fig.subplots[2*ncols-1::ncols]:\n",
    "        subplot.legend(label=\"Mismatched months\", location=\"right\")\n",
    "\n",
    "    # Decorate figure\n",
    "    month_for_title = MONTHS_NAMED[example_month]\n",
    "    title = f\"{label1} vs. {label2}\\nGeospatial comparison of {flag_label}\\n({domain.name}, {month_for_title}, {var}){title_suffix}\"\n",
    "    decorate_fig(fig, title=title)\n",
    "\n",
    "    # Show result\n",
    "    _glue_or_show(fig.fig, glue_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "(section-general_setup)=\n",
    "### 2. General setup\n",
    "This section provides some of the setup for the further analysis,\n",
    "including\n",
    "the timespan and sites to investigate\n",
    "as well as\n",
    "the CDS data downloads.\n",
    "This ensures that the following sections\n",
    "(SPI, SPEI, etc.)\n",
    "can be run independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "#### 2.1 Analysis setup\n",
    "In this assessment,\n",
    "we will calculate SPI and SPEI for the years 1940â€“2024.\n",
    "For the reference period,\n",
    "we will use the World Meteorological Organization (WMO) current standard 30-year reference period of 1991â€“2020,\n",
    "which is also used in ERA5-Drought.\n",
    "Both of these date ranges can be adjusted in the cell below when running the analysis yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preferred analysis and reference periods\n",
    "years           = (1940, 2024)  # Years for the analysis (inclusive)\n",
    "years_reference = (1991, 2020)  # Years for the reference period (inclusive)\n",
    "\n",
    "# Derived variables for convenience:\n",
    "reference_window = {\"time\": slice(f\"{years_reference[0]}-01-01\", f\"{years_reference[1]}-12-01\"),}  #  Slice (1991-01-01, 2020-12-01)\n",
    "entire_window    = {\"time\": slice(f\"{years[0]}-01-01\",           f\"{years[1]}-12-01\"),}            #  Slice (1940-01-01, 2024-12-01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "Some parts of the analysis will be performed globally, others in specific sites for computational reasons.\n",
    "This notebook uses Addis Ababa in Ethiopia (9.00 Â°N, 38.75 Â°E) and the Horn of Africa region around it as an example;\n",
    "a different site can be chosen when running the notebook yourself by editing the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preferred site for the time series analysis\n",
    "example_site = {\"lat\": 9.00, \"lon\": 38.75}  # Compatible with ERA5â€“Drought and pre-processed ERA5 data\n",
    "\n",
    "# Define the size and name of the surrounding region\n",
    "example_region_size = 12  # Degrees to either side of the example side\n",
    "example_region_size = 2  # TESTING ONLY\n",
    "label_region = \"Horn of Africa\"\n",
    "\n",
    "# Define the year for the example region analysis\n",
    "snapshot_year = 2024\n",
    "\n",
    "# Derived variables for convenience\n",
    "# Example site\n",
    "label_site = f\"({example_site['lat']:.2f} Â°N, {example_site['lon']:.2f} Â°E)\"\n",
    "request_site = request_data_for_one_site(lat=example_site[\"lat\"], lon=example_site[\"lon\"])\n",
    "\n",
    "# Example region\n",
    "example_region = {\"lat\": slice(example_site[\"lat\"] + example_region_size, example_site[\"lat\"] - example_region_size),  # Decreasing order\n",
    "                  \"lon\": slice(example_site[\"lon\"] - example_region_size, example_site[\"lon\"] + example_region_size),} # Increasing order\n",
    "request_region = request_data_for_one_site(lat=example_site[\"lat\"], lon=example_site[\"lon\"], half_width=example_region_size)\n",
    "domain_region = ekp.geo.domains.Domain(\n",
    "    bbox=[example_region[\"lon\"].start, example_region[\"lon\"].stop, example_region[\"lat\"].start, example_region[\"lat\"].stop],\n",
    "    name=label_region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### 2.2 CDS download setup\n",
    "Having defined our target years, we can now define our CDS request.\n",
    "First, we define templates with some default parameters\n",
    "(e.g. years, data format)\n",
    "that will also be used later in the notebook.\n",
    "Additional information for specific downloads\n",
    "(e.g. variable, data stream)\n",
    "is mixed into this template where relevant.\n",
    "\n",
    "This notebook uses [earthkit-data](https://github.com/ecmwf/earthkit-data) to download files from the CDS.\n",
    "If you intend to run this notebook multiple times, it is highly recommended that you [enable caching](https://earthkit-data.readthedocs.io/en/latest/guide/caching.html) to prevent having to download the same files multiple times.\n",
    "If you prefer not to use earthkit, the following requests can also be used with the [cdsapi module](https://cds.climate.copernicus.eu/how-to-api#linux-use-client-step).\n",
    "In either case (earthkit-data or cdsapi), it is required to set up a CDS account and API key as explained [on the CDS website](https://cds.climate.copernicus.eu/how-to-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "##### ERA5\n",
    "We start by setting up a template for requests from the [_Complete ERA5 global atmospheric reanalysis_](https://doi.org/10.24381/cds.143582cf) dataset, from which we will obtain precipitation and (for SPEI) potential evapotranspiration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_ERA5 = \"reanalysis-era5-complete\"\n",
    "\n",
    "request_era5_template = {\n",
    "    \"class\": \"ea\",            # Default for ERA5\n",
    "    # Dates: ERA5 takes these in the format 19400101/19400201/.../20241101/20241201\n",
    "    # The following lines generate a string in said format for the chosen year range\n",
    "    \"date\": \"/\".join(f\"{year}{month:02}01\"                    # yyyymm01 format\n",
    "                     for year in range(years[0], years[1]+1)  # All years in specified range, inclusive\n",
    "                     for month in MONTHS),                    # All calendar months\n",
    "    \"expver\": \"1\",            # ERA5 consolidated data\n",
    "    \"levtype\": \"sfc\",         # Surface\n",
    "    \"grid\": \"0.25/0.25\",      # Grid: 0.25Â° by 0.25Â° (interpolated from native)\n",
    "    \"type\": \"fc\",             # Forecast\n",
    "    \"data_format\": \"netcdf\",  # NetCDF data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "##### ERA5â€“Drought\n",
    "We also set up a template for requests from ERA5â€“Drought ([_Monthly drought indices from 1940 to present derived from ERA5 reanalysis_](https://doi.org/10.24381/9bea5e16)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_ERA5_DROUGHT = \"derived-drought-historical-monthly\"\n",
    "\n",
    "# General request template\n",
    "request_era5drought_template = {\n",
    "    \"version\": \"1_0\",                                # Current version\n",
    "    \"dataset_type\": \"consolidated_dataset\",          # Only use consolidated data, i.e. not ERA5T-derived\n",
    "    \"month\": [f\"{month:02d}\" for month in MONTHS],   # All calendar months\n",
    "}\n",
    "\n",
    "# Request for SPI or SPEI data\n",
    "request_era5drought_index = {\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"accumulation_period\": [str(p) for p in ACCUMULATION_PERIODS],\n",
    "    \"year\": [f\"{year}\" for year in range(years[0], years[1]+1)],    \n",
    "} | request_era5drought_template\n",
    "\n",
    "# Request for quality flags\n",
    "request_era5drought_flag = {\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "} | request_era5drought_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### 2.3 Download land-sea mask\n",
    "ERA5â€“Drought uses a land-sea mask to only provide values over land.\n",
    "Here, we download the [ERA5 land-sea mask](https://confluence.ecmwf.int/display/FUG/Section+2.1.3.1+Land-Sea+Mask) from the [_ERA5 hourly data on single levels from 1940 to present_](https://doi.org/10.24381/cds.adbb2d47) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_ERA5_LANDSEAMASK = \"reanalysis-era5-single-levels\"\n",
    "\n",
    "request_landsea_mask = {\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"variable\": \"land_sea_mask\",\n",
    "    \"date\": \"2000-01-01\",  # Does nothing but is required in CDS form\n",
    "    \"time\": \"00:00\",       # Does nothing but is required in CDS form\n",
    "    \"format\": \"netcdf\",\n",
    "    \"download_format\": \"zip\",\n",
    "}\n",
    "\n",
    "# Download mask\n",
    "LAND = ekd.from_source(\"cds\", ID_ERA5_LANDSEAMASK, request_landsea_mask)\n",
    "LAND = LAND.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "\n",
    "# Convert longitude from 0..360 to â€“180..180\n",
    "LAND = rename_era5_dimensions(LAND)\n",
    "LAND = longitude_360_to_180(LAND, \"lon\")\n",
    "\n",
    "# Drop time dimension\n",
    "LAND = LAND.squeeze(\"time\")\n",
    "\n",
    "# Convert to boolean mask: Land if > 0.5 (following ERA5 convention)\n",
    "LAND = (LAND[\"lsm\"] > 0.5) # Land considered to be full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-spi)=\n",
    "### 3. SPI comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### 3.1 Download monthly precipitation data from ERA5\n",
    "First, the monthly-mean total precipitation data from the ERA5 reanalysis is downloaded.\n",
    "Generally, one would use the [_ERA5 monthly averaged data on single levels from 1940 to present_](https://doi.org/10.24381/cds.f17050d7) dataset for this, which provides pre-calculated monthly means at 0.25Â° by 0.25Â° resolution.\n",
    "For this assessment,\n",
    "to be as close to the ERA5â€“Drought data processing pipeline as possible\n",
    "and\n",
    "to make use of some of MARS's functionalities (see [](section-ensemble)),\n",
    "we instead use the [_Complete ERA5 global atmospheric reanalysis_](https://doi.org/10.24381/cds.143582cf) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "We want to download \n",
    "total precipitation data (variable `228.128`)\n",
    "from the\n",
    "`moda` stream (monthly-mean reanalysis data),\n",
    "so we mix this information into the request template set up [previously](section-general_setup)\n",
    "and submit the request to the CDS.\n",
    "More information about the format for these requests is available in the [MARS ERA5 catalogue](https://apps.ecmwf.int/data-catalogues/era5/?class=ea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_precipitation_moda = {\n",
    "    \"param\": \"228.128\",  # Variable: Total precipitation\n",
    "    \"stream\": \"moda\",    # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download data and load into xarray\n",
    "data_era5_precipitation_cds = ekd.from_source(\"cds\", ID_ERA5, request_era5_precipitation_moda)  # Download as field list\n",
    "data_era5_precipitation_cds = data_era5_precipitation_cds.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "\n",
    "# Pre-process to desired format\n",
    "# Note the change in variable name\n",
    "data_era5_precipitation_preprocessed = preprocess_era5(data_era5_precipitation_cds)\n",
    "\n",
    "# Display in notebook\n",
    "data_era5_precipitation_preprocessed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### 3.2 Calculate total precipitation over accumulation periods\n",
    "As explained above,\n",
    "SPI and SPEI are commonly evaluated over different accumulation periods.\n",
    "ERA5â€“Drought provides both indices for periods of 1, 3, 6, 12, 24, 36, and 48 months.\n",
    "Here, we perform this accumulation by calculating the total precipitation over the previous _p_ months\n",
    "at every coordinate and timestamp in the ERA5 data,\n",
    "for each accumulation period _p_.\n",
    "The implementation used here\n",
    "(which can be found in [](section-code_setup))\n",
    "accounts for the variable number of days in each month,\n",
    "including leap years.\n",
    "\n",
    "The resulting accumulated time series for the example site defined in [](section-general_setup),\n",
    "Addis Ababa in Ethiopia in our example,\n",
    "are displayed in Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-tp-accumulated>`.\n",
    "This example clearly shows how shorter accumulation periods probe short-term effects such as seasonality,\n",
    "which are smoothed out in longer periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Accumulate total precipitation\n",
    "data_era5_precipitation = accumulate(data_era5_precipitation_preprocessed, \"tp\")\n",
    "\n",
    "# Display result\n",
    "data_era5_precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display accumulated precipitation at example site\n",
    "plot_accumulated_precipitation(data_era5_precipitation, example_site,\n",
    "                               glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-tp-accumulated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-tp-accumulated\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-tp-accumulated\"\n",
    "\n",
    "Total precipitation from ERA5 accumulated over different periods, for the example site of Addis Ababa, Ethiopia.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### 3.3 Fit gamma distribution by calendar month\n",
    "At the core of SPI is the assumption that (monthly) total precipitation values follow a gamma distribution.\n",
    "Here, this distribution is fitted to data within the reference period (1991â€“2020 by default, see above).\n",
    "A separate distribution is fitted for each calendar month,\n",
    "i.e.\n",
    "we fit one distribution for all 30 Januaries in the reference period,\n",
    "another for all 30 Februaries,\n",
    "and so on.\n",
    "This separation allows the resulting index to account for seasonal differences.\n",
    "This is then repeated for each accumulation period.\n",
    "\n",
    "Here, we use\n",
    "[scipy.stats.gamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html)\n",
    "to fit the gamma distribution.\n",
    "This returns three parameters for each calendar month and accumulation period, namely\n",
    "shape (_Î±_), location (_Î¼_), and scale (_Î²_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Select data in reference window\n",
    "data_era5_precipitation_reference = data_era5_precipitation.sel(**reference_window)\n",
    "\n",
    "# Fit gamma distribution\n",
    "spi_parameters = fit_monthly_spi(data_era5_precipitation_reference)\n",
    "\n",
    "# Display result\n",
    "spi_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "The observed (accumulated) total precipitation values along the entire time series\n",
    "(1940â€“2024 in our example)\n",
    "are then compared to the fitted parameters.\n",
    "This is quantified in terms of where the observed values fall on the cumulative distribution function (CDF):\n",
    "\n",
    "$$\n",
    "P(X \\leq x) = F_X(x ; \\alpha, \\mu, \\beta) = \\int_{-\\infty}^{x}f(y ; \\alpha, \\mu, \\beta) \\, \\text{d}y\n",
    "$$\n",
    "\n",
    "Where\n",
    "_P(X â‰¤ x)_ is the probability for a randomly drawn total precipitation _X_ to be equal to or less than the observed value _x_;\n",
    "_F{sub}`X`_ is the CDF for a gamma distribution with shape _Î±_, location _Âµ_, and scale _Î²_;\n",
    "and\n",
    "the CDF is equal to the integral of the corresponding probability density function up to _x_.\n",
    "\n",
    "As before, there is a distribution for each calendar month, for each accumulation period.\n",
    "\n",
    "Note that actually evaluating the CDF\n",
    "â€“ as opposed to [queueing it up in dask](https://docs.xarray.dev/en/stable/user-guide/dask.html), as done here â€“\n",
    "can be slow, especially for a large dataset like global ERA5 precipitation.\n",
    "As such, if you are interested in a subset of the data,\n",
    "such as a specific site or period in time,\n",
    "it may be best to subset your data _before_ calculating the CDF rather than afterwards.\n",
    "An example of this is provided below in the comparison with ERA5â€“Drought SPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CDF time series\n",
    "cdf = compute_cdf_spi(data_era5_precipitation, spi_parameters)\n",
    "\n",
    "# Display result\n",
    "cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### 3.4 Adjust for months with zero precipitation\n",
    "Sometimes, zero (or near-zero) monthly precipitation is observed,\n",
    "particularly in dry areas like deserts.\n",
    "Maintaining the desired statistics,\n",
    "such as the mean SPI in the reference period being 0,\n",
    "requires adjusting the CDF to account for these zero-precipitation months.\n",
    "\n",
    "ERA5â€“Drought follows the [[Stagge+15](https://doi.org/10.1002/joc.4267)] method to adjust the CDF.\n",
    "First, the probability of zero precipitation is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\bar{p_0} = \\frac{n_0 + 1}{2 (n + 1)}\n",
    "$$\n",
    "\n",
    "where\n",
    "_n{sub}`0`_ is the number of months with zero precipitation in the reference period,\n",
    "and\n",
    "_n_ is the total number of months in the reference period (here 30 for 1991â€“2020).\n",
    "While some datasets define \"zero precipitation\" using a slightly higher threshold\n",
    "(e.g. less than 0.1 mm)\n",
    "to account for measurement uncertainty,\n",
    "ERA5â€“Drought uses a threshold of exactly 0 mm total precipitation in the underpinning ERA5 data.\n",
    "\n",
    "Having calculated the probability of zero precipitation, the CDF is adjusted as follows:\n",
    "\n",
    "$$\n",
    "F'_X(x ; \\alpha, \\mu, \\beta) =\n",
    "\\begin{cases}\n",
    "\\bar{p_0} + (1 - \\bar{p_0}) \\, F_X(x; \\alpha, \\mu, \\beta), & x > 0, \\\\\n",
    "\\frac{n_0 + 1}{2(n+1)}, & x = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As before, this adjustment is carried out individually for each calendar month and accumulation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Adjust CDF\n",
    "cdf_adjusted = adjust_cdf_for_zero_precipitation(cdf, data_era5_precipitation_reference)\n",
    "\n",
    "# Display result\n",
    "cdf_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "#### 3.5 Compute SPI\n",
    "Finally,\n",
    "SPI values\n",
    "for each data point (latitude, longitude, time)\n",
    "are calculated from the adjusted CDF values _F'{sub}`X`_ by transforming to a standard normal distribution.\n",
    "The function [scipy.stats.norm.ppf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) is used to calculate the inverse CDF _Î¦{sup}`-1`_ of the normal distribution:\n",
    "\n",
    "$$\n",
    "\\text{SPI}(x) = \\Phi^{-1}\\left(F'_X(x ; \\alpha, \\mu, \\beta)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SPI from adjusted CDF\n",
    "spi_reproduced = cdf_to_spi(cdf_adjusted)\n",
    "\n",
    "# Display result\n",
    "spi_reproduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### 3.6 SPI comparison: Time series in example site\n",
    "Having reproduced the SPI index from ERA5 precipitation data following the ERA5â€“Drought methodology,\n",
    "we can now compare the results to determine the reproducibility of ERA5â€“Drought.\n",
    "We first compare the datasets over time at the example site defined in [](section-general_setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "The previous sections set up the SPI reproduction pipeline for the entire ERA5 precipitation dataset (global, 1940â€“2024).\n",
    "Because actually executing said calculation takes a long time,\n",
    "as noted in the CDF subsection,\n",
    "here we sub-select the downloaded ERA5 precipitation data and calculate SPI for the example site only.\n",
    "For convenience, the full SPI pipeline is wrapped into a single `calculate_spi_from_era5` function,\n",
    "as defined in [](section-code_setup).\n",
    "\n",
    "Note that while the two may seem equivalent,\n",
    "downloading the ERA5 precipitation data for the example site only and then running the SPI pipeline,\n",
    "rather than downloading the entire dataset and subselecting for the example site,\n",
    "produces different results.\n",
    "This is likely caused by the inner workings of the MARS regridding function.\n",
    "For consistency with ERA5â€“Drought, it is necessary to download the global ERA5 dataset and subselect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Select only desired site\n",
    "data_era5_precipitation_site = data_era5_precipitation_preprocessed.sel(**example_site)\n",
    "\n",
    "# Calculate SPI\n",
    "spi_reproduced_site = calculate_spi_from_era5(data_era5_precipitation_site, reference_window=reference_window)\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spi_reproduced_site = spi_reproduced_site.persist()\n",
    "\n",
    "# Display result\n",
    "spi_reproduced_site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "Next, we download the corresponding data from ERA5â€“Drought.\n",
    "Because of size limits on the CDS,\n",
    "the time series must be downloaded in parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Main request, based on template and example site\n",
    "request_era5drought_spi = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "} | request_era5drought_index | request_site\n",
    "\n",
    "# Split into batches of up to 20 years each\n",
    "subrequests_era5drought_spi = batch_requests(request_era5drought_spi, n=20)\n",
    "\n",
    "# Download data and load into xarray\n",
    "spi_era5drought = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, *subrequests_era5drought_spi)\n",
    "spi_era5drought = spi_era5drought.to_xarray(compat=\"equals\", join=\"outer\")\n",
    "spi_era5drought = spi_era5drought.chunk({\"time\": -1})  # Full time series in one chunk\n",
    "\n",
    "# Select only desired site (remove margins)\n",
    "spi_era5drought = spi_era5drought.sel(**example_site)\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spi_era5drought = spi_era5drought.persist()\n",
    "\n",
    "# Display result\n",
    "spi_era5drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "First, we examine the per-point difference in SPI between ERA5â€“Drought and the reproduction.\n",
    "This comparison is performed separately for each calendar month and each accumulation period\n",
    "to reflect the fitting process.\n",
    "We calculate the median difference _Î”_ and median absolute difference _|Î”|_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SPI median differences\n",
    "spi_difference_site = comparison_monthly_statistics(spi_era5drought, spi_reproduced_site)\n",
    "\n",
    "# Display with style\n",
    "display_monthly_statistics(spi_difference_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "**Description of comparisons**\n",
    "\n",
    "Table for quantitative differences\n",
    "\n",
    "Median and absolute median difference for each accumulation window across months are then calculated  \n",
    "\n",
    "**TODO point-to-make**: SPI is only discrepant for certain months (but that this discrepancy is still quite small)- since fitting is by month only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "**Description of figures**\n",
    "\n",
    "**TODO point-to-make**: SPI for location in Addis Ababa shows excellent agreement across entire time series. Showed that our methodology is consistent with that of ERA5 and reproduceable.  \n",
    "Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-timeseries>`  \n",
    "Same seen in different sites (not shown here)  \n",
    "\n",
    "A confusion matrix is generated to compare drought classifications for the single, above location, between the ERA5â€‘Drought dataset and the reproduced dataset, capturing any discrepancies. The matrix is computed over the full time period (1940â€“2024), where each count corresponds to a single monthly timestep.  \n",
    "Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion>`  \n",
    "**TODO point-to-make**: and that in fact, the indices fall under the same drought severity classification for all timestamps at Addis Ababa.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series comparison (absolute values and differences)\n",
    "plot_time_series_comparison_spi(spi_era5drought, spi_reproduced_site, title_suffix=f\" at {label_site}\",\n",
    "                                glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-timeseries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix comparison\n",
    "# e.g. are all \"extremely dry\" data in ERA5â€“Drought also \"extremely dry\" in the reproduction?\n",
    "plot_confusion_matrices_spi(spi_era5drought, spi_reproduced_site, title_suffix=f\" at {label_site}\",\n",
    "                            glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Time series\n",
    ":sync: timeseries\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-timeseries\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-timeseries\"\n",
    "\n",
    "SPI time series downloaded from ERA5â€“Drought and reproduced from ERA5 precipitation data (left) and the difference between the two (right), for the example site of Addis Ababa, Ethiopia.\n",
    "Colours in the left-hand column correspond to SPI categories (e.g. \"extremely dry\") in [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)].\n",
    "```\n",
    ":::\n",
    ":::{tab-item} Confusion matrix\n",
    ":sync: confusion\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion\"\n",
    "\n",
    "Confusion matrices for SPI categories from ERA5â€“Drought vs. reproduced from ERA5, for the example site of Addis Ababa, Ethiopia, in 1940â€“2024.\n",
    "```\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "#### 3.7 SPI comparison: Regional snapshot\n",
    "Next, we investigate spatial patterns in SPI and the difference therein across a wider region around the example site.\n",
    "This region is defined in [](section-general_setup).\n",
    "In this example, we look at part of the Horn of Africa using a box of 12Â° in all directions around Addis Ababa, Ethiopia.\n",
    "To reduce computing requirements, the comparison is performed for one year only,\n",
    "here 2024, again defined in [](section-general_setup).\n",
    "\n",
    "As in the time series comparison,\n",
    "we subselect the desired data from ERA5 and calculate the corresponding SPI\n",
    "and\n",
    "download SPI from ERA5â€“Drought for the desired region and time span:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Select only desired site\n",
    "data_era5_precipitation_region = data_era5_precipitation_preprocessed.sel(**example_region)\n",
    "\n",
    "# Window in which to evaluate SPI\n",
    "evaluation_window = {\"time\": slice(f\"{snapshot_year}-01-01\", f\"{snapshot_year}-12-01\")}  #  Slice (2024-01-01, 2024-12-01)\n",
    "\n",
    "# Calculate SPI\n",
    "spi_reproduced_region = calculate_spi_from_era5(data_era5_precipitation_region, reference_window=reference_window,\n",
    "                                                evaluation_window=evaluation_window)\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spi_reproduced_region = spi_reproduced_region.persist()\n",
    "\n",
    "# Display result\n",
    "spi_reproduced_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Main request, based on template and example region\n",
    "request_era5drought_spi = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "} | request_era5drought_index | request_region\n",
    "\n",
    "# Select only desired year\n",
    "request_era5drought_spi = request_era5drought_spi | {\"year\": [f\"{snapshot_year}\"],}\n",
    "\n",
    "# Download data and load into xarray\n",
    "spi_era5drought = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, request_era5drought_spi)\n",
    "spi_era5drought = spi_era5drought.to_xarray(compat=\"equals\", join=\"outer\")\n",
    "spi_era5drought = spi_era5drought.chunk({\"time\": -1})  # Full time series in one chunk\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spi_era5drought = spi_era5drought.persist()\n",
    "\n",
    "# Display result\n",
    "spi_era5drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "First, we examine the per-point difference in SPI between ERA5â€“Drought and the reproduction.\n",
    "This comparison is performed separately for each calendar month and each accumulation period\n",
    "to reflect the fitting process.\n",
    "We calculate the median difference _Î”_ and median absolute difference _|Î”|_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SPI median differences\n",
    "spi_difference_region = comparison_monthly_statistics(spi_era5drought, spi_reproduced_region)\n",
    "\n",
    "# Display with style\n",
    "display_monthly_statistics(spi_difference_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "Analysis text\n",
    "\n",
    "(Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-geospatial>`)\n",
    "(Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion-region>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geospatial_comparison(spi_era5drought, spi_reproduced_region, var=\"SPI\", time=f\"{snapshot_year}-12-01\",\n",
    "                           domain=domain_region,\n",
    "                           glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-geospatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix comparison\n",
    "# e.g. are all \"extremely dry\" data in ERA5â€“Drought also \"extremely dry\" in the reproduction?\n",
    "plot_confusion_matrices_spi(spi_era5drought, spi_reproduced_region, title_suffix=f\" in {label_region}\",\n",
    "                            glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion-region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Geospatial\n",
    ":sync: geospatial\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-geospatial\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-geospatial\"\n",
    "\n",
    "SPI downloaded from ERA5â€“Drought (left) vs. reproduced from ERA5 precipitation data (middle) and the difference between the two (right), for the region around the example site of Addis Ababa, Ethiopia.\n",
    "Only one month (December 2024) is displayed.\n",
    "Colours correspond to SPI categories (e.g. \"extremely dry\") in [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)].\n",
    "```\n",
    ":::\n",
    ":::{tab-item} Confusion matrix\n",
    ":sync: confusion\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion-region\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion-region\"\n",
    "\n",
    "Confusion matrices for SPI categories from ERA5â€“Drought vs. reproduced from ERA5, across the region around the example site of Addis Ababa, Ethiopia.\n",
    "Combines data for all months within one year (2024).\n",
    "```\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "#### 3.8 Quality flags: Probability of zero precipitation\n",
    "One of the quality flags included in ERA5â€“Drought is the probability of zero precipitation _p{sub}`0`_.\n",
    "This represents,\n",
    "for each calendar month and accumulation period,\n",
    "the fraction of months within the reference period (1991â€“2020) where precipitation was zero.\n",
    "While the ERA5â€“Drought implementation of SPI accounts for months with zero precipitation by shifting the CDF,\n",
    "as demonstrated above,\n",
    "those months do skew the fitted distribution and reduce the reliability of the index.\n",
    "In ERA5â€“Drought,\n",
    "the gamma distribution is not even fitted if at least 10 out of 30 months in the reference window have 0 precipitation,\n",
    "and the authors recommend that users filter out locations with a lower threshold, such as _p{sub}`0`_ > 0.1 [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)].\n",
    "\n",
    "\n",
    "Unlike in the zero-precipitation correction that is part of the SPI calculation,\n",
    "this quality flag uses an unweighted probability, with _n{sub}`0`_ the number of months with zero precipitation and _n_ the total number of months in the reference window (30):\n",
    "$$\n",
    "p_0 = \\frac{n_0}{n + 1}\n",
    "$$\n",
    "\n",
    "In this subsection, we reproduce the _p{sub}`0`_ quality flag and its derived masks,\n",
    "and compare their values to those provided in ERA5â€“Drought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Accumulate total precipitation\n",
    "data_era5_precipitation = accumulate(data_era5_precipitation_preprocessed, \"tp\")\n",
    "\n",
    "# Select data in reference window\n",
    "data_era5_precipitation_reference = data_era5_precipitation.sel(**reference_window)\n",
    "\n",
    "# Calculate the probability of zero precipitation\n",
    "p_zero_reproduced = probability_of_zero_precipitation(data_era5_precipitation_reference)\n",
    "# p_zero = p_zero.chunk({\"lat\": -1, \"lon\": -1})  # Rechunk to global maps for efficiency\n",
    "# p_zero = p_zero.persist()  # Persist in memory\n",
    "\n",
    "# Display result\n",
    "p_zero_reproduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The CDS provides ERA5â€“Drought's _p{sub}`0`_ flag for each accumulation period,\n",
    "but does not distinguish between them in the variable name.\n",
    "This means that _p{sub}`0`_ must be downloaded separately for each accumulation period, renamed, and merged together,\n",
    "rather than downloading it for all in one request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup: Request for each accumulation period\n",
    "request_p_zero = {\n",
    "    \"variable\": [\"probability_of_zero_precipitation_spi\"],\n",
    "} | request_era5drought_flag\n",
    "\n",
    "requests_per_accumulation_period = {period: {\"accumulation_period\": [str(period)],}\n",
    "                                    for period in ACCUMULATION_PERIODS}\n",
    "\n",
    "subrequests_era5drought_p_zero = {period: (req | request_p_zero)\n",
    "                                  for period, req in requests_per_accumulation_period.items()}\n",
    "\n",
    "# Download data in individual requests\n",
    "p_zero_era5drought = {period: ekd.from_source(\"cds\", ID_ERA5_DROUGHT, subreq)  # Download as field lists\n",
    "                      for period, subreq in subrequests_era5drought_p_zero.items()}\n",
    "\n",
    "# Pre-process: Convert to xarray, handle variable names, month dimension\n",
    "p_zero_era5drought = preprocess_era5drought_qualityflag(p_zero_era5drought, \"tp\")\n",
    "\n",
    "# Display in notebook\n",
    "p_zero_era5drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "For both datasets, we create masks corresponding to\n",
    "the threshold for not fitting a gamma distribution (_p{sub}`0`_ > 0.33)\n",
    "and\n",
    "the recommended threshold for filtering data (_p{sub}`0`_ > 0.1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Create masks from p_zero\n",
    "# True: below threshold, allowed\n",
    "# False: above threshold, flagged\n",
    "p_zero_reproduced_033 = (p_zero_reproduced <= 0.33)\n",
    "p_zero_era5drought_033 = (p_zero_era5drought <= 0.33)\n",
    "\n",
    "p_zero_reproduced_010 = (p_zero_reproduced <= 0.10)\n",
    "p_zero_era5drought_010 = (p_zero_era5drought <= 0.10)\n",
    "\n",
    "# Display result\n",
    "p_zero_era5drought_033"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the zero precipitation quality flag is in fact reproduceable, and with the above, different thresholds can be set by the user (depending on how strict the threshold is). This gives us faith too that the zero precipitation adjustment has been made correctly (pending formula).\n",
    "\n",
    "(Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-p0-geospatial>`)\n",
    "(Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion-region>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Package masks into dictionary for plotting function\n",
    "masks = {r\"$p_0 \\leq 0.33$\": [p_zero_era5drought_033, p_zero_reproduced_033],\n",
    "         r\"$p_0 \\leq 0.10$\": [p_zero_era5drought_010, p_zero_reproduced_010],\n",
    "         }\n",
    "\n",
    "# Display quality flags â€“ this step may take a few minutes\n",
    "plot_geospatial_comparison_quality_flags(p_zero_era5drought, p_zero_reproduced, \"tp3\", example_month=12,\n",
    "                                         masks=masks, shared_mask=LAND,\n",
    "                                         flag_label=r\"$p_0$ (probability of zero precipitation)\",\n",
    "                                         glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-p0-geospatial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Geospatial\n",
    ":sync: geospatial\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-p0-geospatial\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-p0-geospatial\"\n",
    "\n",
    "Probability of zero precipitation and associated masks in ERA5â€“Drought (left) vs. reproduced from ERA5 precipitation data (middle) and the difference or mismatch between the two (right).\n",
    "Probability is displayed for one calendar month (December) across the reference window (1991â€“2020).\n",
    "Masks are displayed as the number of calendar months that get flagged, following [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)] Fig. 3.\n",
    "```\n",
    ":::\n",
    ":::{tab-item} Confusion matrix\n",
    ":sync: confusion\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion-x\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion-x\"\n",
    "\n",
    "Confusion matrices for SPI categories from ERA5â€“Drought vs. reproduced from ERA5, across the region around the example site of Addis Ababa, Ethiopia.\n",
    "Combines data for all months within one year (2024).\n",
    "```\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "We finally show that \"probability of zero precipitation months\" match for all calendar months at all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "x = np.ravel(p_zero[\"tp_mm_accum_1m\"].where(ls_mask[\"lsm\"]))\n",
    "y = np.ravel(era5_d_prob_spi_zero_all[\"prob_zero_1\"].where(ls_mask[\"lsm\"]))\n",
    "\n",
    "p_threshold = 0.33 \n",
    "\n",
    "x_count = np.count_nonzero(~np.isnan(x))\n",
    "y_count = np.count_nonzero(~np.isnan(y))\n",
    "\n",
    "reject_x_count = (x > p_threshold).sum()\n",
    "reject_y_count = (y > p_threshold).sum()\n",
    "\n",
    "percent_reject_x = (reject_x_count/x_count)*100\n",
    "percent_reject_y = (reject_y_count/y_count)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "hb = plt.scatter(\n",
    "    x, y,   \n",
    ")\n",
    "\n",
    "plt.hlines(y=p_threshold, xmin=0, xmax=p_threshold, colors='g', linestyles='--', linewidth=2)\n",
    "plt.vlines(x=p_threshold, ymin=0, ymax=p_threshold, colors='g', linestyles='--', linewidth=2)\n",
    "plt.fill_between([0,p_threshold], 0, p_threshold, facecolor=\"none\", hatch=\"X\", edgecolor=\"green\", linewidth=0.0, label='Acceptance criteria')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Reproduced')\n",
    "plt.ylabel('ERA5-Drought')\n",
    "plt.title('Comparison of Probability of Zero Precipitation')\n",
    "plt.annotate(f\"Reproduced rejection freq: {percent_reject_x:.5g}%\\n ERA5D rejection freq:{percent_reject_y:.5g}%\", xy=(0.5, 0.2))\n",
    "plt.legend()\n",
    "\n",
    "# Display plot\n",
    "plt.show() \n",
    "# TODO: Calculate percentage that are the same, ignoring the NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "#### 3.9 Quality flags: Shapiroâ€“Wilk normality test\n",
    "Quality control is performed by the ERA5-Drought team over the entire dataset. This is done by testing if the calculated distribution of the estimated drought indices over the reference period follows a normal distribution with mean 0 and standard deviation 1. \n",
    "\n",
    "This test is performed using the Shapiro-Wilks test for normality [[Shapiro+65](https://doi.org/10.1093/biomet/52.3-4.591)], with a $\\alpha$ = 0.05 on the data in the reference period (1991-2020). \n",
    "\n",
    "If the resultant p-value is less than $\\alpha$ = 0.05, the corresponding quality parameter is set to 0 (bad), otherwise set to 1 (good). We perform this test over the calculated SPI values, that are adjusted for zero-precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Request for each accumulation period\n",
    "request_p_normality = {\n",
    "    \"variable\": [\"test_for_normality_spi\"],\n",
    "} | request_region | request_era5drought_flag\n",
    "\n",
    "requests_per_accumulation_period = {period: {\"accumulation_period\": [str(period)],}\n",
    "                                    for period in ACCUMULATION_PERIODS}\n",
    "\n",
    "subrequests_era5drought_p_normality = {period: (req | request_p_normality)\n",
    "                                       for period, req in requests_per_accumulation_period.items()}\n",
    "\n",
    "# Download data in individual requests\n",
    "p_normality_era5drought = {period: ekd.from_source(\"cds\", ID_ERA5_DROUGHT, subreq)  # Download as field lists\n",
    "                           for period, subreq in subrequests_era5drought_p_normality.items()}\n",
    "\n",
    "# Pre-process: Convert to xarray, handle variable names, month dimension\n",
    "p_normality_era5drought = preprocess_era5drought_qualityflag(p_normality_era5drought, \"SPI\")\n",
    "\n",
    "# Display in notebook\n",
    "p_normality_era5drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select latitude and longitude box (40 x 40 here)\n",
    "dom_lon_min, dom_lon_max = 30, 53  \n",
    "dom_lat_min, dom_lat_max = -5, 18\n",
    "\n",
    "lon_min, lon_max = 52, 53  \n",
    "lat_min, lat_max = 17, 18\n",
    "\n",
    "# Subset (latitude is descending in ERA5) - should this be wrapped in a function now?\n",
    "ds_loc = era5_monthly_mean_reanal.sel(\n",
    "    latitude=slice(lat_max, lat_min),\n",
    "    longitude=slice(lon_min, lon_max))\n",
    "\n",
    "# Accumulate precipitations\n",
    "precipitation_example_global = accumulate(ds_loc, var = \"tp\") \n",
    "\n",
    "# Select reference precipitation window.\n",
    "precipitation_example_global_reference = precipitation_example_global.sel(**reference_window) \n",
    "\n",
    "# Fit distribution to reference window, extract parameters.\n",
    "params_fitted_global = fit_monthly_spi(precipitation_example_global_reference)\n",
    "\n",
    "# Calculate CDF  with parameters.\n",
    "cdf_global = compute_monthly_spi(precipitation_example_global, params_fitted_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust CDF zero precipitation.\n",
    "adjusted_cdf_global = zero_precip_adjustment(precipitation_example_global, cdf_global, reference_window)\n",
    "\n",
    "# Transform adjusted CDF back to SPI\n",
    "spi_global = cdf_to_znorm_transform(cdf_global, \"SPI\")\n",
    "adjusted_spi_global = cdf_to_znorm_transform(adjusted_cdf_global, \"SPI\")\n",
    "\n",
    "# Apply land-sea mask over region. \n",
    "# adjusted_spi_ds_global = adjusted_spi_global.where(ls_mask[\"lsm\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_spi_global, calc_quality_spi_global = xr_shapiro_test(spi_global, reference_window, index_name = \"SPI\")\n",
    "calc_quality_spi_agg_global = calc_quality_spi_global.sum(dim = \"time\") # Aggregate across months.\n",
    "\n",
    "era5_quality_spi_global = era5_drought_api_multiple(indicator = \"spi\", var = \"quality\", product_type = \"reanalysis\")\n",
    "era5_quality_spi_global_agg = quality_spi_all.sum(dim=\"time\")\n",
    "\n",
    "accumulation_window = 1\n",
    "\n",
    "# --- Colormap as you already have ---\n",
    "beige_to_red = LinearSegmentedColormap.from_list(\n",
    "    \"beige_to_red\",\n",
    "    [\"#fbeec2\", \"#d47a57\", \"#8b0000\"]   # beige â†’ soft red â†’ dark red\n",
    ")\n",
    "\n",
    "ZERO_PRECIP_STYLE      = Style(cmap=beige_to_red, vmin=1,  vmax=12,  normalize=False)\n",
    "BLACK_POINT_STYLE      = Style(cmap=ListedColormap([\"black\"]), vmin=0, vmax=0.99, normalize=False)\n",
    "ZERO_PRECIP_DIFF_STYLE = Style(cmap=\"RdBu_r\", vmin=-12, vmax=12, normalize=False)\n",
    "\n",
    "# ------------------ LEFT PANEL ------------------\n",
    "da1 = calc_quality_spi_agg_global[f\"significance_{accumulation_window}\"] # Reproduced\n",
    "non_ones1 = da1.where(da1 != 0)   # all values except 0\n",
    "ones1     = da1.where(da1 == 0)   # only values equal to 0\n",
    "\n",
    "# ------------------ MIDDLE PANEL ------------------\n",
    "da2 = era5_quality_spi_global_agg[f\"significance_{accumulation_window}\"].sel(\n",
    "    lat=slice(lat_max, lat_min),\n",
    "    lon=slice(lon_min, lon_max)) # ERA5-D\n",
    "\n",
    "non_ones2 = da2.where(da2 != 0)\n",
    "ones2     = da2.where(da2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "horn = ekp.geo.domains.Domain(\n",
    "    bbox=[dom_lon_min, dom_lon_max, dom_lat_min, dom_lat_max],\n",
    "    name=\"Horn of Africa\",\n",
    ")\n",
    "\n",
    "# Create figure with 3 columns\n",
    "fig = ekp.Figure(rows=1, columns=2)\n",
    "\n",
    "# ---------- Subplot 1 ----------\n",
    "subplot1 = fig.add_map(domain=horn, row=0, column=0)\n",
    "mappable1 = subplot1.grid_cells(non_ones1.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot1.grid_cells(ones1.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax1 = subplot1.ax\n",
    "ax1.set_title(f\"Reproduced â€” # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ---------- Subplot 2 ----------\n",
    "subplot2 = fig.add_map(domain=horn, row=0, column=1)\n",
    "mappable2 = subplot2.grid_cells(non_ones2.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot2.grid_cells(ones2.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax2 = subplot2.ax\n",
    "ax2.set_title(f\"ERA5 â€”  # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ------------------ SHARED FIGURE SETTINGS ------------------\n",
    "# Set figure size (wide)\n",
    "fig_mpl = ax1.figure\n",
    "fig_mpl.set_size_inches(18, 6)\n",
    "\n",
    "# Shared colorbar for the first two panels â€” use the mappable from panel 1\n",
    "cbar12 = fig_mpl.colorbar(\n",
    "    mappable1,\n",
    "    ax=[ax1, ax2],\n",
    "    orientation=\"horizontal\",\n",
    "    location=\"bottom\",    # remove this line if your Matplotlib is older than 3.6\n",
    "    fraction=0.06,\n",
    "    pad=0.08,\n",
    "    ticks=np.arange(1, 13)\n",
    ")\n",
    "\n",
    "cbar12.set_label(\"Number of months (1â€“12)\")\n",
    "\n",
    "decorate_fig(fig, title = \"\") # TODO Make colorbar discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the above is the SW quality flag, regionally (this should be below the time series quality flag comparison). What we have seen is that for a region in Yemen, the indices seem to agree with ERA5-D before zero-precipitation adjustment rather than afterwards?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "You can also import the quality flags from ERA5-Drought, for every calendar month, in the reference period, per accumulation period. \n",
    "\n",
    "You are unable to send an API request to download this data for all accumulation periods at once. If you try to do so, you will find that your xarray only contains the data for the last accumulation period (in this case, the 48-month window). \n",
    "\n",
    "We have written a simple script that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One location -- reproduced -- keep --\n",
    "pval_spi, sig_spi = xr_shapiro_test(adjusted_spi_ds, reference_window, index_name = \"SPI\")\n",
    "sig_spi_agg = sig_spi.sum(dim = \"time\") # Aggregate across months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi_all = era5_drought_api_multiple(indicator = \"spi\", var = \"quality\", product_type = \"reanalysis\")\n",
    "quality_spi_all_agg = quality_spi_all.sum(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi_addis = quality_spi_all.sel(coords_dict[\"Addis Ababa, Ethiopia\"])\n",
    "quality_spi_tedd = quality_spi_all.sel(coords_dict[\"Teddington, United Kingdom\"])\n",
    "quality_spi_denv = quality_spi_all.sel(coords_dict[\"Denver, USA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the above is the SW quality flag, at a single location (this should be above the regional quality flag comparison). Except for the 1-month window there seems to be perfect agreement. \n",
    "\n",
    "Interesting that for 1-month window we have more months that passs SW quality check than ERA5-Drought. Consistent with the time series comparison of ERA5-D & Reproduce -> as saw that ERA5-Drought was consistently larger, and \"spikier\". Perhaps our fit is of \"higher\" quality, or perhaps the test was just less good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "xnor_result = quality_spi_addis == sig_spi.sel(sites=0)\n",
    "\n",
    "# --- Config ---\n",
    "COLORS = {\n",
    "    \"era5\": \"#1f77b4\",\n",
    "    \"repro\": \"#ff7f0e\",\n",
    "    \"match\": \"#2ca02c\",\n",
    "}\n",
    "\n",
    "# --- X locations and bar width for 3 bars per group ---\n",
    "x = np.arange(len(ACCUMULATION_PERIODS))\n",
    "n_bars = 3\n",
    "width = 0.8 / n_bars  # keeps total group width ~0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for i, p in enumerate(ACCUMULATION_PERIODS):\n",
    "    # Existing values (assumed scalar fractions)\n",
    "    era5_val = float(quality_spi_all_agg[f\"significance_{p}\"].sel(coords_dict[\"Addis Ababa, Ethiopia\"]).values)\n",
    "    repro_val = float(sig_spi_agg[f\"significance_{p}\"].sel(sites=0).values)\n",
    "\n",
    "    # Match fraction from xnor_result (aggregate over time and normalise)\n",
    "    mean_true = xnor_result[f\"significance_{p}\"].sum(dim=\"time\", skipna=True)\n",
    "    mean_true = float(mean_true.compute())\n",
    "\n",
    "    # Plot 3 bars: left (ERA5D), center (Reproduced), right (Mismatch)\n",
    "    ax.bar(\n",
    "        x[i] - width, era5_val, width,\n",
    "        label=\"ERA5-Drought\" if i == 0 else None, color=COLORS[\"era5\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i], repro_val, width,\n",
    "        label=\"Reproduced\" if i == 0 else None, color=COLORS[\"repro\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i] + width, mean_true, width,\n",
    "        label=\"Match frequency\" if i == 0 else None, color=COLORS[\"match\"]\n",
    "    )\n",
    "\n",
    "# --- Cosmetics ---\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(p) for p in ACCUMULATION_PERIODS])\n",
    "ax.set_xlabel(\"Accumulation windows (months)\")\n",
    "ax.set_ylabel(\"Fraction of months in calendar year\")\n",
    "ax.set_ylim(0, 12)\n",
    "ax.set_title(\"Fraction of months passing S-W test & match frequency (for each accumulation window).\")\n",
    "ax.legend(frameon=False)\n",
    "ax.grid(axis=\"y\", alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-spei)=\n",
    "### 4. SPEI comparison\n",
    "The process of calculating SPEI is very similar to that for SPI, with three major differences:\n",
    "* SPEI is based on the water balance (precipitation and potential evaporation or evapotranspiration), rather than just precipitation.\n",
    "* SPEI is based on a log-logistic distribution, rather than a gamma distribution.\n",
    "* SPEI does not need to be adjusted for months with zero precipitation.\n",
    "\n",
    "As such, this section is structured similarly to [](section-spi) and can be run entirely independently, but some of the more detailed explanations from said section are left out for brevity this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "#### 4.1 Download monthly precipitation and potential evaporation data from ERA5\n",
    "First, the monthly-mean total precipitation (variable `228.128`) and evaporation (variable `251.228`) are downloaded from [_Complete ERA5 global atmospheric reanalysis_ (reanalysis-era5-complete)](https://doi.org/10.24381/cds.143582cf).\n",
    "More information about the format for these requests is available in the [MARS ERA5 catalogue](https://apps.ecmwf.int/data-catalogues/era5/?class=ea).\n",
    "\n",
    "When reproducing this notebook yourself, if you have previously run through [](section-spi) and have [caching](https://earthkit-data.readthedocs.io/en/latest/guide/caching.html) enabled in earthkit-data, the previously downloaded precipitation data will be re-used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_precipitation_moda = {\n",
    "    \"param\": \"228.128\",  # Variable: Total precipitation\n",
    "    \"stream\": \"moda\",    # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template\n",
    "\n",
    "request_era5_pev_moda = {\n",
    "    \"param\": \"251.228\",  # Variable: Potential evaporation\n",
    "    \"stream\": \"moda\",    # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and load into xarray\n",
    "data_era5_waterbalance_cds = ekd.from_source(\"cds\", ID_ERA5, request_era5_precipitation_moda, request_era5_pev_moda)  # Download as field list\n",
    "data_era5_waterbalance_cds = data_era5_waterbalance_cds.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "\n",
    "# Pre-process to desired format\n",
    "# Note the change in variable name\n",
    "data_era5_waterbalance_preprocessed = preprocess_era5(data_era5_waterbalance_cds)\n",
    "\n",
    "# Display in notebook\n",
    "data_era5_waterbalance_preprocessed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate water balance over accumulation periods\n",
    "The water balance represents the net gain or loss of water in an area due to precipitation (gain) and evaporation (loss).\n",
    "[ECMWF's convention](https://codes.ecmwf.int/grib/param-db/182), as used in ERA5, is that downward fluxes are positive and upward fluxes are negative.\n",
    "This means that\n",
    "total precipitation is always positive (or zero),\n",
    "potential evaporation is always negative (or zero),\n",
    "and the water balance is simply the sum of the two: _WB = TP + PEV_.\n",
    "\n",
    "Here, we calculate the water balance per point in the downloaded ERA5 data and accumulate it over the same periods as before (1, 3, 6, 12, 24, 36, and 48 months).\n",
    "\n",
    "The resulting accumulated time series for the example site defined in [](section-general_setup),\n",
    "Addis Ababa in Ethiopia in our example,\n",
    "are displayed in Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-wb-accumulated>`.\n",
    "This example clearly shows how shorter accumulation periods probe short-term effects such as seasonality,\n",
    "which are smoothed out in longer periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate water balance\n",
    "data_era5_waterbalance = calculate_waterbalance(data_era5_waterbalance_preprocessed)\n",
    "\n",
    "# Accumulate water balance\n",
    "data_era5_waterbalance = accumulate(data_era5_waterbalance, \"wb\")\n",
    "\n",
    "# Display result\n",
    "data_era5_waterbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The plot below illustrates the seasonal water balance for a country like Ethiopia, where periods of high evapotranspiration exceed precipitation, resulting in a negative water balance. Conversely, during wetter months, precipitation surpasses evapotranspiration, producing a surplus of available water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display accumulated precipitation at example site\n",
    "plot_accumulated_waterbalance(data_era5_waterbalance, example_site,\n",
    "                              glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-wb-accumulated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-wb-accumulated\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-wb-accumulated\"\n",
    "\n",
    "Water balance from ERA5 accumulated over different accumulation periods, for the example site of Addis Ababa, Ethiopia.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "#### 4.3 Fit log-logistic distribution and compute SPEI\n",
    "While SPI assumes a gamma distribution for total precipitation,\n",
    "this assumption does not hold for the water balance.\n",
    "For SPEI,\n",
    "various distributions are used in the literature [[Stagge+15](https://doi.org/10.1002/joc.4267)].\n",
    "While [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)] specifies that ERA5â€“Drought's SPEI is based on the generalised log-logistic distribution,\n",
    "following [[Vicente-Serrano+10](https://doi.org/10.1175/2009JCLI2909.1)],\n",
    "private communication with the authors indicates that a generalised logistic distribution is used instead.\n",
    "\n",
    "As before, the distribution is fitted to data in the reference window (1991â€“2020 by default),\n",
    "Here, we use\n",
    "[scipy.stats.genlogistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.genlogistic.html)\n",
    "to fit the generalised logistic distribution.\n",
    "This returns three parameters for each calendar month and accumulation period, namely\n",
    "shape (_c_), location (_Î¼_), and scale (_Î²_).\n",
    "\n",
    "The observed (accumulated) water balance values along the entire time series\n",
    "are then compared to the fitted parameters in terms of where they fall on the cumulative distribution function (CDF).\n",
    "From these CDF values,\n",
    "SPEI values\n",
    "for each data point (latitude, longitude, time)\n",
    "are calculated by transforming to a standard normal distribution.\n",
    "The zero-precipitation correction from [](section-spi) is not relevant to SPEI.\n",
    "\n",
    "Note that actually evaluating the CDF\n",
    "â€“ as opposed to [queueing it up in dask](https://docs.xarray.dev/en/stable/user-guide/dask.html), as done here â€“\n",
    "can be slow, especially for a large dataset like global ERA5 precipitation.\n",
    "As such, if you are interested in a subset of the data,\n",
    "such as a specific site or period in time,\n",
    "it may be best to subset your data _before_ calculating the CDF rather than afterwards.\n",
    "An example of this is provided below in the comparison with ERA5â€“Drought SPEI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Select data in reference window\n",
    "data_era5_waterbalance_reference = data_era5_waterbalance.sel(**reference_window)\n",
    "\n",
    "# Fit generalised logistic distribution\n",
    "spei_parameters = fit_monthly_spei(data_era5_waterbalance_reference)\n",
    "\n",
    "# Compute CDF time series\n",
    "cdf = compute_cdf_spei(data_era5_waterbalance, spei_parameters)\n",
    "\n",
    "# Calculate SPI from adjusted CDF\n",
    "spei_reproduced = cdf_to_spei(cdf)\n",
    "\n",
    "# Display result\n",
    "spei_reproduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "#### 4.4 SPEI comparison: Time series in example site\n",
    "Having reproduced the SPEI index from ERA5 precipitation and potential evaporation data following the ERA5â€“Drought methodology,\n",
    "we can now compare the results to determine the reproducibility of ERA5â€“Drought.\n",
    "We first compare the datasets over time at the example site defined in [](section-general_setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "As before,\n",
    "because the global calculation set up in the previous subsections can take a long time to execute,\n",
    "here we sub-select the downloaded ERA5 data and calculate SPEI for the example site only.\n",
    "For convenience, the full SPEI pipeline is wrapped into a single `calculate_spei_from_era5` function,\n",
    "as defined in [](section-code_setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Select only desired site\n",
    "data_era5_waterbalance_site = data_era5_waterbalance_preprocessed.sel(**example_site)\n",
    "\n",
    "# Calculate SPI\n",
    "spei_reproduced_site = calculate_spei_from_era5(data_era5_waterbalance_site, reference_window=reference_window)\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spei_reproduced_site = spei_reproduced_site.persist()\n",
    "\n",
    "# Display result\n",
    "spei_reproduced_site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "Next, we download the corresponding data from ERA5â€“Drought.\n",
    "Because of size limits on the CDS,\n",
    "the time series must be downloaded in parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Main request, based on template and example site\n",
    "request_era5drought_spei = {\n",
    "    \"variable\": [\"standardised_precipitation_evapotranspiration_index\"],\n",
    "} | request_era5drought_index | request_site\n",
    "\n",
    "# Split into batches of up to 20 years each\n",
    "subrequests_era5drought_spei = batch_requests(request_era5drought_spei, n=20)\n",
    "\n",
    "# Download data and load into xarray\n",
    "spei_era5drought = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, *subrequests_era5drought_spei)\n",
    "spei_era5drought = spei_era5drought.to_xarray(compat=\"equals\", join=\"outer\")\n",
    "spei_era5drought = spei_era5drought.chunk({\"time\": -1})  # Full time series in one chunk\n",
    "\n",
    "# Select only desired site (remove margins)\n",
    "spei_era5drought = spei_era5drought.sel(**example_site)\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spei_era5drought = spei_era5drought.persist()\n",
    "\n",
    "# Display result\n",
    "spei_era5drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "Again, we examine the per-point difference in SPEI between ERA5â€“Drought and the reproduction for each calendar month and each accumulation period.\n",
    "We calculate the median difference _Î”_ and median absolute difference _|Î”|_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SPEI median differences\n",
    "spei_difference_site = comparison_monthly_statistics(spei_era5drought, spei_reproduced_site)\n",
    "\n",
    "# Display with style\n",
    "display_monthly_statistics(spei_difference_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {},
   "source": [
    "**Description of comparisons**\n",
    "\n",
    "Table for quantitative differences\n",
    "\n",
    "Median and absolute median difference for each accumulation window across months are then calculated  \n",
    "\n",
    "**TODO point-to-make**: SPI is only discrepant for certain months (but that this discrepancy is still quite small)- since fitting is by month only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the SPEI-index shows considerably less agreement than the SPI index in the previous section, for the same location. We understand that the fitting distribution in the case of the SPEI index is particularly sensitive to the parameterisation used, and that the parameters that are outputted are highly dependent on the initial \"guess\" of the parameters. This in the ERA5-D paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "**Description of figures**\n",
    "\n",
    "**TODO point-to-make**: SPI for location in Addis Ababa shows excellent agreement across entire time series. Showed that our methodology is consistent with that of ERA5 and reproduceable.  \n",
    "Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-timeseries>`  \n",
    "Same seen in different sites (not shown here)  \n",
    "\n",
    "A confusion matrix is generated to compare drought classifications for the single, above location, between the ERA5â€‘Drought dataset and the reproduced dataset, capturing any discrepancies. The matrix is computed over the full time period (1940â€“2024), where each count corresponds to a single monthly timestep.  \n",
    "Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spi-confusion>`  \n",
    "**TODO point-to-make**: and that in fact, the indices fall under the same drought severity classification for all timestamps at Addis Ababa.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: even though there is considerably less agreement in the SPEI index, the agreement in drought severity for SPEI is not particularly affected, except for a few timestamps where the severity is +- 1 category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series comparison (absolute values and differences)\n",
    "plot_time_series_comparison_spei(spei_era5drought, spei_reproduced_site, title_suffix=f\" at {label_site}\",\n",
    "                                 glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-timeseries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix comparison\n",
    "# e.g. are all \"extremely dry\" data in ERA5â€“Drought also \"extremely dry\" in the reproduction?\n",
    "plot_confusion_matrices_spei(spei_era5drought, spei_reproduced_site, title_suffix=f\" at {label_site}\",\n",
    "                             glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-confusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Time series\n",
    ":sync: timeseries\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-timeseries\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-timeseries\"\n",
    "\n",
    "SPEI time series downloaded from ERA5â€“Drought and reproduced from ERA5 precipitation data (left) and the difference between the two (right), for the example site of Addis Ababa, Ethiopia.\n",
    "Colours in the left-hand column correspond to SPEI categories (e.g. \"extremely dry\") in [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)].\n",
    "```\n",
    ":::\n",
    ":::{tab-item} Confusion matrix\n",
    ":sync: confusion\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-confusion\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-confusion\"\n",
    "\n",
    "Confusion matrices for SPEI categories from ERA5â€“Drought vs. reproduced from ERA5, for the example site of Addis Ababa, Ethiopia, in 1940â€“2024.\n",
    "```\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "#### 4.5 SPEI comparison: Regional snapshot\n",
    "Next, we investigate spatial patterns in SPEI and the difference therein across a wider region around the example site.\n",
    "This region is defined in [](section-general_setup).\n",
    "In this example, we look at part of the Horn of Africa using a box of 12Â° in all directions around Addis Ababa, Ethiopia.\n",
    "To reduce computing requirements, the comparison is performed for one year only,\n",
    "here 2024, again defined in [](section-general_setup).\n",
    "\n",
    "As in the time series comparison,\n",
    "we subselect the desired data from ERA5 and calculate the corresponding SPEI\n",
    "and\n",
    "download SPEI from ERA5â€“Drought for the desired region and time span:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Select only desired site\n",
    "data_era5_waterbalance_region = data_era5_waterbalance_preprocessed.sel(**example_region)\n",
    "\n",
    "# Window in which to evaluate SPI\n",
    "evaluation_window = {\"time\": slice(f\"{snapshot_year}-01-01\", f\"{snapshot_year}-12-01\")}  #  Slice (2024-01-01, 2024-12-01)\n",
    "\n",
    "# Calculate SPI\n",
    "spei_reproduced_region = calculate_spei_from_era5(data_era5_waterbalance_region, reference_window=reference_window,\n",
    "                                                  evaluation_window=evaluation_window)\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spei_reproduced_region = spei_reproduced_region.persist()\n",
    "\n",
    "# Display result\n",
    "spei_reproduced_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Main request, based on template and example region\n",
    "request_era5drought_spei = {\n",
    "    \"variable\": [\"standardised_precipitation_evapotranspiration_index\"],\n",
    "} | request_era5drought_index | request_region\n",
    "\n",
    "# Select only desired year\n",
    "request_era5drought_spei = request_era5drought_spei | {\"year\": [f\"{snapshot_year}\"],}\n",
    "\n",
    "# Download data and load into xarray\n",
    "spei_era5drought = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, request_era5drought_spei)\n",
    "spei_era5drought = spei_era5drought.to_xarray(compat=\"equals\", join=\"outer\")\n",
    "spei_era5drought = spei_era5drought.chunk({\"time\": -1})  # Full time series in one chunk\n",
    "\n",
    "# Persist in memory to speed up analysis â€“ this step may take a few minutes\n",
    "spei_era5drought = spei_era5drought.persist()\n",
    "\n",
    "# Display result\n",
    "spei_era5drought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "First, we examine the per-point difference in SPI between ERA5â€“Drought and the reproduction.\n",
    "This comparison is performed separately for each calendar month and each accumulation period\n",
    "to reflect the fitting process.\n",
    "We calculate the median difference _Î”_ and median absolute difference _|Î”|_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SPEI median differences\n",
    "spei_difference_region = comparison_monthly_statistics(spei_era5drought, spei_reproduced_region)\n",
    "\n",
    "# Display with style\n",
    "display_monthly_statistics(spei_difference_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "Analysis text\n",
    "\n",
    "(Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-geospatial>`)\n",
    "(Figure {numref}`{number} <indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-confusion-region>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geospatial_comparison(spei_era5drought, spei_reproduced_region, var=\"SPEI\", time=f\"{snapshot_year}-12-01\",\n",
    "                           domain=domain_region,\n",
    "                           glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-geospatial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix comparison\n",
    "# e.g. are all \"extremely dry\" data in ERA5â€“Drought also \"extremely dry\" in the reproduction?\n",
    "plot_confusion_matrices_spei(spei_era5drought, spei_reproduced_region, title_suffix=f\" in {label_region}\",\n",
    "                             glue_label=\"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-confusion-region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "::::{tab-set}\n",
    ":::{tab-item} Geospatial\n",
    ":sync: geospatial\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-geospatial\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-geospatial\"\n",
    "\n",
    "SPEI downloaded from ERA5â€“Drought (left) vs. reproduced from ERA5 precipitation data (middle) and the difference between the two (right), for the region around the example site of Addis Ababa, Ethiopia.\n",
    "Only one month (December 2024) is displayed.\n",
    "Colours correspond to SPEI categories (e.g. \"extremely dry\") in [[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)].\n",
    "```\n",
    ":::\n",
    ":::{tab-item} Confusion matrix\n",
    ":sync: confusion\n",
    "```{glue:figure} indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-confusion-region\n",
    ":name: \"indicator_derived-drought-historical-monthly_consistency_q02_fig-spei-confusion-region\"\n",
    "\n",
    "Confusion matrices for SPEI categories from ERA5â€“Drought vs. reproduced from ERA5, across the region around the example site of Addis Ababa, Ethiopia.\n",
    "Combines data for all months within one year (2024).\n",
    "```\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "#### 4.6 Quality flags: Shapiroâ€“Wilk normality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Request for each accumulation period\n",
    "request_p_normality = {\n",
    "    \"variable\": [\"test_for_normality_spei\"],\n",
    "} | request_region | request_era5drought_flag\n",
    "\n",
    "requests_per_accumulation_period = {period: {\"accumulation_period\": [str(period)],}\n",
    "                                    for period in ACCUMULATION_PERIODS}\n",
    "\n",
    "subrequests_era5drought_p_normality = {period: (req | request_p_normality)\n",
    "                                       for period, req in requests_per_accumulation_period.items()}\n",
    "\n",
    "# Download data in individual requests\n",
    "p_normality_era5drought = {period: ekd.from_source(\"cds\", ID_ERA5_DROUGHT, subreq)  # Download as field lists\n",
    "                           for period, subreq in subrequests_era5drought_p_normality.items()}\n",
    "\n",
    "# Pre-process: Convert to xarray, handle variable names, month dimension\n",
    "p_normality_era5drought = preprocess_era5drought_qualityflag(p_normality_era5drought, \"SPEI\")\n",
    "\n",
    "# Display in notebook\n",
    "p_normality_era5drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select latitude and longitude box (40 x 40 here)\n",
    "dom_lon_min, dom_lon_max = 30, 53  \n",
    "dom_lat_min, dom_lat_max = -5, 18\n",
    "\n",
    "lon_min, lon_max = 30, 31  \n",
    "lat_min, lat_max = 17, 18\n",
    "\n",
    "# Subset (latitude is descending in ERA5) - should this be wrapped in a function now?\n",
    "ds_loc = era5_wb_monthly.sel(\n",
    "    latitude=slice(dom_lat_max, dom_lat_min),\n",
    "    longitude=slice(dom_lon_min, dom_lon_max))\n",
    "\n",
    "# Accumulate precipitations\n",
    "wb_example_global = accumulate(ds_loc, var = \"wb\") \n",
    "\n",
    "# Select reference precipitation window.\n",
    "wb_example_global_reference = wb_example_global.sel(**reference_window) \n",
    "\n",
    "# Fit distribution to reference window, extract parameters.\n",
    "params_fitted_global = fit_monthly_spei(wb_example_global_reference)\n",
    "\n",
    "# Calculate CDF  with parameters.\n",
    "cdf_global = compute_monthly_spei(wb_example_global, params_fitted_global)\n",
    "\n",
    "spei_global = cdf_to_znorm_transform(cdf_global, \"SPEI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_spei_global, sig_spei_global = xr_shapiro_test(spei_global, reference_window, index_name = \"SPEI\")\n",
    "sig_spei_agg_global = sig_spei_global.sum(dim = \"time\") # Aggregate across months.\n",
    "\n",
    "quality_spei_all = era5_drought_api_multiple(indicator = \"spei\", var = \"quality\", product_type = \"reanalysis\")\n",
    "quality_spei_all_agg = quality_spei_all.sum(dim=\"time\")\n",
    "\n",
    "accumulation_window = 1\n",
    "\n",
    "# --- Colormap as you already have ---\n",
    "beige_to_red = LinearSegmentedColormap.from_list(\n",
    "    \"beige_to_red\",\n",
    "    [\"#fbeec2\", \"#d47a57\", \"#8b0000\"]   # beige â†’ soft red â†’ dark red\n",
    ")\n",
    "\n",
    "ZERO_PRECIP_STYLE      = Style(cmap=beige_to_red, vmin=1,  vmax=12,  normalize=False)\n",
    "BLACK_POINT_STYLE      = Style(cmap=ListedColormap([\"black\"]), vmin=0, vmax=0.99, normalize=False)\n",
    "ZERO_PRECIP_DIFF_STYLE = Style(cmap=\"RdBu_r\", vmin=-12, vmax=12, normalize=False)\n",
    "\n",
    "# ------------------ LEFT PANEL ------------------\n",
    "da1 = sig_spei_agg_global[f\"significance_{accumulation_window}\"] # Reproduced\n",
    "non_ones1 = da1.where(da1 != 0)   # all values except 0\n",
    "ones1     = da1.where(da1 == 0)   # only values equal to 0\n",
    "\n",
    "# ------------------ MIDDLE PANEL ------------------\n",
    "da2 = quality_spei_all_agg[f\"significance_{accumulation_window}\"].sel(\n",
    "    lat=slice(lat_max, lat_min),\n",
    "    lon=slice(lon_min, lon_max)) # ERA5-D\n",
    "\n",
    "non_ones2 = da2.where(da2 != 0)\n",
    "ones2     = da2.where(da2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "horn = ekp.geo.domains.Domain(\n",
    "    bbox=[dom_lon_min, dom_lon_max, dom_lat_min, dom_lat_max],\n",
    "    name=\"Horn of Africa\",\n",
    ")\n",
    "\n",
    "# Create figure with 3 columns\n",
    "fig = ekp.Figure(rows=1, columns=2)\n",
    "\n",
    "# ---------- Subplot 1 ----------\n",
    "subplot1 = fig.add_map(domain=horn, row=0, column=0)\n",
    "mappable1 = subplot1.grid_cells(non_ones1.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot1.grid_cells(ones1.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax1 = subplot1.ax\n",
    "ax1.set_title(f\"Reproduced â€” # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ---------- Subplot 2 ----------\n",
    "subplot2 = fig.add_map(domain=horn, row=0, column=1)\n",
    "mappable2 = subplot2.grid_cells(non_ones2.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot2.grid_cells(ones2.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax2 = subplot2.ax\n",
    "ax2.set_title(f\"ERA5 â€”  # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ------------------ SHARED FIGURE SETTINGS ------------------\n",
    "# Set figure size (wide)\n",
    "fig_mpl = ax1.figure\n",
    "fig_mpl.set_size_inches(18, 6)\n",
    "\n",
    "# Shared colorbar for the first two panels â€” use the mappable from panel 1\n",
    "cbar12 = fig_mpl.colorbar(\n",
    "    mappable1,\n",
    "    ax=[ax1, ax2],\n",
    "    orientation=\"horizontal\",\n",
    "    location=\"bottom\",    # remove this line if your Matplotlib is older than 3.6\n",
    "    fraction=0.06,\n",
    "    pad=0.08,\n",
    "    ticks=np.arange(1, 13)\n",
    ")\n",
    "\n",
    "cbar12.set_label(\"Number of months (1â€“12)\")\n",
    "\n",
    "decorate_fig(fig, title = \"\") # TODO Make colorbar discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: have shown that region around Yemen seems to disagreement in SW quality flag values. This is to be expected since the calculated dataset is not in fact in good agreement with ERA5-Drought. Check at another location with more precipitation (somewhere in mainland europe maybe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {},
   "source": [
    "Quality flags are imported from ERA5-Drought, for every calendar month, in the reference period, per accumulation period for the SPEI index.\n",
    "\n",
    "You are unable to send an API request to download this data for all accumulation periods at once. We have written a simple script that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_quality_spei_global = era5_drought_api_multiple(indicator = \"spei\", var = \"quality\", product_type = \"reanalysis\")\n",
    "era5_quality_spei_global_agg = era5_quality_spei_global.sum(dim=\"time\")\n",
    "\n",
    "era5_quality_spei_addis = era5_quality_spei_global.sel(sites_dict[\"Addis Ababa, Ethiopia\"])\n",
    "era5_quality_spei_tedd = era5_quality_spei_global.sel(sites_dict[\"Teddington, United Kingdom\"])\n",
    "era5_quality_spei_denv = era5_quality_spei_global.sel(sites_dict[\"Denver, USA\"])\n",
    "\n",
    "era5_sig_spei_addis_agg = era5_quality_spei_global_agg.sel(sites_dict[\"Addis Ababa, Ethiopia\"])\n",
    "era5_sig_spei_tedd_agg = era5_quality_spei_global_agg.sel(sites_dict[\"Teddington, United Kingdom\"])\n",
    "era5_sig_spei_denv_agg = era5_quality_spei_global_agg.sel(sites_dict[\"Denver, USA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182",
   "metadata": {},
   "source": [
    "The calculated Shapiro-Wilks significance values are then compared with those from ERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XNOR Result \n",
    "xnor_spei_result = era5_quality_spei_addis == sig_spei\n",
    "\n",
    "#TODO: Wrap up all code in the function.\n",
    "\n",
    "# --- Config ---\n",
    "COLORS = {\n",
    "    \"era5\": \"#1f77b4\",\n",
    "    \"repro\": \"#ff7f0e\",\n",
    "    \"match\": \"#2ca02c\",\n",
    "}\n",
    "\n",
    "# --- X locations and bar width for 3 bars per group ---\n",
    "x = np.arange(len(ACCUMULATION_PERIODS))\n",
    "n_bars = 3\n",
    "width = 0.8 / n_bars  # keeps total group width ~0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for i, p in enumerate(ACCUMULATION_PERIODS):\n",
    "    # Existing values (assumed scalar fractions)\n",
    "    era5_val = float(era5_sig_spei_addis_agg[f\"significance_{p}\"].values)\n",
    "    repro_val = float(sig_spei_aggregated[f\"significance_{p}\"].values)\n",
    "\n",
    "    # Mismatch fraction from xnor_result (aggregate over time)\n",
    "    mean_true = xnor_spei_result[f\"significance_{p}\"].sum(dim=\"time\", skipna=True)\n",
    "    mean_true = float(mean_true.compute())\n",
    "\n",
    "    # Plot 3 bars: left (ERA5D), center (Reproduced), right (Mismatch)\n",
    "    ax.bar(\n",
    "        x[i] - width, era5_val, width,\n",
    "        label=\"ERA5-Drought\" if i == 0 else None, color=COLORS[\"era5\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i], repro_val, width,\n",
    "        label=\"Reproduced\" if i == 0 else None, color=COLORS[\"repro\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i] + width, mean_true, width,\n",
    "        label=\"Match frequency\" if i == 0 else None, color=COLORS[\"match\"]\n",
    "    )\n",
    "\n",
    "# --- Cosmetics ---\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(p) for p in ACCUMULATION_PERIODS])\n",
    "ax.set_xlabel(\"Accumulation windows (months)\")\n",
    "ax.set_ylabel(\"# of months\")\n",
    "ax.set_title(\"Significance  and Match Frequency by Accumulation Period\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_ylim([-12, 12])\n",
    "ax.grid(axis=\"y\", alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: SW quality flag does show good agreement however at single location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "#### 4.7 Comparison of ERA5-Drought & Reproduced SPEI-index with quality flags (Addis Ababa, Ethiopia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "spei_ds_addis_masked = apply_sw_quality_mask(era5_quality_spei_addis, spei_ds.sel(sites=0), \"SPEI\")\n",
    "era5_masked = apply_sw_quality_mask(era5_quality_spei_addis, spei_addis_point, \"SPEI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_index_comparison_side_by_side(\"SPEI\", era5_masked, spei_ds_addis_masked, spei_categories) # takes a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: make comparison with quality flags- perhaps look at a location where there are more failed months? Otherwise, get rid since only 1 month fails SW test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.245188,
     "end_time": "2024-03-08T17:39:21.277354",
     "exception": false,
     "start_time": "2024-03-08T17:39:21.032166",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-ensemble)=\n",
    "### 5. Ensemble comparison\n",
    "Once again, calculating the SPI-index for all 10 ensemble members are the same as calculating the SPI index for one member, with the only modification being that there is an extra coordinate, \"number\" that identifies each ensemble member.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "Download monthly ensemble precipitation data, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_precipitation_edmo = {\n",
    "    \"param\": \"228.128\",       # Variable: Total precipitation\n",
    "    \"stream\": \"edmo\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and convert to desired format\n",
    "era5_monthly_mean_ens = ekd.from_source(\"cds\", ID_ERA5, request_era5_precipitation_edmo)  # Download as field list\n",
    "era5_monthly_mean_ens = era5_monthly_mean_ens.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_tp_monthly_mean_ens = rechunk(era5_monthly_mean_ens)  # Re-chunk for speed gain in fitting\n",
    "era5_tp_monthly_mean_ens  # Display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "And we also download the monthly-mean total potential evaporation data from the ERA5 reanalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_pev_edmo = {\n",
    "    \"param\": \"251.228\",       # Variable: Potential evaporation (pev)\n",
    "    \"stream\": \"edmo\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and convert to desired format\n",
    "era5_pev_monthly_mean_ens = ekd.from_source(\"cds\", ID_ERA5, request_era5_pev_edmo)  # Download as field list\n",
    "era5_pev_monthly_mean_ens = era5_pev_monthly_mean_ens.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_pev_monthly_mean_ens = rechunk(era5_pev_monthly_mean_ens)  # Re-chunk for speed gain in fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {},
   "source": [
    "Precipitation is accumulated for all the accumulation periods in ERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_site = {\"latitude\": 9.25, \"longitude\": 40.5,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the grid point and time slice for Ethiopia\n",
    "precipitation_example_site = era5_tp_monthly_mean_ens.sel(**example_site)\n",
    "\n",
    "# Perform the accumulation for each accumulation period\n",
    "precipitation_ens_example_site = accumulate(precipitation_example_site, \"tp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199",
   "metadata": {},
   "source": [
    "As is potential evaporaiton, for all the accumulation periods in ERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the grid point and time slice for Ethiopia\n",
    "pev_example_site = era5_pev_monthly_mean_ens.sel(**example_site)\n",
    "\n",
    "# Perform the accumulation for each accumulation period\n",
    "pev_ens_example_site = accumulate(pev_example_site, \"pev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "The \"water balance\" is calculated, ensuring once again the -ve of the PET, as discussed earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_ens_example_site = xr.Dataset(\n",
    "    {\n",
    "        f\"wb_{var.replace('tp_', '')}\": precipitation_ens_example_site[var]\n",
    "                                       + pev_ens_example_site[var.replace(\"tp_\", \"pev_\")]\n",
    "        for var in precipitation_ens_example_site.data_vars # loop through every variable.\n",
    "        if var.startswith(\"tp_\") and var.replace(\"tp_\", \"pev_\") in pev_ens_example_site.data_vars\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203",
   "metadata": {},
   "source": [
    "Gamma distribution is fitted for each calendar month (12) for each ensemble member (here, 10), with their parameters calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit log logistic distributions\n",
    "gamma_ens_params_fitted = fit_monthly_spi(precipitation_ens_example_site.sel(**reference_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205",
   "metadata": {},
   "source": [
    "Log-logistic distribution is fitted for each calendar month (12) for each ensemble member (here, 10), with their parameters calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_logistic_ens_params_fitted = fit_monthly_spei(wb_ens_example_site.sel(**reference_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207",
   "metadata": {},
   "source": [
    "SPI is calculated for each accumulation period and each ensemble member from the fitted gamma distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SPI series -- new function\n",
    "cdf_ens_ds = compute_monthly_spi(precipitation_ens_example_site, gamma_ens_params_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209",
   "metadata": {},
   "source": [
    "SPEI is calculated for each accumulation period and each ensemble member from the fitted log-logistic distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SPEI series -- new function\n",
    "cdf_wb_ens_ds = compute_monthly_spei(wb_ens_example_site, log_logistic_ens_params_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211",
   "metadata": {},
   "source": [
    "Quality control is tested with the Shapiro-Wilks test on calculated ensemble SPI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_spi_stat, ens_spi_pval, ens_spi_sig = xr_shapiro_test(spi_ens_ds, reference_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi_ens_all = era5_drought_api_multiple(indicator = \"spi\", var = \"quality\", product_type = \"ensemble_members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214",
   "metadata": {},
   "source": [
    "Quality control using the Shapiro-Wilks test on calculated ensemble SPEI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_spei_stat, ens_spei_pval, ens_spei_sig = xr_shapiro_test(spei_ens_ds, reference_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spei_ens_all = era5_drought_api_multiple(indicator = \"spei\", var = \"quality\", product_type = \"ensemble_members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217",
   "metadata": {},
   "source": [
    "Reading in ensemble SPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_spi_ens = era5_drought_index_multiple(\"SPI\", accum_period = [1,3])\n",
    "era5_spi_ens = era5_spi_ens.sel(lat = 9.25, lon = 40.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219",
   "metadata": {},
   "source": [
    "Time-series comparison of SPI at one location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(0, 10):  #### patch this code so titles show which plot is which ensemble member. is obvious but for full thing?\n",
    "    plot_index_comparison_side_by_side(\"SPI\", era5_spi_ens.sel(number=m), spi_ens_ds.sel(number=m), accumulation_periods = [1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have a table for each accumulation window maybe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222",
   "metadata": {},
   "source": [
    "Reading in ensemble SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_spei_ens = era5_drought_index_multiple(\"SPEI\", accum_period = [3, 24, 48])\n",
    "era5_spei_ens = era5_spei_ens.sel(lat = 9.25, lon = 40.5) ### data needs to be downloaded overnight, couldn't do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224",
   "metadata": {},
   "source": [
    "Time-series comparison of SPEI at one location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(0, 10):  #### patch this code so titles show which plot is which ensemble member. is obvious but for full thing?\n",
    "    plot_index_comparison_side_by_side(\"SPEI\", era5_spei_ens.sel(number=m), spei_ens_ds.sel(number=m), [3, 24, 48])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: each member from the drought-index ensemble is in fact reproduceable from each member from the precipitation / potential evaporation ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "(section-conclusion)=\n",
    "### 6. Conclusions\n",
    "**SPI**- As shown in Sectionâ€¯2, the C3S ERA5 Drought SPIâ€‘Indicator dataset and its manually reproduced counterpart are highly consistent. This agreement is evident across the full time series at a point location in Addis Ababa, Ethiopia, as well as regionally over the Horn of Africa. The median difference and median absolute difference are both close to 0, and the vast majority of pixels exhibit a nearâ€‘zero difference (defined here as |Î”| â‰¤ Îµ with Îµ = 1Ã—10â»âµ to avoid floatingâ€‘point artefacts) across all comparisons.\n",
    "\n",
    "Furthermore, the \"probability of zero precipitation\" quality flag was also shown to be reproducible, globally, with no discrepancy. Since the Shapiro-Wilks quality flag is dependent on the dataset after statistical fitting, there was discrepancy at certain months, for Addis Ababa, and other locations...\n",
    "\n",
    "We also examined discrepancies in droughtâ€‘severity categorisation between the C3S ERA5â€‘Drought dataset and the manually reproduced counterpart at individual locations, including Addis Ababa, London, and Denver. While the severity classifications were generally consistent between ERA5â€‘Drought and the reproduced dataset, there were instances at specific timestamps where the two SPI indicators diverged sufficiently to fall into different categories.\n",
    "\n",
    "Overall, \n",
    "\n",
    "**SPEI**- Following on from Section 3, \n",
    "\n",
    "**SPI/SPEI Ensemble**- Lastly, as was shown in Section 4, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.271597,
     "end_time": "2024-03-08T17:40:01.664067",
     "exception": false,
     "start_time": "2024-03-08T17:40:01.392470",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## â„¹ï¸ If you want to know more\n",
    "\n",
    "### Key resources\n",
    "The CDS catalogue entries for the data used were:\n",
    "* Complete ERA5 global atmospheric reanalysis: [reanalysis-era5-complete](https://doi.org/10.24381/cds.143582cf)\n",
    "* ERA5 hourly data on single levels from 1940 to present: [reanalysis-era5-single-levels](https://doi.org/10.24381/cds.adbb2d47)\n",
    "* Monthly drought indices from 1940 to present derived from ERA5 reanalysis: [derived-drought-historical-monthly](https://doi.org/10.24381/9bea5e16)\n",
    "\n",
    "Code libraries used:\n",
    "* [earthkit](https://github.com/ecmwf/earthkit)\n",
    "  * [earthkit-data](https://github.com/ecmwf/earthkit-data)\n",
    "  * [earthkit-plots](https://github.com/ecmwf/earthkit-plots)\n",
    "    \n",
    "### References\n",
    "\n",
    "[[ECIU+25](https://eciu.net/analysis/reports/2025/estimated-financial-losses-faced-by-uk-farmers-due-dry-weather-impacts-on-key-arable-crops)] Energy & Climate Intelligence Unit, â€˜Estimated financial losses faced by UK farmers due dry weather impacts on key arable cropsâ€™, Energy & Climate Intelligence Unit, London, United Kingdom, Dec. 2025.\n",
    "\n",
    "[[IPCC+23](https://doi.org/10.59327/IPCC/AR6-9789291691647)] IPCC, â€˜Climate Change 2023: Synthesis Report. Contribution of Working Groups I, II and III to the Sixth Assessment Report of the Intergovernmental Panel on Climate Changeâ€™, Intergovernmental Panel on Climate Change (IPCC), Geneva, Switzerland, Jul. 2023. doi: 10.59327/IPCC/AR6-9789291691647.\n",
    "\n",
    "[[UNICEF+24](https://www.unicef.org/media/165191/file/LACR-Flash-Update-11-November-2024.pdf)] UNICEF, â€˜Latin America and Caribbean Region Flash Update No. 2 (Climate-related crisis in the Amazon Region)â€™, UNICEF, Nov. 2024.\n",
    "\n",
    "[[Franch-Pardo+25](https://doi.org/10.48088/ejg.i.fra.16.2.286.297)] I. Franch-Pardo, P. A. F. Puig, and A. CerdÃ , â€˜Geospatial Technologies in Crisis Response: Analyzing the 2024 Floods in Valencia, Spainâ€™, European Journal of Geography, vol. 16, no. 2, pp. 286â€“297, Aug. 2025, doi: 10.48088/ejg.i.fra.16.2.286.297.\n",
    "\n",
    "[[McKee+93](https://climate.colostate.edu/pdfs/relationshipofdroughtfrequency.pdf)] T. B. McKee, N. J. Doesken, and J. Kleist, â€˜The relationship of drought frequency and duration to time scalesâ€™, in Eighth Conference on Applied Climatology, Anaheim, California, USA, Jan. 1993.\n",
    "\n",
    "[[Vicente-Serrano+10](https://doi.org/10.1175/2009JCLI2909.1)] S. M. Vicente-Serrano, S. BeguerÃ­a, and J. I. LÃ³pez-Moreno, â€˜A Multiscalar Drought Index Sensitive to Global Warming: The Standardized Precipitation Evapotranspiration Indexâ€™, Journal of Climate, vol. 23, no. 7, pp. 1696â€“1718, Apr. 2010, doi: 10.1175/2009JCLI2909.1.\n",
    "\n",
    "[[Soci+24](https://doi.org/10.1002/qj.4803)] C. Soci et al., â€˜The ERA5 global reanalysis from 1940 to 2022â€™, Quarterly Journal of the Royal Meteorological Society, vol. 150, no. 764, pp. 4014â€“4048, Jul. 2024, doi: 10.1002/qj.4803.\n",
    "\n",
    "[[Hersbach+20](https://doi.org/10.1002/qj.3803)] H. Hersbach et al., â€˜The ERA5 global reanalysisâ€™, Quarterly Journal of the Royal Meteorological Society, vol. 146, no. 730, pp. 1999â€“2049, May 2020, doi: 10.1002/qj.3803.\n",
    "\n",
    "[[Keune+25](https://doi.org/10.1038/s41597-025-04896-y)] J. Keune, F. Di Giuseppe, C. Barnard, E. Damasio da Costa, and F. Wetterhall, â€˜ERA5â€“Drought: Global drought indices based on ECMWF reanalysisâ€™, Scientific Data, vol. 12, p. 616, Apr. 2025, doi: 10.1038/s41597-025-04896-y.\n",
    "\n",
    "[[Stagge+15](https://doi.org/10.1002/joc.4267)] J. H. Stagge, L. M. Tallaksen, L. Gudmundsson, A. F. Van Loon, and K. Stahl, â€˜Candidate Distributions for Climatological Drought Indices (SPI and SPEI)â€™, International Journal of Climatology, vol. 35, no. 13, pp. 4027â€“4040, Feb. 2015, doi: 10.1002/joc.4267.\n",
    "\n",
    "[[Shapiro+65](https://doi.org/10.1093/biomet/52.3-4.591)] S. S. Shapiro and M. B. Wilk, â€˜An analysis of variance test for normality (complete samples)â€™, Biometrika, vol. 52, no. 3â€“4, pp. 591â€“611, Dec. 1965, doi: 10.1093/biomet/52.3-4.591."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 968.520731,
   "end_time": "2024-03-08T17:40:03.783430",
   "environment_variables": {},
   "exception": null,
   "input_path": "D520.3.2.3b.SEASONAL_multimodel-bias_v5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T17:23:55.262699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
