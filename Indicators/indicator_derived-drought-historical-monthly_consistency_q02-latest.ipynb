{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Quality Assessment for ERA5 Drought Indicator: Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Production date: 2026-xx-xx\n",
    "\n",
    "**Please note that this repository is used for development and review, so quality assessments should be considered work in progress until they are merged into the main branch.**\n",
    "\n",
    "Dataset version: 1.0.\n",
    "\n",
    "Produced by: Enis Gerxhalija, Olivier Burggraaff (National Physical Laboratory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸŒ Use case: Retrieving drought indicators from the ERA5-Drought dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## â“ Quality assessment question\n",
    "* **Are the drought indicators in the ERA5-Drought dataset consistent with and reproducible from ERA5 data?**\n",
    "* **Are the drought indicators in the ERA5-Drought dataset presented in a format that ensures optimal usability for users?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Feedback:** Make reference format consistent with Atlas and AgERA5 notebooks\n",
    "(further feedback on introduction at later stage)\n",
    "\n",
    "Human-induced climate change is likely the primary driver behind the number of increased droughts and heavy precipitation since the 1950s, per the latest assessment report by the Intergovernmental Panel on Climate Change [[IPCC, 2023]](https://doi.org/10.59327/IPCC/AR6-9789291691647). With further global warming at 1.5Â°C and above, heavy precipitation, flooding and drought events are projected to intensify and become more frequent in most regions of Africa, Asia, North America and Europe. The environmental and societal impact of such extreme weather events are far-reaching. In the United Kingdom alone, the 2020s have seen three of the five worst harvests on record, with extreme heat and drought in 2025 causing more than Â£800mn lost revenue in harvest [[Energy & Climate Intelligence Unit]](https://mcusercontent.com/8ed7ad7972fae058e8f4fb7e8/files/6d02e6e7-8639-a44d-5a8c-2313124ef699/Costs_of_climate_analysis_011225.pdf). In 2023-2024, the Amazon region in Brazil faced an 18-month drought considered the most severe since drought monitoring began in 1954. By November 2024, it left 720 health centres in drought-affected areas of Brazil to become non-operational [[UNICEF, 2024]](https://www.unicef.org/media/165191/file/LACR-Flash-Update-11-November-2024.pdf). [**Mention opposite use case of extremely high precipitation**]\n",
    "\n",
    "A large scientific effort has gone into identifying areas more prone to drought along with monitoring areas currently experiencing drought conditions and accurately quantifying their severity. The objective quantification of drought severity remains an ongoing endeavour amongst scientists as there is not one physical variable that describes a drought. One might assume that drought severity can be measured by the total precipitation in that region, but this would overlook water-loss from the land surface through evapotranspiration, soil moisture levels, temperature anomalies and other natural variables. There is however consensus that drought indices, proxies based on long-term and shorter-term historical weather data, can accurately quantify drought severity and their impact, with studies linking the variability of drought indices to crop yields [[Vicenteâ€Serrano et al. , 2006.]](https://doi.org/10.1080/01431160500296032), and frequency of wildfires [[Russo, A et al. , 2017.]]((https://doi.org/10.1016/j.agrformet.2017.01.021) ). Two widely-employed drought indices are the Standardised Precipitation Index (SPI) [[McKee et al. ,1993]](https://climate.colostate.edu/pdfs/relationshipofdroughtfrequency.pdf), endorsed by the World Metereological Organisation (WMO), and the more recent Standardised Precipitation-Evapotranspiration Index (SPEI) [[Vicente-Serrano et al., 2010]](https://doi.org/10.1175/2009JCLI2909.1). **[why were they created, what is the difference, how are they used? The stuff about accumulation windows can also go here.]**\n",
    "\n",
    "The ERA5-Drought dataset provides a reanalysis-based dataset of the aforementioned indices using the ECMWF Reanalysis version 5 (ERA5), at a resolution of 0.25Â° globally (around 28 km) from the start of the reanalysis (in 1940) to today [[Keune, J et al. , 2025.]](https://doi.org/10.1038/s41597-025-04896-y) . The ERA5-Drought dataset consists of 1 deterministic and 10 ensemble drought-index members from slightly different initial conditions, enabling an estimate of the uncertainty. [**The third paragraph should introduce ERA5 a bit more (couple sentences, including the reanalysis and ensemble components) and then go into ERA5-Drought as a more convenient point of access because it's pre-calculated.**]\n",
    "\n",
    "This notebook aims to give users much-needed confidence and transparency in the calculation of the SPI and SPEI drought indices along with their quality flags. The C3S ERA5-Drought dataset must be consistent with and reproducible from its origins. Here, we assess this consistency and reproducibility by comparing drought-indicators retrieved from the ERA5-Drought dataset with their equivalents calculated from the origin dataset (or similar). While a full analysis and reproduction of every record within the C3S ERA5-Drought dataset is outside the scope of quality assessment (as it would require high-performance computing infrastructure), a case study with a narrower scope probes these quality attributes of the dataset and can be a jumping-off point for further analysis by the reader."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ“¢ Quality assessment statement\n",
    "\n",
    "```{admonition} These are the key outcomes of this assessment\n",
    ":class: note\n",
    "* Conclusion 1\n",
    "* Conclusion 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸ“‹ Methodology\n",
    "\n",
    "This quality assessment tests the consistency between drought indices retrieved from the [C3S ERA5-Drought dataset] and their equivalents calculated from the origin datasets, as well as the reproducibility and usability of said dataset.\n",
    "\n",
    "We will examine the SPI and SPEI drought indicators calculated from the following datasets:\n",
    "\n",
    "(include table here of the parameter, description of that parameters, and the origin dataset)\n",
    "\n",
    "The analysis and results are organised in the following steps, which are detailed in the sections below:\n",
    "\n",
    "**[](section-code_setup)**\n",
    " * Import all required libraries.\n",
    " * Define helper functions.\n",
    "**[](section-cds_setup)**\n",
    " * Define SPI Indicator\n",
    " * Download ERA5 precipitation\n",
    " * Accumulate\n",
    " * Calculate SPI\n",
    " * Calculate quality flags from ERA5 data\n",
    " * Download quality flags from ERA5-Drought\n",
    " * Compare quality flags\n",
    " * Download ERA5-Drought SPI\n",
    " * Comparison\n",
    "**[](section-spi)**\n",
    " * Define SPEI Indicator\n",
    " * Download ERA5 potential evaporation\n",
    " * Accumulate\n",
    " * Calculate SPEI\n",
    " * Calculate quality flags from ERA5 data\n",
    " * Download quality flags from ERA5-Drought\n",
    " * Compare quality flags\n",
    " * Download ERA5-Drought SPEi\n",
    " * Comparison\n",
    "**[](section-spei)**\n",
    "**[](section-ens)** \n",
    "**[](section-conclusion)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Analysis and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-code_setup)=\n",
    "### 1. Code setup.\n",
    "\n",
    "```{note}\n",
    "This notebook uses [earthkit](https://github.com/ecmwf/earthkit) for downloading ([earthkit-data](https://github.com/ecmwf/earthkit-data)) and visualising ([earthkit-plots](https://github.com/ecmwf/earthkit-plots)) data. Because earthkit is in active development, some functionality may change after this notebook is published. If any part of the code stops functioning, please raise an issue on our GitHub repository so it can be fixed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import required libraries\n",
    "In this section, we import all the relevant packages needed for running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Input / Output\n",
    "from pathlib import Path\n",
    "import earthkit.data as ekd\n",
    "import warnings\n",
    "\n",
    "# General data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "\n",
    "# Analysis\n",
    "import calendar\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Visualisation\n",
    "import earthkit.plots as ekp\n",
    "from earthkit.plots.styles import Style\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap, LogNorm, BoundaryNorm\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "from tqdm import tqdm  # Progress bars\n",
    "\n",
    "# Visualisation in Jupyter book -- automatically ignored otherwise\n",
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = None\n",
    "\n",
    "# Type hints\n",
    "from typing import Callable, Iterable, Optional\n",
    "from scipy.stats import rv_continuous as Distribution\n",
    "from earthkit.plots.geo.domains import Domain\n",
    "AnyDomain = (Domain | str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "This section defines some functions and variables used in the following analysis, allowing code cells in later sections to be shorter and ensuring consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "##### Data (pre-)processing.\n",
    "The following functions handle [data chunking in dask](https://docs.xarray.dev/en/latest/user-guide/dask.html) for computational efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunk(data: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\" Re-chunk a dataset into pre-determined optimal chunks. \"\"\"\n",
    "    # Might need to be adjusted for different coordinate names\n",
    "    return data.chunk({\"valid_time\": -1, \"latitude\": 103, \"longitude\": 360})\n",
    "\n",
    "# Feedback: Look at the equivalent function in the uncertainty notebook, use (and improve â€“ I just noticed the n_members kwarg is not used optimally) that\n",
    "def create_ens(dataset, no_ens = 10):\n",
    "    _, index = np.unique(dataset['time'], return_index=True)\n",
    "    \n",
    "    ens_dataset = []\n",
    "    \n",
    "    for i in range(0,no_ens):\n",
    "        ens_member = dataset.isel(time = index + i)    \n",
    "        ens_dataset.append(ens_member)\n",
    "        \n",
    "    drought_ens = xr.concat(ens_dataset, dim=\"number\")\n",
    "    drought_ens = drought_ens.assign_coords(number=range(0,no_ens))  # or 1..10 if you prefer\n",
    "\n",
    "    return drought_ens\n",
    "\n",
    "def add_number_dimension(data: xr.Dataset, *, n_members: int=10, time_dim: str=\"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Convert a dataset\n",
    "    from ERA5-Drought format (10-duplicate `time` dimension)\n",
    "    to ERA5 format (`number` dimension for ensemble members)\n",
    "    \"\"\"\n",
    "    # Find unique times and use these to generate datasets for successive members\n",
    "    member_numbers = np.arange(n_members)\n",
    "    _, index = np.unique(data[time_dim], return_index=True)\n",
    "    data = [data.isel({time_dim: index + i}) for i in member_numbers]\n",
    "\n",
    "    # Combine into one dataset\n",
    "    data = xr.concat(data, dim=\"number\").assign_coords(number=member_numbers)\n",
    "\n",
    "    # Rechunk for memory efficiency\n",
    "    data = data.chunk({\"number\": 10, time_dim: 48, \"lat\": 360, \"lon\": 103})\n",
    "\n",
    "    return data\n",
    "\n",
    "def safe_rename(ds: xr.Dataset):\n",
    "    rename_map = {\n",
    "        \"valid_time\": \"time\",\n",
    "        \"latitude\": \"lat\",\n",
    "        \"longitude\": \"lon\",\n",
    "    }\n",
    "\n",
    "    # Only rename keys that exist\n",
    "    actual_map = {old: new for old, new in rename_map.items() if old in ds.coords}\n",
    "\n",
    "    # Perform rename\n",
    "    ds = ds.rename(actual_map)\n",
    "\n",
    "    # Remove duplicate coords if they exist\n",
    "    for old, new in rename_map.items():\n",
    "        if new in ds.coords and old in ds.coords and old != new:\n",
    "            ds = ds.drop_vars(old)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Helper functions for reading data.\n",
    "The following functions are for handling read of data and ease-of-use for downloading C3S data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def era5_drought_api_multiple(indicator: str, var:str, product_type: str, accum_period = [1, 3, 6, 12, 24, 36, 48]):\n",
    "    \"\"\"\"Function to send multiple API requests for quality indicators and probability of zero precipitation.\"\"\"\n",
    "    \n",
    "    # --- Normalise & validate indicator ---\n",
    "    ind= indicator.strip().upper()\n",
    "    if ind not in {\"SPI\", \"SPEI\"}:\n",
    "        raise ValueError(\"indicator must be 'SPI' or 'SPEI'\")\n",
    "    ind = ind.lower()  # \"spi\" or \"spei\"\n",
    "\n",
    "    # --- Normalise & validate variable key ---\n",
    "    var_key = var.strip().lower()\n",
    "\n",
    "    dataset_name = \"derived-drought-historical-monthly\"\n",
    "\n",
    "\n",
    "    # --- Dictionary-based mapping ---\n",
    "    variable_map = {\n",
    "        \"prob_zero\": {\n",
    "            \"request_var\": \"probability_of_zero_precipitation_{}\",\n",
    "            \"source_var_name\": \"pzero\",\n",
    "            \"rename_prefix\": \"prob_zero\",\n",
    "        },\n",
    "        \"quality\": {\n",
    "            \"request_var\": \"test_for_normality_{}\",\n",
    "            \"source_var_name\": \"significance\",\n",
    "            \"rename_prefix\": \"significance\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    product_type = product_type.strip().lower()\n",
    "    \n",
    "    if product_type not in {\"reanalysis\", \"ensemble_members\"}:\n",
    "        raise ValueError(\"product_type must be either 'reanalysis' or 'ens'\")\n",
    "            \n",
    "    if var_key not in variable_map:\n",
    "        raise ValueError(\"var must be 'prob_zero' or 'quality'\")\n",
    "\n",
    "    mapping = variable_map[var_key]\n",
    "\n",
    "    # Construct the request variable using the indicator\n",
    "    request_var = mapping[\"request_var\"].format(ind)\n",
    "    source_var_name = mapping[\"source_var_name\"]\n",
    "    rename_prefix = mapping[\"rename_prefix\"]\n",
    "\n",
    "    out = []\n",
    "    \n",
    "    for p in accum_period:\n",
    "        request = {\n",
    "            \"variable\": [request_var],\n",
    "            \"accumulation_period\": [str(p)],\n",
    "            \"version\": \"1_0\",\n",
    "            \"product_type\": product_type,\n",
    "            \"dataset_type\": \"consolidated_dataset\",\n",
    "            \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "        }\n",
    "\n",
    "        ds = ekd.from_source(\"cds\", dataset_name, request).to_xarray(compat=\"equals\")\n",
    "        \n",
    "        if source_var_name not in ds.variables:\n",
    "            raise KeyError(\n",
    "                f\"Expected variable '{source_var_name}' not found for period {p}. \"\n",
    "                f\"Available: {list(ds.variables)}\"\n",
    "            )\n",
    "\n",
    "        new_name = f\"{rename_prefix}_{p}\"\n",
    "        ds_renamed = ds.rename({source_var_name: new_name})[[new_name]]\n",
    "        out.append(ds_renamed)\n",
    "\n",
    "    merged = xr.merge(out, compat=\"override\")\n",
    "    merged.to_array(\"accumulation_period\")\n",
    "    merged.assign_coords(accumulation_period=(\"accumulation_period\", accum_period))\n",
    "        \n",
    "    return merged\n",
    "\n",
    "def era5_drought_index_multiple(indicator, accum_period = [1, 3, 6, 12, 24, 36, 48]):\n",
    "    ind = indicator.strip().upper()\n",
    "    if ind not in {\"SPI\", \"SPEI\"}:\n",
    "        raise ValueError(\"indicator must be 'SPI' or 'SPEI'\")\n",
    "\n",
    "    if ind == \"SPI\":\n",
    "        request_var = \"standardised_precipitation_index\"\n",
    "    elif ind == \"SPEI\":\n",
    "        request_var = \"standardised_precipitation_evapotranspiration_index\"\n",
    "\n",
    "    ens_dataset = \"derived-drought-historical-monthly\"\n",
    "    \n",
    "    out = []\n",
    "\n",
    "    for p in accum_period:\n",
    "        request = {\n",
    "            \"variable\": [request_var],\n",
    "            \"accumulation_period\": [str(p)],\n",
    "            \"version\": \"1_0\",\n",
    "            \"product_type\": [\"ensemble_members\"],\n",
    "            \"dataset_type\": \"consolidated_dataset\",\n",
    "            \"year\": [f\"{y}\" for y in range(1940,2025)],\n",
    "            \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "            \"area\":[10, 40, 9, 41]\n",
    "        }\n",
    "\n",
    "        ds = ekd.from_source(\"cds\", ens_dataset, request).to_xarray(compat=\"equals\")\n",
    "        ds = create_ens(ds)\n",
    "        out.append(ds)\n",
    "\n",
    "    out = xr.merge(out, compat=\"equals\")\n",
    "    return out\n",
    "\n",
    "def box_around(lat, lon, half_size=0.25):\n",
    "    \"\"\"\n",
    "    Return a geographic bounding box around a point.\n",
    "\n",
    "    The box extends `half_size` degrees north, south, east, and west \n",
    "    from the given latitude and longitude. Output is returned as \n",
    "    [north, west, south, east], rounded to two decimal places.\n",
    "    \"\"\"\n",
    "\n",
    "    north = lat + half_size\n",
    "    south = lat - half_size\n",
    "    west  = lon - half_size\n",
    "    east  = lon + half_size\n",
    "    return [round(north, 2), round(west, 2), round(south, 2), round(east, 2)]\n",
    "\n",
    "def find_time_dim(ds: xr.Dataset):\n",
    "    \"\"\"\n",
    "    Detect and return the name of the time dimension in an xarray Dataset.\n",
    "\n",
    "    Searches through dataset dimensions and returns the first one containing \n",
    "    the substring \"time\". Intended for automatic time-dimension detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    time_dim = next(dim for dim in ds.dims if \"time\" in dim)  \n",
    "    \n",
    "    return time_dim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### Helper functions for accumualtion precipitation/potential evaporation and calculating zero-precipitation statistics.\n",
    "The following cells contain constants and functions used in accumulating variables (e.g. precipitation) over time and calculating zero precipitation statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants such as the accumulation periods to use\n",
    "ACCUMULATION_PERIODS = [1, 3, 6, 12, 24, 36, 48]  # Months\n",
    "MONTHS = range(1, 13)  # January to December (inclusive)\n",
    "\n",
    "# Perform accumulation\n",
    "def accum_var(data: xr.Dataset, var: str, *,\n",
    "              accumulation_periods: Iterable[int]=ACCUMULATION_PERIODS, time_dim: Optional[str]=None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Compute the precipitation / potential evaporation accumulation window. \n",
    "    \n",
    "    1. Convert precipitation/potential evaporation from meters to millimeters.\n",
    "    2. Compute monthly totals (accounting for days in month).\n",
    "    3. Add rolling accumulation windows on monthly totals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Detect time dimension if not provided\n",
    "    time_dim = find_time_dim(data)\n",
    "    \n",
    "    # Step 1: Convert to mm\n",
    "    data[var] = data[var] * 1000\n",
    "    data[var].attrs[\"units\"] = \"mm\"\n",
    "\n",
    "\n",
    "    # Step 2: Compute monthly totals\n",
    "    time_index = pd.to_datetime(data[time_dim].values)\n",
    "   \n",
    "    days_in_month = xr.DataArray(\n",
    "        time_index.days_in_month,\n",
    "        coords={time_dim: data[time_dim].values},\n",
    "        dims=[time_dim]\n",
    "    ) # Create days in month x-array with time dimension aligned for easy computation.\n",
    "\n",
    "    data[f\"{var}_monthly_total\"] = data[var] * days_in_month\n",
    "    \n",
    "    # Step 3: Add rolling accumulation windows\n",
    "    for period in accumulation_periods:\n",
    "        rolling_sum = data[f\"{var}_monthly_total\"].rolling({time_dim: period}, center=False).sum()\n",
    "        data[f'{var}_mm_accum_{period}m'] = rolling_sum\n",
    "\n",
    "    data = data.chunk({time_dim: -1})  # Full time dimension in 1 chunk for time series analysis efficiency\n",
    "\n",
    "    return data\n",
    "\n",
    "def zero_precip_prob(precip_ds: xr.Dataset, reference_window):\n",
    "    \"\"\"\n",
    "    Compute the per-calendar-month probability of zero precipitation.\n",
    "\n",
    "    This function:\n",
    "      1) Rounds to 0.01 mm,\n",
    "      2) Flags months with cumulative precipitation <= 0.00 mm as \"zero\",\n",
    "      3) Groups by calendar month and computes the fraction of zero months.\n",
    "    \"\"\"\n",
    "\n",
    "    precip_ds_ref = precip_ds.sel(**reference_window) \n",
    "    \n",
    "    precip = precip_ds_ref.round(2) # round to nearest 0.01 mm.\n",
    "    \n",
    "    # Threshold precipitation (all months with cumulative precipitation below are \"zero\").\n",
    "    prec_eps = 0.00\n",
    "    \n",
    "    # All months with precipitation less than precipitation.\n",
    "    is_zero = (precip <= prec_eps)\n",
    "    \n",
    "    month = is_zero[\"valid_time\"].dt.month                         \n",
    "    \n",
    "    # Zero precipitation stats.\n",
    "    n_zero  = is_zero.groupby(month).sum(dim=\"valid_time\")\n",
    "                              \n",
    "    # Count total months per calendar month\n",
    "    n_month = 30 \n",
    "    \n",
    "    # Probablility of months with zero precipitation.\n",
    "    p_zero = xr.where(\n",
    "        n_zero > 0,\n",
    "        (n_zero) / ((n_month + 1)),\n",
    "        0\n",
    "    )\n",
    "   \n",
    "    # Weighted probability with zero precipitation.\n",
    "    weighted_p_zero = xr.where(\n",
    "        n_zero > 0,\n",
    "        (n_zero + 1) / (2 * (n_month + 1)),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    return p_zero, weighted_p_zero\n",
    "\n",
    "def zero_precip_rejection(prob_ds: xr.Dataset, ls_mask):\n",
    "    \"\"\"\n",
    "    Compute zero-precipitation rejection masks from monthly probabilities and\n",
    "    apply a landâ€“sea mask.\n",
    "\n",
    "    Logic:\n",
    "      - Mark a month as 'rejected' if probability >= 0.33 (1 = reject, 0 = keep).\n",
    "      - Aggregate across the 'month' dimension to get the count per grid cell.\n",
    "      - Apply a landâ€“sea mask so ocean pixels are set to NaN.\n",
    "      - Return both the monthly rejection field and the aggregated counts across time.\n",
    "    \"\"\"\n",
    "    \n",
    "    rejection = xr.where(\n",
    "            prob_ds >= 0.33,\n",
    "            1,\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # Aggregate across time \n",
    "    rejection_agg = rejection.sum(dim=[d for d in [\"month\", \"time\"] if d in rejection.dims])\n",
    "\n",
    "    # Apply land-sea mask to monthly\n",
    "    rejection_ds = rejection.where(ls_mask[\"lsm\"])\n",
    "    \n",
    "    # Apply land-sea mask to aggregated\n",
    "    rejection_agg_ds = rejection_agg.where(ls_mask[\"lsm\"])\n",
    "\n",
    "    return rejection_ds, rejection_agg_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### Functions used for computing the SPI / SPEI index\n",
    "The following function fits the gamma / general log-logistic distribution to the precipitation data for each calendar month along with calculating the cumulative distribution functions (CDFs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General fitting function\n",
    "from scipy.stats import fit as stats_fit, gamma\n",
    "\n",
    "def fit_distributions(reference_data: xr.Dataset, distribution: Distribution, *,\n",
    "                                  time_dim: Optional[str]=None) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Fit distributions (e.g. gamma) for each month and accumulation period using xarray parallelisation.\n",
    "    Data are assumed to have been sliced to the reference period.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define fitting function\n",
    "    def fit(y):\n",
    "        y = y[np.isfinite(y)]\n",
    "        try:\n",
    "            params = distribution.fit(y) \n",
    "        except:\n",
    "            params = [np.nan]*(distribution.numargs+2)\n",
    "        finally:\n",
    "            params = np.stack(params, axis=-1)  # Extend with axis for stats (alpha, loc, scale ...)\n",
    "        return params\n",
    "\n",
    "    # Find time dimension name\n",
    "    time_dim = find_time_dim(reference_data)\n",
    "\n",
    "    # Split dataset by month\n",
    "    reference_data_by_month = reference_data.groupby(reference_data[time_dim].dt.month)\n",
    "\n",
    "    # Apply fitting function by month\n",
    "    params = xr.apply_ufunc(fit, reference_data_by_month,\n",
    "                            input_core_dims=[[time_dim]], output_core_dims=[[\"stat\"]],\n",
    "                            vectorize=True, dask=\"parallelized\",\n",
    "                            dask_gufunc_kwargs={\"output_sizes\": {\"stat\": distribution.numargs+2}},  # e.g. 3 for gamma (alpha, loc, scale)\n",
    "                            output_dtypes=[np.float64],\n",
    "                           )\n",
    "    params = params.chunk({\"month\": -1})\n",
    "\n",
    "    return params\n",
    "\n",
    "# Fitting functions for SPI, SPEI specifically\n",
    "fit_monthly_spi  = partial(fit_distributions, distribution=stats.gamma) # partial function\n",
    "fit_monthly_spei = partial(fit_distributions, distribution=stats.genlogistic)\n",
    "\n",
    "def cdf_clip(cdf, eps = 1e-16):\n",
    "    \"\"\"\n",
    "    Clip CDF values to threshold (here given by eps).\n",
    "    \"\"\"\n",
    "    clipped = cdf.clip(eps, 1 - eps)\n",
    "    return clipped\n",
    "\n",
    "def compute_cdf(data: xr.Dataset, monthly_params: xr.Dataset, distribution: Distribution, *,\n",
    "                               index_name: Optional[str]=None, time_dim: Optional[str]=None) -> tuple[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Compute SPI time series for each accumulation period using fitted distribution parameters.\n",
    "    Returns two datasets: CDF values and index (SPI/SPEI) values.\n",
    "    \"\"\"\n",
    "    # Detect time dimension if not provided\n",
    "    time_dim = find_time_dim(data)\n",
    "\n",
    "    # Create a month dimension for broadcasting with the one in monthly_params\n",
    "    month_da = data[time_dim].dt.month.rename(\"month\")\n",
    "\n",
    "    # Extract parameters\n",
    "    nr_params = monthly_params.sizes[\"stat\"]  # 3 for gamma / genlogistic\n",
    "    params_extracted = [monthly_params.sel(stat=j).sel(month=month_da) for j in range(nr_params)]\n",
    "\n",
    "    # Calculate CDF values by month\n",
    "    cdf: xr.Dataset = xr.apply_ufunc(distribution.cdf, data, *params_extracted,\n",
    "                         input_core_dims=[[], [], [], []], output_core_dims=[[]],\n",
    "                         vectorize=True, dask=\"parallelized\",\n",
    "                         output_dtypes=[np.float64],\n",
    "                         keep_attrs=True\n",
    "                        )\n",
    "    \n",
    "    cdf = cdf.chunk({time_dim: -1})\n",
    "    cdf = cdf_clip(cdf)  # clip values that are greater than or less than 1e-16.\n",
    "    \n",
    "    return cdf\n",
    "\n",
    "# Computing functions for SPI, SPEI specifically\n",
    "compute_monthly_spi  = partial(compute_cdf, distribution=stats.gamma,       index_name=\"SPI\")\n",
    "compute_monthly_spei = partial(compute_cdf, distribution=stats.genlogistic, index_name=\"SPEI\")\n",
    "\n",
    "    \n",
    "def cdf_to_znorm_transform(adjusted_cdf_ds: xr.Dataset, index_name: Optional[str]=None):\n",
    "    \"\"\" Function to transform CDF to a Z-Normal distribution with Mean of 0 and Variance of 1.\n",
    "     Returns transformed CDF \"\"\"\n",
    "        \n",
    "    clipped = cdf_clip(adjusted_cdf_ds) \n",
    "    \n",
    "    # Convert CDF to index\n",
    "    adjusted_spi_ds = xr.apply_ufunc(stats.norm.ppf, clipped, 0.0, 1.0,  \n",
    "                                     input_core_dims=[[], [], []], output_core_dims=[[]],\n",
    "                                     vectorize=True, dask=\"parallelized\",\n",
    "                                     output_dtypes=[np.float64],\n",
    "                                     keep_attrs=True,\n",
    "                                    )\n",
    "    accumulation_variables = {var: var.split(\"_\")[-1][:-1] for var in adjusted_spi_ds.variables if \"accum\" in var}  # Get periods as number strings\n",
    "    rename_variables = {var: f\"{f'{index_name}'}{accumulation_period}\" for var, accumulation_period in accumulation_variables.items()}\n",
    "    \n",
    "    return adjusted_spi_ds.rename_vars(rename_variables) # Returns transformed dataset with renamed variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "##### Functions used for adjusting indices with weighted zero precipitation probability.\n",
    "The following functions adjust the CDF with the weighted zero-precipitation probability, and also performs the Shapiro-Wilks test for normality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_precip_adjustment(prec_data: xr.Dataset, cdf_ds: xr.Dataset, reference_window: dict[str, slice], \n",
    "                           accum_periods: Iterable[int]=ACCUMULATION_PERIODS):\n",
    "    \"\"\"\n",
    "    Adjust CDF for zero precipitation months in xarray.\n",
    "    Returns CDF adjusted for zero precipitation, along with a statistical summary of months and # of zero precipitation. \n",
    "    \"\"\" \n",
    "    \n",
    "    _, weight_p_zero = zero_precip_prob(prec_data, reference_window) # Calculate weighted probability.\n",
    "\n",
    "    weight_p_zero = weight_p_zero.chunk({\"month\": -1})\n",
    "\n",
    "    cdf_data_by_month = cdf_ds.groupby(\"valid_time.month\")\n",
    "\n",
    "    adjusted_cdf = cdf_data_by_month.map( # Map lambda function (below) that adjusts CDF (per month).\n",
    "        lambda x: weight_p_zero.sel(month=x[\"valid_time.month\"]) + (1 - weight_p_zero.sel(month=x[\"valid_time.month\"])) * x \n",
    "    )  \n",
    "\n",
    "    return adjusted_cdf\n",
    "    \n",
    "def xr_shapiro_test(spi_ds: xr.Dataset, reference_window: dict[str, slice],\n",
    "                    accum_periods: Iterable[int]=ACCUMULATION_PERIODS,\n",
    "                    months = range(1,13), index_name: Optional[str]=None ):\n",
    "    \n",
    "    \"\"\" Function to perform Shapiro-Wilks test on data within reference period\n",
    "    Returns Shapiro-Wilks statistic along with probability of falling under normal distribution, along with significance value (1/0).\"\"\"\n",
    "    \n",
    "    spi_ref = spi_ds.sel(**reference_window)  \n",
    "\n",
    "    time_dim = find_time_dim(spi_ds) # find time dimension.\n",
    "        \n",
    "    spi_ref = spi_ref.chunk({time_dim: -1}) \n",
    "    \n",
    "    spi_ref = spi_ref.where(np.isfinite(spi_ref))  # Mask non-finite values before applying shapiro-wilks.\n",
    "\n",
    "    spi_ref_by_month = spi_ref.groupby(f\"{time_dim}.month\")\n",
    "\n",
    "    # Perform shapiro on xarray\n",
    "    _ , pval = xr.apply_ufunc(stats.shapiro, spi_ref_by_month,\n",
    "                                input_core_dims=[[time_dim]], output_core_dims=[[],[]],\n",
    "                                vectorize=True, dask=\"parallelized\",\n",
    "                                output_dtypes=[np.float64, np.float64],\n",
    "                                keep_attrs=True,\n",
    "                               )\n",
    "    \n",
    "    normality = xr.where(pval < 0.05, 0, 1)  # Values < 0.05 â†’ 0\n",
    "\n",
    "    for p in accum_periods:\n",
    "        normality = normality.rename({f\"{index_name}{p}\": f\"significance_{p}\"})\n",
    "\n",
    "    month_ts = pd.to_datetime([f\"2020-{int(m):02d}-01\" for m in normality[\"month\"].values])\n",
    "    \n",
    "    # Assign those timestamps to the existing 'month' dimension,\n",
    "    # then rename the dimension from 'month' -> 'time'\n",
    "    normality = (\n",
    "        normality\n",
    "        .assign_coords(month=(\"month\", month_ts))\n",
    "        .rename(month=\"time\")\n",
    "    )\n",
    "\n",
    "    return pval, normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "##### Functions used for apply S-W quality mask & categorising drought by severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sw_quality_mask(era5_quality: xr.Dataset, index_ds: xr.Dataset, indicator: str):\n",
    "    \"\"\"\n",
    "    Apply significance-based quality masks to drought indicator datasets.\n",
    "    \"\"\"\n",
    "    index_ds = safe_rename(index_ds)\n",
    "    for period in ACCUMULATION_PERIODS:\n",
    "        sig = era5_quality[f\"significance_{period}\"]\n",
    "        sig = sig.assign_coords(time=sig.time.dt.month)\n",
    "        mask = sig.sel(time=index_ds[f\"{indicator}{period}\"].time.dt.month)\n",
    "        index_ds[f\"{indicator}{period}\"] = index_ds[f\"{indicator}{period}\"].where(mask.values == 1)\n",
    "    return index_ds\n",
    "\n",
    "def bin_dataset(ds: xr.Dataset, bin_edges: list):\n",
    "\n",
    "    \"\"\"\n",
    "    Bin all variables in an xarray Dataset using numpy digitization.\n",
    "    \"\"\"\n",
    "    \n",
    "    return xr.apply_ufunc(\n",
    "        np.digitize,\n",
    "        ds,\n",
    "        input_core_dims=[[]],\n",
    "        kwargs={\"bins\": bin_edges, \"right\": False}, \n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[int],\n",
    "    )\n",
    "\n",
    "def categorisation(rep_ds, categories):\n",
    "    \n",
    "    \"\"\"\n",
    "    Categorise a dataset into bins defined by category thresholds.\n",
    "    NB. The categorisation given in the ERA5-Drought Paper are discrete!\n",
    "    \"\"\"\n",
    "    cat_ds = rep_ds.copy()\n",
    "    \n",
    "    bin_edges = []\n",
    "    for i, cat in enumerate(categories):\n",
    "        bin_edges.append(cat[0])\n",
    "        \n",
    "    del bin_edges[0] # delete first element (not needed)\n",
    "\n",
    "    cat_ds = bin_dataset(cat_ds, bin_edges) \n",
    "    \n",
    "    return cat_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "##### Functions for plotting data and stats.\n",
    "The following functions are for plotting and decorating data that is of interest to users of drought indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _glue_or_show(fig, glue_label=None):\n",
    "    try:\n",
    "        glue(glue_label, fig, display=False)\n",
    "    except TypeError:\n",
    "        plt.show()\n",
    "    finally:\n",
    "        plt.close()\n",
    "        \n",
    "def comparison_monthly_statistics(era5_drought_index: xr.Dataset, calculated_index: xr.Dataset\n",
    "                                  , indicator: str, accumulation_periods=ACCUMULATION_PERIODS) -> pd.DataFrame:\n",
    "    # Rename dims for alignment\n",
    "    calculated_index_ds = calculated_index.rename({\n",
    "        \"valid_time\": \"time\",\n",
    "        \"latitude\": \"lat\",\n",
    "        \"longitude\": \"lon\",\n",
    "    }).compute()\n",
    "    \n",
    "    \n",
    "    era5_drought_spi = era5_drought_index.assign_coords(\n",
    "        month = era5_drought_index[\"time\"].dt.month\n",
    "    ).compute()\n",
    "    \n",
    "    residual = (calculated_index_ds - era5_drought_spi)\n",
    "    abs_residual = abs(residual)\n",
    "    \n",
    "    # Prepare output containers\n",
    "    abs_median_table = pd.DataFrame(index=range(1, 13))\n",
    "    median_table = pd.DataFrame(index=range(1, 13))\n",
    "\n",
    "\n",
    "    for p in accumulation_periods:\n",
    "        index = f\"{indicator}{p}\"\n",
    "        \n",
    "        # Extract relevant arrays\n",
    "        res_index  = residual[index]\n",
    "        abs_index  = abs_residual[index]\n",
    "        \n",
    "        # Monthly medians\n",
    "        median_per_month = res_index.groupby(\"month\").median().to_pandas()\n",
    "        abs_median_per_month = abs_index.groupby(\"month\").median().to_pandas()\n",
    "\n",
    "\n",
    "        # Add columns for this accumulation period\n",
    "        abs_median_table[p] = abs_median_per_month\n",
    "        median_table[p] = median_per_month\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"abs_median\": abs_median_table,\n",
    "        \"median\": median_table\n",
    "    }\n",
    "\n",
    "def display_monthly_statistics(results: pd.DataFrame, var:str, title: str):\n",
    "    df = results[f\"{var}\"]\n",
    "    \n",
    "    # Month names\n",
    "    month_names = {\n",
    "        1: \"January\", 2: \"February\", 3: \"March\",\n",
    "        4: \"April\",   5: \"May\",      6: \"June\",\n",
    "        7: \"July\",    8: \"August\",   9: \"September\",\n",
    "        10:\"October\", 11:\"November\", 12:\"December\"\n",
    "    }\n",
    "        \n",
    "    df.index = df.index.map(month_names)\n",
    "    \n",
    "    # Style the dataframe\n",
    "    styled_df = df.style \\\n",
    "        .format(precision=5) \\\n",
    "        .set_caption(f\"C3S ERA5 Drought - {title}\")\n",
    "\n",
    "    # Glue the styled dataframe\n",
    "    glue(f\"{var}\", styled_df)\n",
    "\n",
    "def plot_index_comparison_side_by_side(\n",
    "    indicator: str,\n",
    "    era5_drought_index: xr.Dataset,\n",
    "    calculated_index: xr.Dataset,\n",
    "    categories,\n",
    "    accumulation_periods=ACCUMULATION_PERIODS,\n",
    "    figsize=(18, 3.5)\n",
    "):\n",
    "    \"\"\"\n",
    "    Produces a 2-column plot for each accumulation period:\n",
    "        Left: ERA5-Drought vs Reproduced drought index\n",
    "        Right: Residual + comparison statistics\n",
    "    \"\"\"\n",
    "\n",
    "    calculated_index_ds = safe_rename(calculated_index)\n",
    "\n",
    "    # Compute residual once\n",
    "    residual = calculated_index_ds - era5_drought_index\n",
    "\n",
    "    # Prepare figure with dynamic number of rows\n",
    "    n_periods = len(accumulation_periods)\n",
    "    fig, axs = plt.subplots(\n",
    "        n_periods, 2,\n",
    "        figsize=(figsize[0], figsize[1] * n_periods),\n",
    "        constrained_layout=True,\n",
    "        sharex=True\n",
    "    )\n",
    "\n",
    "    # Ensure axs is always 2D\n",
    "    if n_periods == 1:\n",
    "        axs = np.expand_dims(axs, axis=0)\n",
    "\n",
    "    # Overarching title.\n",
    "    fig.suptitle(f\"{indicator}: ERA5-Drought vs Reproduced + Residuals (Calc - ERA5D)\", fontsize=18)\n",
    "\n",
    "    for row, period in enumerate(accumulation_periods):\n",
    "        ax_left = axs[row, 0]\n",
    "        ax_right = axs[row, 1]\n",
    "\n",
    "        # --------------------------\n",
    "        # (1) LEFT PANEL â€” Visual Comparison\n",
    "        # --------------------------\n",
    "        ax_left.plot(\n",
    "            era5_drought_index[f\"{indicator}{period}\"].time,\n",
    "            era5_drought_index[f\"{indicator}{period}\"],\n",
    "            label=f\"ERA5 {indicator}{period}\",\n",
    "            color=\"tab:blue\"\n",
    "        )\n",
    "        ax_left.plot(\n",
    "            calculated_index_ds[f\"{indicator}{period}\"].time,\n",
    "            calculated_index_ds[f\"{indicator}{period}\"],\n",
    "            label=f\"Calculated {indicator}{period}\",\n",
    "            color=\"tab:orange\",\n",
    "            linestyle=\"--\"\n",
    "        )\n",
    "\n",
    "        # Add background bands\n",
    "        for low, high, label, colour in categories:\n",
    "            ax_left.axhspan(low, high, color=colour, alpha=0.25)  # alpha controls transparency\n",
    "        \n",
    "        ax_left.set_title(f\"{period}-Month Window\", fontsize=12)\n",
    "        ax_left.set_ylabel(indicator)\n",
    "        ax_left.grid(True)\n",
    "        ax_left.legend(fontsize=9)\n",
    "        ax_left.set_ylim([-8, 8])\n",
    "        \n",
    "        # --------------------------\n",
    "        # (2) RIGHT PANEL â€” Residual\n",
    "        # --------------------------\n",
    "        resid_values = residual[f\"{indicator}{period}\"]\n",
    "\n",
    "        mean_diff = resid_values.mean(\"time\").values\n",
    "        mean_abs_diff = abs(resid_values).mean(\"time\").values\n",
    "\n",
    "        ax_right.plot(\n",
    "            resid_values.time,\n",
    "            resid_values.values,\n",
    "            color=\"tab:purple\",\n",
    "            label=f\"Mean Î” = {mean_diff:.4f}\\nMean |Î”| = {mean_abs_diff:.4f}\"\n",
    "        )\n",
    "\n",
    "        ax_right.set_title(f\"Residuals ({period}-Month)\", fontsize=12)\n",
    "        ax_right.grid(True)\n",
    "        ax_right.set_ylabel(\"Residual\")\n",
    "        ax_right.legend(fontsize=9)\n",
    "        ax_right.set_ylim([-1, 1])\n",
    "    plt.show()\n",
    "\n",
    "def plot_accumulation(var: str, example_site_dataset: xr.Dataset, example_site,\n",
    "                     accumulation_periods=ACCUMULATION_PERIODS):\n",
    "    \n",
    "    \"\"\"Plot precipitation accumulation time series for multiple accumulation periods\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6), constrained_layout=True)\n",
    "    x = example_site_dataset['valid_time'].values\n",
    "    \n",
    "    for p in accumulation_periods:\n",
    "        y = example_site_dataset[f\"{var}_mm_accum_{p}m\"].values\n",
    "        ax.plot(x, y, label=f\"{p}-accumulated months\")\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_title(f\"Precipitation Accumulation at ({example_site['lat']} Â°N, {example_site['lon']} Â°E)\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(f\"{var} [mm]\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(reverse=True) # plot legend in reverse to match order of the lines (top-to-bottom).\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "    \n",
    "def plot_category_confusion_matrices(rep_xr: xr.Dataset, era_xr: xr.Dataset, index: str, index_categories: dict,\n",
    "                                     location: str, accumulation_windows = ACCUMULATION_PERIODS):\n",
    "    \n",
    "    # Build per-category boolean masks for each timestamp\n",
    "    cat_labels = [c[2] for c in index_categories] \n",
    "\n",
    "    # Number of categories\n",
    "    len_categories = len(index_categories) \n",
    "\n",
    "    # Replace index values with categorisation values.\n",
    "    rep_cat = categorisation(rep_xr, index_categories)\n",
    "    era_cat = categorisation(era_xr, index_categories)\n",
    "    \n",
    "    # Figure settings\n",
    "    n = len(accumulation_windows)\n",
    "    cols = 2\n",
    "    rows = (len(accumulation_windows) // cols) + 1\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 6*rows), gridspec_kw={\"hspace\": 0.01, \"wspace\": 0.01}, layout= \"constrained\") # Use constrained as default.\n",
    "    \n",
    "    fig.suptitle(f\"Confusion Matrix for {index} Severity Classification at {location}\", fontsize=20, y=1)\n",
    "    \n",
    "    fig.supylabel(\"ERA5-Drought\", fontweight = \"bold\", fontsize=20)\n",
    "    fig.supxlabel(\"Reproduced\", fontweight = \"bold\", fontsize=20)\n",
    "\n",
    "    for ax, p in zip(axes.ravel(), accumulation_windows):\n",
    "            \n",
    "        # Categorisation values for each accumulation window.\n",
    "        rep = rep_cat[f\"{index}{p}\"].values\n",
    "        era = era_cat[f\"{index}{p}\"].values\n",
    "        \n",
    "        ax.grid(False)\n",
    "        \n",
    "        # Create confusion matrix from categorisation values.\n",
    "        cm = confusion_matrix(era, rep, labels=np.arange(len_categories))\n",
    "\n",
    "        # Sum over row.\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Avoid division by zero and NaNs (rows with 0 samples)\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            cm_frac = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=(row_sums != 0))\n",
    "  \n",
    "        # Plot fractions for color background (vmin/vmax for consistent color scale)\n",
    "        im = ax.imshow(cm_frac, cmap=plt.cm.Blues, vmin=0.0, vmax=1.0)\n",
    "        last_im = im  # keep reference for colorbar\n",
    "      \n",
    "        # Ticks and labels\n",
    "        ax.set_xticks(np.arange(len(cat_labels)))\n",
    "        ax.set_yticks(np.arange(len(cat_labels)))\n",
    "        ax.set_xticklabels(cat_labels, rotation=30, ha=\"right\", fontsize=9)\n",
    "        ax.set_yticklabels(cat_labels, fontsize=9)\n",
    "\n",
    "        # Annotate with **counts** (not fractions)\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, f\"{cm[i, j]:.0f}\", ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm_frac[i, j] > 0.5 else \"black\", fontsize=9, \n",
    "                        clip_on=True )\n",
    "    \n",
    "        ax.set_title(f\"{index} {p}\")\n",
    "    \n",
    "    # Hide extra axes\n",
    "    for ax in axes.ravel()[n:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    # Add a single shared colorbar showing row-wise fraction\n",
    "    cbar = fig.colorbar(last_im, ax=axes, location='right', shrink=0.6)\n",
    "    cbar.set_label(\"Row-wise fraction\")\n",
    "        \n",
    "    # plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "    plt.show()\n",
    "\n",
    "def decorate_fig(fig: ekp.Figure, *, title: Optional[str]=\"\") -> None:\n",
    "    \"\"\" Decorate an earthkit figure with land, coastlines, etc. \"\"\"\n",
    "    # Add progress bar because individual steps can be very slow for large plots\n",
    "    with tqdm(total=4, desc=\"Decorating\", leave=False) as progressbar:\n",
    "        fig.land()\n",
    "        progressbar.update()\n",
    "        fig.coastlines()\n",
    "        progressbar.update()\n",
    "        # fig.borders()\n",
    "        # progressbar.update()\n",
    "        fig.gridlines(linestyle=plt.rcParams[\"grid.linestyle\"])\n",
    "        progressbar.update()\n",
    "        fig.title(title)\n",
    "        progressbar.update()\n",
    "\n",
    "def quality_flag_spatial_comparison(repro_ds: xr.Dataset, era5d_ds: xr.Dataset, accumulation_window: Iterable[int],\n",
    "                                    domain: str, fig_title: str, repro_var_tpl: str = \"tp_mm_accum_{p}m\",\n",
    "                                    era5d_var_tpl: str = \"prob_zero_{p}\"):\n",
    "    \"\"\"\n",
    "    Plot a 3-panel spatial comparison of a quality flag between two datasets, with a discrete\n",
    "    shared colorbar (1..12) for panels 1&2 and a discrete difference colorbar (-12..12) for panel 3.\n",
    "    \"\"\"\n",
    "    \n",
    "    p = int(accumulation_window)\n",
    "    \n",
    "    repro_var = repro_var_tpl.format(p=p)\n",
    "    era5d_var = era5d_var_tpl.format(p=p)\n",
    "\n",
    "\n",
    "    if repro_var not in repro_ds:\n",
    "        raise KeyError(f\"Variable '{repro_var}' not found in repro_ds.\")\n",
    "    if era5d_var not in era5d_ds:\n",
    "        raise KeyError(f\"Variable '{era5d_var}' not found in era5d_ds.\")\n",
    "        \n",
    "    # ---- Select data variables ----\n",
    "    da1 = repro_ds[repro_var] # Reproduced\n",
    "    da2 = era5d_ds[era5d_var] # ERA5-D\n",
    "    \n",
    "    # --- Colormap ---\n",
    "    beige_to_red = LinearSegmentedColormap.from_list(\n",
    "        \"beige_to_red\",\n",
    "        [\"#fbeec2\", \"#d47a57\", \"#8b0000\"]   # beige â†’ soft red â†’ dark red\n",
    "    )\n",
    "\n",
    "    ZERO_PRECIP_STYLE      = Style(cmap=beige_to_red, vmin=1,  vmax=12,  normalize=False)\n",
    "    BLACK_POINT_STYLE      = Style(cmap=ListedColormap([\"black\"]), vmin=0, vmax=0.99, normalize=False)\n",
    "    ZERO_PRECIP_DIFF_STYLE = Style(cmap=\"RdBu_r\", vmin=-12, vmax=12, normalize=False)\n",
    "\n",
    "    # ------------------ LEFT PANEL ------------------\n",
    "    non_ones1 = da1.where(da1 != 0)   # all values except 0\n",
    "    ones1     = da1.where(da1 == 0)   # only values equal to 0\n",
    "\n",
    "    # ------------------ MIDDLE PANEL ------------------\n",
    "    non_ones2 = da2.where(da2 != 0) \n",
    "    ones2     = da2.where(da2 == 0)\n",
    "\n",
    "    # ------------------ RIGHT PANEL (DIFFERENCE) ------------------\n",
    "    diff = da1 - da2\n",
    "\n",
    "    # Create figure with 3 columns\n",
    "    fig = ekp.Figure(rows=1, columns=3)\n",
    "\n",
    "    # ---------- Subplot 1 ----------\n",
    "    subplot1 = fig.add_map(domain=domain, row=0, column=0)\n",
    "    mappable1 = subplot1.grid_cells(non_ones1, style=ZERO_PRECIP_STYLE) # Mappable \n",
    "    subplot1.grid_cells(ones1, style=BLACK_POINT_STYLE, zorder=101)\n",
    "    ax1 = subplot1.ax\n",
    "    ax1.set_title(f\"Reproduced ({accumulation_window}â€‘Month Window)\")\n",
    "    \n",
    "    # ---------- Subplot 2 ----------\n",
    "    subplot2 = fig.add_map(domain=domain, row=0, column=1)\n",
    "    mappable2 = subplot2.grid_cells(non_ones2, style=ZERO_PRECIP_STYLE)\n",
    "    subplot2.grid_cells(ones2, style=BLACK_POINT_STYLE, zorder=101)\n",
    "    ax2 = subplot2.ax\n",
    "    ax2.set_title(f\"ERA5 ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "    # ---------- Subplot 3 (Difference) ----------\n",
    "    subplot3 = fig.add_map(domain=domain, row=0, column=2)\n",
    "    mappable3 = subplot3.grid_cells(diff, style=ZERO_PRECIP_DIFF_STYLE)\n",
    "    ax3 = subplot3.ax\n",
    "    ax3.set_title(f\"Reproduced âˆ’ ERA5 Difference ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "    # ------------------ SHARED FIGURE SETTINGS ------------------\n",
    "    fig_mpl = ax1.figure\n",
    "    fig_mpl.set_size_inches(18, 6)\n",
    "    \n",
    "    # --- DISCRETE for panels 1 & 2: integers 1..12 ---\n",
    "    bounds12 = np.arange(0.5, 12.5 + 1, 1.0)     # 0.5, 1.5, ..., 12.5  (12 bins)\n",
    "    norm12   = BoundaryNorm(bounds12, ncolors=beige_to_red.N)\n",
    "    \n",
    "    mappabl1.set_cmap(beige_to_red)\n",
    "    mappable1.set_norm(norm12)\n",
    "    \n",
    "    # Ensure subplot 2 uses the exact same mapping as subplot 1\n",
    "    mappable2.set_cmap(beige_to_red)\n",
    "    mappable2.set_norm(norm12)\n",
    "    \n",
    "    # Shared colorbar for the first two panels â€” use the mappable from panel 1\n",
    "    cbar12 = fig_mpl.colorbar(\n",
    "        mappable1,\n",
    "        ax=[ax1, ax2],\n",
    "        orientation=\"horizontal\",\n",
    "        location=\"bottom\",    # remove this line if your Matplotlib is older than 3.6\n",
    "        fraction=0.06,\n",
    "        pad=0.08,\n",
    "        ticks=np.arange(1, 13),\n",
    "        boundaries=bounds12,  # ensures discrete steps\n",
    "        spacing='uniform'     # equal-sized color blocks\n",
    "    )\n",
    "    \n",
    "    cbar12.set_label(\"Number of months (1â€“12)\")\n",
    "    \n",
    "    # --- DISCRETE for panel 3 (difference): integers -12..12 ---\n",
    "    bounds_diff = np.arange(-12.5, 12.5 + 1, 1.0)  # -12.5, -11.5, ..., 12.5  (25 bins)\n",
    "    \n",
    "    mappable3.set_norm(BoundaryNorm(bounds_diff, ncolors=mappable3.get_cmap().N))\n",
    "    \n",
    "    # Separate colorbar for the difference panel (now discrete)\n",
    "    cbar_diff = fig_mpl.colorbar(\n",
    "        mappable3,\n",
    "        ax=ax3,\n",
    "        orientation=\"horizontal\",\n",
    "        location=\"bottom\",     # remove if older Matplotlib\n",
    "        fraction=0.06,\n",
    "        pad=0.08,\n",
    "        ticks=np.arange(-12, 13, 4),\n",
    "        boundaries=bounds_diff,\n",
    "        spacing='uniform'\n",
    "    )\n",
    "    \n",
    "    cbar_diff.set_label(\"Difference (months)\")\n",
    "    \n",
    "    decorate_fig(fig, title=f\"{fig_title}\")\n",
    "\n",
    "    return fig_mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "(section-cds_setup)=\n",
    "### 2. CDS download setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "In this assessment, we will calculate SPI and SPEI for each month (with different accumulation periods, see below) for the years 1940â€“2024. For the reference period, we will use the World Meteorological Organization (WMO) current standard 30-year reference period of 1991â€“2020, which is also used in ERA5-Drought. Both of these date ranges can be adjusted in the cell below when running the analysis yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preferred analysis and reference periods\n",
    "years           = (1940, 2024)  # Years for the analysis (inclusive)\n",
    "years_reference = (1991, 2020)  # Years for the reference period (inclusive)\n",
    "\n",
    "# Derived variables for convenience:\n",
    "reference_window = {\"valid_time\": slice(f\"{years_reference[0]}-01-01\", f\"{years_reference[1]}-12-01\"),}  #  Slice (1991-01-01, 2020-12-01)\n",
    "entire_window    = {\"valid_time\": slice(f\"{years[0]}-01-01\",           f\"{years[1]}-12-01\"),}            #  Slice (1940-01-01, 2024-12-01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Having defined our target years, we can now define our CDS request.\n",
    "First, we define a template with some default parameters\n",
    "(e.g. years, data format)\n",
    "that will also be used later in the notebook.\n",
    "Additional information for specific downloads\n",
    "(e.g. variable, data stream)\n",
    "is mixed into this template where relevant.\n",
    "\n",
    "This notebook uses [earthkit-data](https://github.com/ecmwf/earthkit-data) to download files from the CDS.\n",
    "If you intend to run this notebook multiple times, it is highly recommended that you [enable caching](https://earthkit-data.readthedocs.io/en/latest/guide/caching.html) to prevent having to download the same files multiple times.\n",
    "If you prefer not to use earthkit, the following requests can also be used with the [cdsapi module](https://cds.climate.copernicus.eu/how-to-api#linux-use-client-step).\n",
    "In either case (earthkit-data or cdsapi), it is required to set up a CDS account and API key as explained [on the CDS website](https://cds.climate.copernicus.eu/how-to-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_ERA5 = \"reanalysis-era5-complete\"\n",
    "\n",
    "request_era5_template = {\n",
    "    \"class\": \"ea\",            # Default for ERA5\n",
    "    # Dates: ERA5 takes these in the format 19400101/19400201/.../20241101/20241201\n",
    "    # The following line generates a string in said format from the chosen year range\n",
    "    \"date\": \"/\".join(f\"{year}{month:02}01\"\n",
    "            for year in range(years[0] ,years[1]+1)\n",
    "            for month in MONTHS),\n",
    "    \"expver\": \"1\",            # ERA5 consolidated data\n",
    "    \"levtype\": \"sfc\",         # Surface\n",
    "    \"grid\": \"0.25/0.25\",      # Grid: 0.25Â° by 0.25Â°\n",
    "    \"type\": \"fc\",             # Forecast\n",
    "    \"data_format\": \"netcdf\",  # NetCDF data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5-Drought dataset name.\n",
    "\n",
    "ID_ERA5_DROUGHT = \"derived-drought-historical-monthly\"\n",
    "\n",
    "request_era5_drought_template = {\n",
    "    \"version\": \"1_0\",\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "We also want to download a land-sea mask, since values over the ocean from the origin dataset are not masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_mask_req = {\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"variable\": \"land_sea_mask\",\n",
    "    \"date\": \"2000-01-01\",\n",
    "    \"time\": \"00:00\",\n",
    "    \"format\": \"netcdf\",\n",
    "    \"grid\": \"0.25/0.25\",\n",
    "}\n",
    "\n",
    "ls_mask = ekd.from_source(\"cds\", \"reanalysis-era5-single-levels\", ls_mask_req)\n",
    "ls_mask = ls_mask.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "ls_mask =  ls_mask.assign_coords(longitude=((ls_mask.longitude - 180) % 360) - 180)\n",
    "ls_mask = ls_mask.sortby(ls_mask.longitude).squeeze()\n",
    "ls_mask = (ls_mask > 0.5) # Land considered to be full\n",
    "# For more information https://confluence.ecmwf.int/display/FUG/Section+2.1.3.1+Land-Sea+Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "And we want a list of tuples that allow us to classify drought severity based its value, both for SPI and SPEI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_categories = [ # Color-scheme roughly corresponds to GDO scheme.\n",
    "    (-100, -2.0, \"Extremely dry\",  \"#ff0000\"),  # red\n",
    "    (-1.99, -1.5, \"Severely dry\",  \"#ffa500\"),  # orange\n",
    "    (-1.49, -1.0, \"Moderately dry\", \"#ffff00\"), # yellow\n",
    "    (-0.99,  0.0, \"Nearâ€‘normal / mildly dry\",  \"#ffffff\"), # white\n",
    "    (0.0,   0.99, \"Nearâ€‘normal / mildly wet\",  \"#e6d5f7\"), # light lavender\n",
    "    (1.0,   1.49, \"Moderately wet\", \"#c27bd2\"), # medium purple\n",
    "    (1.50,  1.99, \"Severely wet\",  \"#8b008b\"),  # dark purple\n",
    "    (2.0, 100, \"Extremely wet\", \"#4b0055\"),  # deepest purple \n",
    "]\n",
    "\n",
    "spei_categories = [\n",
    "    (-100,  -2.33, \"Extremely dry\",  \"#6E0000\"),  # dark red\n",
    "    (-2.32, -1.65, \"Severely dry\",   \"#C22E1A\"),  # red\n",
    "    (-1.64, -1.28, \"Moderately dry\", \"#E95B26\"),  # orange\n",
    "    (-1.27, -0.84, \"Mildly dry\",     \"#F9C62F\"),  # yellow\n",
    "    (-0.83,  0.83, \"Near-normal\",    \"#B9F376\"),  # light green\n",
    "    (0.84,   1.27, \"Mildly wet\",     \"#3CF5E8\"),  # cyan\n",
    "    (1.28,   1.64, \"Moderately wet\", \"#33A2EA\"),  # medium blue\n",
    "    (1.65,   2.32, \"Severely wet\",   \"#1C4DC9\"),  # strong blue\n",
    "    (2.33,   100,  \"Extremely wet\",  \"#08146A\"),  # very dark navy\n",
    "]\n",
    "\n",
    "# # Define your preferred sites for the SPI example - you can choose your own.\n",
    "sites_dict = {\n",
    "    \"Addis Ababa, Ethiopia\": {\"lat\": 9.00, \"lon\": 38.75},\n",
    "    \"Teddington, United Kingdom\": {\"lat\": 51.50, \"lon\": -0.25},\n",
    "    \"Denver, USA\": {\"lat\": 39.75, \"lon\": -105.00}\n",
    "}\n",
    "\n",
    "lats = [v[\"lat\"] for v in sites_dict.values()]\n",
    "lons = [v[\"lon\"] for v in sites_dict.values()]\n",
    "\n",
    "area_dict = {\n",
    "    name: box_around(meta[\"lat\"], meta[\"lon\"])\n",
    "    for name, meta in sites_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-spi)=\n",
    "### 3. Calculate SPI from ERA5 reanalysis data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### 3.0 Download monthly precipitation data and land-sea mask.\n",
    "First, the monthly-mean total precipitation data from the ERA5 reanalysis is downloaded. Generally, one would use the [_ERA5 monthly averaged data on single levels from 1940 to present_ (reanalysis-era5-single-levels-monthly-means)](https://doi.org/10.24381/cds.f17050d7) dataset for this, which provides pre-calculated monthly means at 0.25Â° by 0.25Â° resolution.\n",
    "\n",
    "For this assessment, to be as close to the ERA5-Drought data processing pipeline as possible\n",
    "and to make use of some of MARS's functionalities (see [below](section-4)),\n",
    "we instead use the [_Complete ERA5 global atmospheric reanalysis_ (reanalysis-era5-complete)](https://doi.org/10.24381/cds.143582cf) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "We want to download \n",
    "total precipitation data (variable `228.128`)\n",
    "from the\n",
    "`moda` stream (monthly-mean reanalysis data),\n",
    "so we mix this information into the template\n",
    "and submit the request to the CDS.\n",
    "More information about formatting these requests is available in the [MARS ERA5 catalogue](https://apps.ecmwf.int/data-catalogues/era5/?class=ea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_precipitation_moda = {\n",
    "    \"param\": \"228.128\",       # Variable: Total precipitation\n",
    "    \"stream\": \"moda\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and convert to desired format\n",
    "era5_monthly_mean_reanal = ekd.from_source(\"cds\", ID_ERA5, request_era5_precipitation_moda)  # Download as field list\n",
    "era5_monthly_mean_reanal = era5_monthly_mean_reanal.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_monthly_mean_reanal =  era5_monthly_mean_reanal.assign_coords(longitude=((era5_monthly_mean_reanal.longitude - 180) % 360) - 180)\n",
    "era5_monthly_mean_reanal = era5_monthly_mean_reanal.sortby(era5_monthly_mean_reanal.longitude)\n",
    "era5_monthly_mean_reanal = rechunk(era5_monthly_mean_reanal)  # Re-chunk for speed gain in fitting\n",
    "era5_monthly_mean_reanal  # Display in notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### 3.1 Calculate accumulated precipitation for different windows.\n",
    "A time series of precipitation for one grid point is extracted and precipitation is accumulated over the previous $n$ months using a moving window. Since the origin precipitation data from ERA5 is from the monthly mean dataset, we calculate the total precipitation for that month by multiplying with the total number of days in that month, correcting for leap years.\n",
    "\n",
    "The different accumulation windows are used to determine the timescale of the drought. The longer the drought, typically the more severe the impact it will have.\n",
    "\n",
    "* **1-, 3-month window**: useful for soil moisture, flow in small creaks.\n",
    "* **6-, 12-month window**: looking at reservoir storage, reduced stream flow.\n",
    "* **24-, 36-, 48-month window**: groundwater recharge, reduced reservoir. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "TO DO: Some text about the user choosing their point here, which point did we choose and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_example_sites = era5_monthly_mean_reanal.sel(latitude=xr.DataArray(lats, dims=\"sites\"),\n",
    "                     longitude=xr.DataArray(lons, dims=\"sites\"))\n",
    "\n",
    "acc_precip_site = accum_var(precipitation_example_sites, var = \"tp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot accumulated precipitation at grid point within domain (here Addis Ababa).\n",
    "fig, ax = plot_accumulation(\"tp\", acc_precip_site.sel(sites=0), sites_dict[\"Addis Ababa, Ethiopia\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.2 Fit gamma distribution (over calendar months) to different accumulation periods.\n",
    "\n",
    "The gamma distribution [reference] is fitted only to the data within the reference period (1991-2020). A separate distribution is fitted for each calendar month per accumulation window. \n",
    "\n",
    "For e.g. : in the 3-month accumulation window, a gamma distribution is fitted on all 30 Januaries in that reference period, all the Februaries (30 of them) and so forth... \n",
    "\n",
    "This fitting is done with the [scipy.stats.gamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html) object in Python. Three parameters are then outputted per calendar month, per accumulation period: the shape ($\\alpha$), location ($\\beta$) and scale ($\\lambda$) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_precip_site_reference = acc_precip_site.sel(**reference_window) # select only reference precipitation window.\n",
    "params_fitted = fit_monthly_spi(acc_precip_site_reference) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### 3.3 Compute SPI series\n",
    "\n",
    "From the parameters from the gamma distribution fitting, we calculate the cumulative distribution function or CDF, $F(x, \\alpha, \\beta, \\lambda)$, up to the accumulated precipitation $x$, from the probability distribution function or PDF, $f(u, \\alpha, \\beta, \\lambda)$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F(x, \\alpha, \\beta, \\lambda) = \\int_{0}^{x}f(u, \\alpha, \\beta, \\lambda) \\, du.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "A one-to-one mapping of SPI-index to accumulated precipitation value is then obtained by transforming the cumulative probability values to a standard normal distribution with a mean ($\\mu$) of zero and standard deviation ($\\sigma$) of 1. This mapping is applied to the historical record of the accumulated precipitation values in that calendar month and accumulation window.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{SPI-index} = \\Phi^{-1}\\big(F(x, \\alpha, \\beta, \\lambda)\\big)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CDF time-series\n",
    "cdf_ds = compute_monthly_spi(acc_precip_site, params_fitted) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.4 Calculating historical ratio of months without precipitation and finding zero adjusted SPI (ref dataset)\n",
    "In regions of extremely low precipitation (e.g. the Sahara desert), months may have little to no accumulated precipitation. This poses a problem when fitting the gamma distribution since it is defined only for positive, real values. Furthermore, a criteria is set so that out of the 30 months in the reference period, per calendar month & accumulation window, 10 months must have non-zero accumulated precipitations. Otherwise, the calendar month is reported as having no value.\n",
    "\n",
    "Months with zero precipitation are defined as less than 0.1 mm by the [Copernicus European (EDO) and Global (GDO) Drought Observatories](https://drought.emergency.copernicus.eu/). The ERA5-Drought dataset appears to define months with zero precipitation as having an accumulated precipitation exactly equal to zero. \n",
    "\n",
    "To get around some months having zero precipitation, the CDF is adjusted for the number of months with zero precipitation,  $n_{0}$  [Stagge et al., â€˜Candidate Distributions for Climatological Drought Indices](https://doi.org/10.1002/joc.4267).\n",
    "$$\n",
    "\\begin{equation}\n",
    "F_{p_{0}}(x_{p>0},  \\alpha, \\beta, \\lambda) = p_{0} + \\big(1 - p_{0}\\big) \\, F(x_{p>0},  \\alpha, \\beta, \\lambda),\n",
    "\\end{equation}\n",
    "$$\n",
    "where $F_{p_{0}}(x_{p>0},  \\alpha, \\beta, \\lambda)$ is the CDF adjusted for zero precipitation:\n",
    "\n",
    "$$\\begin{equation}\n",
    "p_{0} = \\frac{n_{0}}{n + 1} ,\n",
    "\\end{equation}$$\n",
    "\n",
    "and $p_{0}$ is the historical occurence of months with zero precipitation. This simple treatment can lead to a z-distribution that is skewed, with a non-zero mean, however. Therefore, special care must be taken, and $p_{0}$ must be adjusted for the \"centre of probability mass\":\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\bar{p}_{0} = \\frac{n_{0} + 1}{2(n + 1)},\n",
    "\\end{equation}$$\n",
    "\n",
    "where $n_{0}$ are the number of calendar months in the reference period per accumulation window, and $n$ are the total number of calendar months (30 reference calendar months):\n",
    "\n",
    "$$\\begin{equation}\n",
    "F_{\\bar{p}_{0}}(x, \\alpha, \\beta, \\lambda) =\n",
    "\\begin{cases}\n",
    "\\bar{p}_{0} + (1 - \\bar{p}_{0}) \\, F(x_{p>0},  \\alpha, \\beta, \\lambda), & x > 0, \\\\[6pt]\n",
    "\\frac{n_{p=0} + 1}{2(n+1)}, & x = 0.\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "This maintains the mean SPI value of zero, allowing for an objective statistical comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_cdf_ds = zero_precip_adjustment(acc_precip_site, cdf_ds, reference_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "The CDF adjusted for months of zero precipitation is once again transformed to the z-normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Adjusted SPI-index} = \\Phi^{-1}\\big(F_{\\bar{p}_{0}}(x,  \\alpha, \\beta, \\lambda)\\big)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_spi_ds = cdf_to_znorm_transform(adjusted_cdf_ds, \"SPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "#### 3.5 Importing ERA5-Drought SPI data from dataset for comparison (Addis Ababa, Ethiopia)\n",
    "Data request must be \"weaved\" as request too big for entire time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "addis_req1 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [str(p) for p in ACCUMULATION_PERIODS],\n",
    "    \"area\": area_dict[\"Addis Ababa, Ethiopia\"],\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"year\": [str(y) for y in range(1940, 1981)],\n",
    "} | request_era5_drought_template\n",
    "\n",
    "addis_req2 = {\n",
    "    **addis_req1,\n",
    "    \"year\": [str(y) for y in range(1981, 2025)],\n",
    "}\n",
    "\n",
    "addis_src = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, addis_req1, addis_req2)\n",
    "addis_ds = addis_src.to_xarray(compat=\"equals\")\n",
    "addis_point = addis_ds.sel(sites_dict[\"Addis Ababa, Ethiopia\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "#### 3.6 Comparison of the ERA5-Drought vs Reproduced SPI-index (Addis Ababa, Ethiopia)\n",
    "Now that we have calculated the SPI-index for one grid point, for all accumulation windows, we make a qualatitive and quantitative comparison with the corresponding data in the ERA5-Drought dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_index_comparison_side_by_side(\"SPI\", addis_point, adjusted_spi_ds.sel(sites=0),spi_categories) # takes a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: SPI for location in Addis Ababa shows excellent agreement across entire time series. Showed that our methodology is consistent with that of ERA5 and reproduceable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Median and absolute median difference for each accumulation window across months are then calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_results_diff_addis = comparison_monthly_statistics(addis_point, adjusted_spi_ds.sel(sites=0), \"SPI\", ACCUMULATION_PERIODS) # takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_monthly_statistics(spi_results_diff_addis, \"median\", \"Median Difference - Addis Ababa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_monthly_statistics(spi_results_diff_addis, \"abs_median\", \"Absolute Median Difference - Addis Ababa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: SPI is only discrepant for certain months (but that this discrepancy is still quite small)- since fitting is by month only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "#### 3.7 Comparison of drought severity classification (Addis Ababa, Ethiopia)\n",
    "A confusion matrix is generated to compare drought classifications for the single, above location, between the ERA5â€‘Drought dataset and the reproduced dataset, capturing any discrepancies. The matrix is computed over the full time period (1940â€“2024), where each count corresponds to a single monthly timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_confusion_matrices(adjusted_spi_ds.sel(sites=0),addis_point,\"SPI\",spi_categories, \"Addis Ababa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: and that in fact, the indices fall under the same drought severity classification for all timestamps at Addis Ababa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "#### 3.8 Calculating ERA5-Drought SPI (across entire Horn of Africa region)\n",
    "The SPI-index is then calculated for a larger region (Horn of Africa) to compare the ERA5-Drought and Reproduced index spatially, for a singular timestamp, not only temporally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select latitude and longitude box \n",
    "lon_min, lon_max = 30, 53  \n",
    "lat_min, lat_max = -5, 18\n",
    "\n",
    "request_era5_drought_region_template = {\n",
    "    # The following line generates a string in said format from the chosen year range\n",
    "    \"accumulation_period\": [\"48\"], # Chosen accumulation period\n",
    "    \"version\": \"1_0\", # Version\n",
    "    \"dataset_type\": \"consolidated_dataset\", # Consolidated dataset\n",
    "    \"month\": [f\"{m:02d}\" for m in range(1, 13)], # Months of interest\n",
    "    \"area\": [lat_max, lon_min, lat_min, lon_max] # Area of interest\n",
    "}\n",
    "\n",
    "spi_region_request1 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"], # drought index\n",
    "    \"year\": [f\"{y}\" for y in range(1940,1981)],\n",
    "    \"product_type\": [\"reanalysis\"]\n",
    "} | request_era5_drought_region_template\n",
    "\n",
    "spi_region_request2 = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"], # drought index\n",
    "    \"year\": [f\"{y}\" for y in range(1981,2025)],\n",
    "    \"product_type\": [\"reanalysis\"]\n",
    "} | request_era5_drought_region_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_region_spi = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, spi_region_request1, spi_region_request2) # Sends request for this dataset to CDS.\n",
    "era5_region_spi = era5_region_spi.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset (latitude is descending in ERA5) - should this be wrapped in a function now?\n",
    "ds_loc = era5_monthly_mean_reanal.sel(\n",
    "    latitude=slice(lat_max, lat_min),\n",
    "    longitude=slice(lon_min, lon_max),\n",
    ")\n",
    "\n",
    "# Accumulate precipitations\n",
    "precipitation_example_region = accum_var(ds_loc, var = \"tp\") \n",
    "\n",
    "# Select reference precipitation window.\n",
    "precipitation_example_region_reference = precipitation_example_region.sel(**reference_window) \n",
    "\n",
    "# Fit distribution to reference window, extract parameters.\n",
    "params_fitted_region = fit_monthly_spi(precipitation_example_region_reference)\n",
    "\n",
    "# Calculate CDF + SPI with parameters.\n",
    "cdf_region = compute_monthly_spi(precipitation_example_region, params_fitted_region)\n",
    "\n",
    "# Adjust CDF zero precipitation.\n",
    "adjusted_cdf_region = zero_precip_adjustment(precipitation_example_region, cdf_region, reference_window)\n",
    "\n",
    "# Transform adjusted CDF back to SPI\n",
    "adjusted_spi_region = cdf_to_znorm_transform(adjusted_cdf_region, \"SPI\")\n",
    "\n",
    "adjusted_spi_ds_region = adjusted_spi_region.rename({\n",
    "    \"valid_time\": \"time\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\"})\n",
    "\n",
    "era5_spi, adjusted_spi_ds = xr.align(era5_region_spi, adjusted_spi_ds_region, join=\"left\")   # only overlapping coords\n",
    "\n",
    "# # Apply land-sea mask over region. \n",
    "# adjusted_spi_ds_region = adjusted_spi_ds_region.where(ls_mask[\"lsm\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechunk for optimisation\n",
    "adjusted_spi_ds_region = adjusted_spi_ds_region.chunk({\n",
    "    'time': 1,\n",
    "    'lat': 93,  # or 'lat': 93\n",
    "    'lon': 93  # or 'lon': 93\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_interest = \"2024-12-01\"\n",
    "\n",
    "# Select one time slice (optional- also mean difference). Best to run overnight.\n",
    "map_1 = era5_spi[\"SPI48\"].astype(\"float32\").sel(time = time_of_interest).persist()\n",
    "map_2 = adjusted_spi_ds_region[\"SPI48\"].astype(\"float32\").sel(time = time_of_interest).persist()\n",
    "\n",
    "spi_region_diff = abs(map_1 - map_2)\n",
    "\n",
    "horn = ekp.geo.domains.Domain(\n",
    "    bbox=[lon_min, lon_max, lat_min, lat_max],\n",
    "    name=\"Horn of Africa\",\n",
    ")\n",
    "\n",
    "# # Plot with EarthKit\n",
    "SPI_STYLE = Style(cmap='RdBu_r', vmin = -4, vmax = 4, normalize=False)\n",
    "SPI_DIFF_STYLE = Style(cmap='viridis', vmin = 0, vmax = 1, normalize=False)\n",
    "\n",
    "# Create figure with 2 columns\n",
    "fig = ekp.Figure(rows=1, columns=3, size=(12, 6))  \n",
    "\n",
    "# First subplot (left)\n",
    "subplot1 = fig.add_map(domain=horn, row=0, column=0)\n",
    "subplot1.grid_cells(map_1,style=SPI_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "subplot1.title(f\"ERA5-Drought SPI-48 at {time_of_interest}\")\n",
    "\n",
    "# Second subplot (left)\n",
    "subplot2 = fig.add_map(domain=horn, row=0, column=1)\n",
    "subplot2.grid_cells(map_2,style=SPI_STYLE)\n",
    "subplot2.legend(location=\"right\")\n",
    "subplot2.title(f\"Reproduced SPI-48 at {time_of_interest}\")\n",
    "\n",
    "# First subplot (left)\n",
    "subplot3 = fig.add_map(domain=horn, row=0, column=2)\n",
    "subplot3.grid_cells(spi_region_diff, style=SPI_DIFF_STYLE)\n",
    "subplot3.legend(location=\"right\")\n",
    "subplot3.title(\"Absolute difference, ERA5-Drought - Reproduced, SPI-48\")\n",
    "\n",
    "# Add decorations\n",
    "decorate_fig(fig, title = \"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the above calculation for a single location is now performed around that location so we make a spatial comparison. This is to ensure that the calculated SPI is not just by coincidence at that location in agreement with ERA5-Drought but in fact reproducable around it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "####  3.9 Comparison of \"Probability of Zero Precipitation\"\n",
    "The \"probability of zero precipitation\" for a given calendar month, in the reference period, per accumulation period may also be imported from the ERA5-Drought dataset, instead of calculating it as we have done. \n",
    "\n",
    "You are unable to send an API request to download the data for all accumulation periods at once. If you try to do so, you will find that your xarray only contains the data for the last accumulation period. \n",
    "\n",
    "We have written a simple helper function, \"era5_api_multiple\" that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_d_prob_spi_zero_all = era5_drought_api_multiple(indicator = \"spi\", var = \"prob_zero\", product_type = \"reanalysis\")\n",
    "\n",
    "# Rename coordinates to match that of reproduced.\n",
    "era5_d_prob_spi_zero_all = era5_d_prob_spi_zero_all.rename({\"lat\": \"latitude\", \"lon\":\"longitude\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "Months with defined \"zero precipitation\" are then calculated from the origin dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate precipitation globally.\n",
    "acc_precip_global = accum_var(era5_monthly_mean_reanal, var = \"tp\")\n",
    "\n",
    "# Find probability of zero precipitation across months.\n",
    "p_zero, weighted_p_zero = zero_precip_prob(acc_precip_global, reference_window)\n",
    "    \n",
    "# Find months that fail zero-precipitation quality criteria (> 0.33) in reproduced dataset.\n",
    "zero_rejection, zero_rejection_agg = zero_precip_rejection(p_zero, ls_mask)\n",
    "\n",
    "# Find months that fail zero-precipitation quality criteria in reproduced dataset.\n",
    "era5_zero_rejection, era5_zero_rejection_agg = zero_precip_rejection(era5_d_prob_spi_zero_all, ls_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "And a global spatial comparison of the number of zero-precipitation calendar months is made against ERA5-Drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_window = [1]\n",
    "\n",
    "fig_mpl = quality_flag_spatial_comparison(zero_rejection_agg,era5_zero_rejection_agg,\n",
    "                                          accumulation_window, \"global\", \"Zero Precipitation\") # Zero preecipitation\n",
    "fig_mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "We finally show that \"probability of zero precipitation months\" match for all calendar months at all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "x = np.ravel(p_zero[\"tp_mm_accum_1m\"].where(ls_mask[\"lsm\"]))\n",
    "y = np.ravel(era5_d_prob_spi_zero_all[\"prob_zero_1\"].where(ls_mask[\"lsm\"]))\n",
    "\n",
    "p_threshold = 0.33 \n",
    "\n",
    "x_count = np.count_nonzero(~np.isnan(x))\n",
    "y_count = np.count_nonzero(~np.isnan(y))\n",
    "\n",
    "reject_x_count = (x > p_threshold).sum()\n",
    "reject_y_count = (y > p_threshold).sum()\n",
    "\n",
    "percent_reject_x = (reject_x_count/x_count)*100\n",
    "percent_reject_y = (reject_y_count/y_count)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "hb = plt.scatter(\n",
    "    x, y,   \n",
    ")\n",
    "\n",
    "plt.hlines(y=p_threshold, xmin=0, xmax=p_threshold, colors='g', linestyles='--', linewidth=2)\n",
    "plt.vlines(x=p_threshold, ymin=0, ymax=p_threshold, colors='g', linestyles='--', linewidth=2)\n",
    "plt.fill_between([0,p_threshold], 0, p_threshold, facecolor=\"none\", hatch=\"X\", edgecolor=\"green\", linewidth=0.0, label='Acceptance criteria')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Reproduced')\n",
    "plt.ylabel('ERA5-Drought')\n",
    "plt.title('Comparison of Probability of Zero Precipitation')\n",
    "plt.annotate(f\"Reproduced rejection freq: {percent_reject_x:.5g}%\\n ERA5D rejection freq:{percent_reject_y:.5g}%\", xy=(0.5, 0.2))\n",
    "plt.legend()\n",
    "\n",
    "# Display plot\n",
    "plt.show() \n",
    "# TODO: Calculate percentage that are the same, ignoring the NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the zero precipitation quality flag is in fact reproduceable, and with the above, different thresholds can be set by the user (depending on how strict the threshold is). This gives us faith too that the zero precipitation adjustment has been made correctly (pending formula)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "#### 3.10 Comparison of S-W quality indicator.\n",
    "\n",
    "Quality control is performed by the ERA5-Drought team over the entire dataset. This is done by testing if the calculated distribution of the estimated drought indices over the reference period follows a normal distribution with mean 0 and standard deviation 1. \n",
    "\n",
    "This test is performed using the Shapiro-Wilks test for normality [S. S. Shapiro et al, 1965.](https://doi.org/10.1093/biomet/52.3-4.591), with a $\\alpha$ = 0.05 on the data in the reference period (1991-2020). \n",
    "\n",
    "If the resultant p-value is less than $\\alpha$ = 0.05, the corresponding quality parameter is set to 0 (bad), otherwise set to 1 (good). We perform this test over the calculated SPI values, that are adjusted for zero-precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select latitude and longitude box (40 x 40 here)\n",
    "dom_lon_min, dom_lon_max = 30, 53  \n",
    "dom_lat_min, dom_lat_max = -5, 18\n",
    "\n",
    "lon_min, lon_max = 52, 53  \n",
    "lat_min, lat_max = 17, 18\n",
    "\n",
    "# Subset (latitude is descending in ERA5) - should this be wrapped in a function now?\n",
    "ds_loc = era5_monthly_mean_reanal.sel(\n",
    "    latitude=slice(lat_max, lat_min),\n",
    "    longitude=slice(lon_min, lon_max))\n",
    "\n",
    "# Accumulate precipitations\n",
    "precipitation_example_global = accum_var(ds_loc, var = \"tp\") \n",
    "\n",
    "# Select reference precipitation window.\n",
    "precipitation_example_global_reference = precipitation_example_global.sel(**reference_window) \n",
    "\n",
    "# Fit distribution to reference window, extract parameters.\n",
    "params_fitted_global = fit_monthly_spi(precipitation_example_global_reference)\n",
    "\n",
    "# Calculate CDF  with parameters.\n",
    "cdf_global = compute_monthly_spi(precipitation_example_global, params_fitted_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust CDF zero precipitation.\n",
    "adjusted_cdf_global = zero_precip_adjustment(precipitation_example_global, cdf_global, reference_window)\n",
    "\n",
    "# Transform adjusted CDF back to SPI\n",
    "spi_global = cdf_to_znorm_transform(cdf_global, \"SPI\")\n",
    "adjusted_spi_global = cdf_to_znorm_transform(adjusted_cdf_global, \"SPI\")\n",
    "\n",
    "# Apply land-sea mask over region. \n",
    "# adjusted_spi_ds_global = adjusted_spi_global.where(ls_mask[\"lsm\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_spi_global, calc_quality_spi_global = xr_shapiro_test(spi_global, reference_window, index_name = \"SPI\")\n",
    "calc_quality_spi_agg_global = calc_quality_spi_global.sum(dim = \"time\") # Aggregate across months.\n",
    "\n",
    "era5_quality_spi_global = era5_drought_api_multiple(indicator = \"spi\", var = \"quality\", product_type = \"reanalysis\")\n",
    "era5_quality_spi_global_agg = quality_spi_all.sum(dim=\"time\")\n",
    "\n",
    "accumulation_window = 1\n",
    "\n",
    "# --- Colormap as you already have ---\n",
    "beige_to_red = LinearSegmentedColormap.from_list(\n",
    "    \"beige_to_red\",\n",
    "    [\"#fbeec2\", \"#d47a57\", \"#8b0000\"]   # beige â†’ soft red â†’ dark red\n",
    ")\n",
    "\n",
    "ZERO_PRECIP_STYLE      = Style(cmap=beige_to_red, vmin=1,  vmax=12,  normalize=False)\n",
    "BLACK_POINT_STYLE      = Style(cmap=ListedColormap([\"black\"]), vmin=0, vmax=0.99, normalize=False)\n",
    "ZERO_PRECIP_DIFF_STYLE = Style(cmap=\"RdBu_r\", vmin=-12, vmax=12, normalize=False)\n",
    "\n",
    "# ------------------ LEFT PANEL ------------------\n",
    "da1 = calc_quality_spi_agg_global[f\"significance_{accumulation_window}\"] # Reproduced\n",
    "non_ones1 = da1.where(da1 != 0)   # all values except 0\n",
    "ones1     = da1.where(da1 == 0)   # only values equal to 0\n",
    "\n",
    "# ------------------ MIDDLE PANEL ------------------\n",
    "da2 = era5_quality_spi_global_agg[f\"significance_{accumulation_window}\"].sel(\n",
    "    lat=slice(lat_max, lat_min),\n",
    "    lon=slice(lon_min, lon_max)) # ERA5-D\n",
    "\n",
    "non_ones2 = da2.where(da2 != 0)\n",
    "ones2     = da2.where(da2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "horn = ekp.geo.domains.Domain(\n",
    "    bbox=[dom_lon_min, dom_lon_max, dom_lat_min, dom_lat_max],\n",
    "    name=\"Horn of Africa\",\n",
    ")\n",
    "\n",
    "# Create figure with 3 columns\n",
    "fig = ekp.Figure(rows=1, columns=2)\n",
    "\n",
    "# ---------- Subplot 1 ----------\n",
    "subplot1 = fig.add_map(domain=horn, row=0, column=0)\n",
    "mappable1 = subplot1.grid_cells(non_ones1.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot1.grid_cells(ones1.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax1 = subplot1.ax\n",
    "ax1.set_title(f\"Reproduced â€” # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ---------- Subplot 2 ----------\n",
    "subplot2 = fig.add_map(domain=horn, row=0, column=1)\n",
    "mappable2 = subplot2.grid_cells(non_ones2.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot2.grid_cells(ones2.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax2 = subplot2.ax\n",
    "ax2.set_title(f\"ERA5 â€”  # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ------------------ SHARED FIGURE SETTINGS ------------------\n",
    "# Set figure size (wide)\n",
    "fig_mpl = ax1.figure\n",
    "fig_mpl.set_size_inches(18, 6)\n",
    "\n",
    "# Shared colorbar for the first two panels â€” use the mappable from panel 1\n",
    "cbar12 = fig_mpl.colorbar(\n",
    "    mappable1,\n",
    "    ax=[ax1, ax2],\n",
    "    orientation=\"horizontal\",\n",
    "    location=\"bottom\",    # remove this line if your Matplotlib is older than 3.6\n",
    "    fraction=0.06,\n",
    "    pad=0.08,\n",
    "    ticks=np.arange(1, 13)\n",
    ")\n",
    "\n",
    "cbar12.set_label(\"Number of months (1â€“12)\")\n",
    "\n",
    "decorate_fig(fig, title = \"\") # TODO Make colorbar discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the above is the SW quality flag, regionally (this should be below the time series quality flag comparison). What we have seen is that for a region in Yemen, the indices seem to agree with ERA5-D before zero-precipitation adjustment rather than afterwards?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "You can also import the quality flags from ERA5-Drought, for every calendar month, in the reference period, per accumulation period. \n",
    "\n",
    "You are unable to send an API request to download this data for all accumulation periods at once. If you try to do so, you will find that your xarray only contains the data for the last accumulation period (in this case, the 48-month window). \n",
    "\n",
    "We have written a simple script that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One location -- reproduced -- keep --\n",
    "pval_spi, sig_spi = xr_shapiro_test(adjusted_spi_ds, reference_window, index_name = \"SPI\")\n",
    "sig_spi_agg = sig_spi.sum(dim = \"time\") # Aggregate across months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi_all = era5_drought_api_multiple(indicator = \"spi\", var = \"quality\", product_type = \"reanalysis\")\n",
    "quality_spi_all_agg = quality_spi_all.sum(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi_addis = quality_spi_all.sel(coords_dict[\"Addis Ababa, Ethiopia\"])\n",
    "quality_spi_tedd = quality_spi_all.sel(coords_dict[\"Teddington, United Kingdom\"])\n",
    "quality_spi_denv = quality_spi_all.sel(coords_dict[\"Denver, USA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "Significance values are then compared between ERA5-Drought and calculated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "xnor_result = quality_spi_addis == sig_spi.sel(sites=0)\n",
    "\n",
    "# --- Config ---\n",
    "COLORS = {\n",
    "    \"era5\": \"#1f77b4\",\n",
    "    \"repro\": \"#ff7f0e\",\n",
    "    \"match\": \"#2ca02c\",\n",
    "}\n",
    "\n",
    "# --- X locations and bar width for 3 bars per group ---\n",
    "x = np.arange(len(ACCUMULATION_PERIODS))\n",
    "n_bars = 3\n",
    "width = 0.8 / n_bars  # keeps total group width ~0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for i, p in enumerate(ACCUMULATION_PERIODS):\n",
    "    # Existing values (assumed scalar fractions)\n",
    "    era5_val = float(quality_spi_all_agg[f\"significance_{p}\"].sel(coords_dict[\"Addis Ababa, Ethiopia\"]).values)\n",
    "    repro_val = float(sig_spi_agg[f\"significance_{p}\"].sel(sites=0).values)\n",
    "\n",
    "    # Match fraction from xnor_result (aggregate over time and normalise)\n",
    "    mean_true = xnor_result[f\"significance_{p}\"].sum(dim=\"time\", skipna=True)\n",
    "    mean_true = float(mean_true.compute())\n",
    "\n",
    "    # Plot 3 bars: left (ERA5D), center (Reproduced), right (Mismatch)\n",
    "    ax.bar(\n",
    "        x[i] - width, era5_val, width,\n",
    "        label=\"ERA5-Drought\" if i == 0 else None, color=COLORS[\"era5\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i], repro_val, width,\n",
    "        label=\"Reproduced\" if i == 0 else None, color=COLORS[\"repro\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i] + width, mean_true, width,\n",
    "        label=\"Match frequency\" if i == 0 else None, color=COLORS[\"match\"]\n",
    "    )\n",
    "\n",
    "# --- Cosmetics ---\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(p) for p in ACCUMULATION_PERIODS])\n",
    "ax.set_xlabel(\"Accumulation windows (months)\")\n",
    "ax.set_ylabel(\"Fraction of months in calendar year\")\n",
    "ax.set_ylim(0, 12)\n",
    "ax.set_title(\"Fraction of months passing S-W test & match frequency (for each accumulation window).\")\n",
    "ax.legend(frameon=False)\n",
    "ax.grid(axis=\"y\", alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the above is the SW quality flag, at a single location (this should be above the regional quality flag comparison). Except for the 1-month window there seems to be perfect agreement. \n",
    "\n",
    "Interesting that for 1-month window we have more months that passs SW quality check than ERA5-Drought. Consistent with the time series comparison of ERA5-D & Reproduce -> as saw that ERA5-Drought was consistently larger, and \"spikier\". Perhaps our fit is of \"higher\" quality, or perhaps the test was just less good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-spei)=\n",
    "###  4. Calculate SPEI from ERA5 reanalysis data.\n",
    "The steps to calculating the SPEI-index are exactly the same as calculating the SPI index, with the only modification being that the SPEI integrates both the precipitation and potential evapotranspiration (PET) data, and uses a different statistical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### 4.0 Importing monthly-average precipitation &  potential evaporation data.\n",
    "\n",
    "We import the monthly-mean precipitation & potential evaporation data from the \"ERA5 monthly-averaged data on single levels from 1940 to present\" dataset. (https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means?tab=download)\n",
    "\n",
    "In case you start from this section, we also import the monthly-mean preciptiation data too from the previous SPI section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_tp_moda_request = {\n",
    "    \"param\": \"228.128\",       # Variable: Total precipitation\n",
    "    \"stream\": \"moda\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template\n",
    "\n",
    "# Download precipitation data and convert to desired format\n",
    "era5_tp_monthly = ekd.from_source(\"cds\", ID_ERA5, era5_tp_moda_request)  # Download as field list\n",
    "era5_tp_monthly = era5_tp_monthly.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_tp_monthly =  era5_tp_monthly.assign_coords(\n",
    "    longitude=((era5_tp_monthly.longitude - 180) % 360) - 180).sortby(\"longitude\")\n",
    "era5_tp_monthly = rechunk(era5_tp_monthly)  # Re-chunk for speed gain in fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_pev_moda_request = {\n",
    "    \"param\": \"251.228\",       # Variable: Potential evaporation (pev)\n",
    "    \"stream\": \"moda\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template\n",
    "\n",
    "# Download data and convert to desired format\n",
    "era5_pev_monthly = ekd.from_source(\"cds\", ID_ERA5, era5_pev_moda_request)  # Download as field list\n",
    "era5_pev_monthly = era5_pev_monthly.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_pev_monthly =  era5_pev_monthly.assign_coords(\n",
    "    longitude=((era5_pev_monthly.longitude - 180) % 360) - 180).sortby(\"longitude\")\n",
    "era5_pev_monthly = rechunk(era5_pev_monthly)  # Re-chunk for speed gain in fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "#### 4.1 Calculate accumulated water balance (P-PET) for different windows.\n",
    "\n",
    "A time series of both precipitation & potential evaporation (PET/PEV) from one grid point are extracted and the precipitation & potential evaporation are accumulated over the previous $n$ months using a moving window, analogous to the SPI.\n",
    "\n",
    "Note, [ECMWF convention](https://codes.ecmwf.int/grib/param-db/182) is that negative values for PEV/PET indicate evaporation, whereas positive values indicate condensation. Since the ECMWF convention is that a negative value of PET indicates evaporation, care must be taken when subtracting the precipitation (P) by the potential evaporation (PEV/PET).\n",
    "\n",
    "The negative of PEV/PET must be applied, $P âˆ’ (âˆ’PET) = P + PET$ in this case. This approach aligns with the definition of the water balance, where a negative water balance value indicates that more water is potentially being transferred to the atmosphere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take difference (P- (-PET)) and then accumulate\n",
    "era5_wb_monthly = era5_pev_monthly[\"pev\"] - (- era5_tp_monthly[\"tp\"])\n",
    "era5_wb_monthly = era5_wb_monthly.rename(\"wb\")\n",
    "era5_wb_monthly = era5_wb_monthly.to_dataset()\n",
    "era5_wb_example_sites = era5_wb_monthly.sel(latitude=xr.DataArray(lats, dims=\"sites\"),\n",
    "                     longitude=xr.DataArray(lons, dims=\"sites\"))\n",
    "\n",
    "# Perform the accumulation for each accumulation period at single location\n",
    "wb_example_site = accum_var(era5_wb_example_sites, \"wb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "The plot below illustrates the seasonal water balance for a country like Ethiopia, where periods of high evapotranspiration exceed precipitation, resulting in a negative water balance. Conversely, during wetter months, precipitation surpasses evapotranspiration, producing a surplus of available water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_accumulation(\"wb\", wb_example_site.sel(sites=0), sites_dict[\"Addis Ababa, Ethiopia\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate SPEI Index time-series at location (Addis Ababa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "The general log-logistic distribution [reference] is fitted only to the data within the reference period (1991-2020), similar to the gamma distribution being fitted in the calculation of the SPI-index. A separate distribution is fitted for each calendar month per accumulation window.\n",
    "\n",
    "This fitting is done with the [scipy.stats.genlogistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.genlogistic.html) object in Python. Three parameters are then outputted per calendar month, per accumulation period: the shape (), location () and scale () parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit log logistic distributions\n",
    "genlog_params_fitted = fit_monthly_spei(wb_example_site.sel(**reference_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "Similar to the calculation for the SPI index, the three parameters from the fitted general log-logistic distribution are taken and the cumulative distribution function (CDF) is calculated up to the accumulated water balance, from the probability distribution function (PDF).\n",
    "\n",
    "As $ P âˆ’ PET $ is barely identical to 0, no modifications analogous to the SPI such as adjusting for zero precipitation, are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CDF series\n",
    "cdf_spei_ds = compute_monthly_spei(wb_example_site, genlog_params_fitted)\n",
    "# Compute SPEI series\n",
    "spei_ds = cdf_to_znorm_transform(cdf_spei_ds , \"SPEI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "#### 4.3 Comparison of ERA5-Drought & Reproduced SPEI-index (Addis Ababa, Ethiopia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "spei_addis_req1 = {\n",
    "    \"variable\": [\"standardised_precipitation_evapotranspiration_index\"],\n",
    "    \"accumulation_period\": [str(p) for p in ACCUMULATION_PERIODS],\n",
    "    \"version\": \"1_0\",\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "    \"area\": area_dict[\"Addis Ababa, Ethiopia\"],\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"year\": [str(y) for y in range(1940, 1981)],\n",
    "}\n",
    "\n",
    "spei_addis_req2 = {\n",
    "    **spei_addis_req1,\n",
    "    \"year\": [str(y) for y in range(1981, 2025)],\n",
    "}\n",
    "\n",
    "era5_spei_addis_src = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, spei_addis_req1, spei_addis_req2)\n",
    "era5_spei_addis_ds = era5_spei_addis_src.to_xarray(compat=\"equals\")\n",
    "era5_spei_addis_point = era5_spei_addis_ds.sel(sites_dict[\"Addis Ababa, Ethiopia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a time series comparison at Addis Ababa, Ethiopia\n",
    "plot_index_comparison_side_by_side(\"SPEI\", era5_spei_addis_point, spei_ds.sel(sites=0),spei_categories) # takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quantitative absolute median and absolute median difference. \n",
    "spei_results_diff_addis = comparison_monthly_statistics(era5_spei_addis_point, spei_ds.sel(sites=0), \"SPEI\", ACCUMULATION_PERIODS) # takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display median difference\n",
    "display_monthly_statistics(spei_results_diff_addis, \"median\", \"SPEI - Median Difference - Addis Ababa\")\n",
    "# Display median absolute difference\n",
    "display_monthly_statistics(spi_results_diff_addis, \"abs_median\", \"SPEI - Absolute Median Difference - Addis Ababa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: the SPEI-index shows considerably less agreement than the SPI index in the previous section, for the same location. We understand that the fitting distribution in the case of the SPEI index is particularly sensitive to the parameterisation used, and that the parameters that are outputted are highly dependent on the initial \"guess\" of the parameters. This in the ERA5-D paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "#### 4.4 ERA5-Drought vs Reproduced SPEI (discrepancy in categorisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_confusion_matrices(spei_ds.sel(sites=0),spei_addis_point,\"SPEI\",spei_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: even though there is considerably less agreement in the SPEI index, the agreement in drought severity for SPEI is not particularly affected, except for a few timestamps where the severity is +- 1 category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.5 Comparison of ERA5-Drought & Reproduced SPEI-index over region (Horn of Africa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "spei_region_request1 = {\n",
    "    \"variable\": [\"standardised_precipitation_evaporatranspiration_index\"], # drought index\n",
    "    \"year\": [f\"{y}\" for y in range(1940,1981)],\n",
    "    \"product_type\": [\"reanalysis\"]\n",
    "} | request_era5_drought_region_template\n",
    "\n",
    "spei_region_request2 = {\n",
    "    \"variable\": [\"standardised_precipitation_evaporatranspiration_index\"], # drought index\n",
    "    \"year\": [f\"{y}\" for y in range(1981,2025)],\n",
    "    \"product_type\": [\"reanalysis\"]\n",
    "} | request_era5_drought_region_template\n",
    "\n",
    "era5_spei = ekd.from_source(\"cds\", dataset, spei_region_request1, spei_region_request2) # Sends request for this dataset to CDS.\n",
    "era5_spei = era5_spei.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset (latitude is descending in ERA5) - should this be wrapped in a function now?\n",
    "ds_loc = era5_monthly_mean_wb_reanal.sel(\n",
    "    latitude=slice(lat_max, lat_min),\n",
    "    longitude=slice(lon_min, lon_max),\n",
    ")\n",
    "\n",
    "# Accumulate water balance\n",
    "wb_example_region = accum_var(ds_loc, var = \"wb\") \n",
    "\n",
    "# Select reference precipitation window.\n",
    "wb_example_region_reference = wb_example_region.sel(**reference_window) \n",
    "\n",
    "# Fit distribution to reference window, extract parameters.\n",
    "log_params_fitted_region = fit_monthly_spei(wb_example_region_reference)\n",
    "\n",
    "# Calculate CDF + SPEI with parameters.\n",
    "cdf_spei_region = compute_monthly_spei(wb_example_region, log_params_fitted_region)\n",
    "\n",
    "# Transform CDF to SPEI\n",
    "spei_region = cdf_to_znorm_transform(adjusted_cdf_region, \"SPEI\")\n",
    "\n",
    "spei_region = spei_region.rename({\n",
    "    \"valid_time\": \"time\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\"})\n",
    "\n",
    "era5_spei, spei_region = xr.align(era5_spei, spei_region, join=\"left\")   # only overlapping coords\n",
    "\n",
    "# # Apply land-sea mask over region. \n",
    "# spei_region = spei_region.where(ls_mask[\"lsm\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one time slice and one ensemble\n",
    "map_1 = era5_spei[\"SPI48\"].sel(time = \"2024-12-01\")\n",
    "map_2 = spei_region[\"SPI48\"].sel(time = \"2024-12-01\")\n",
    "\n",
    "horn = domains.Domain.from_bbox(\n",
    "    bbox=[30, 53, -5, 18],\n",
    "    name=\"Horn of Africa\",\n",
    ")\n",
    "\n",
    "# # Plot with EarthKit\n",
    "SPI_STYLE = Style(cmap='RdBu_r', vmin = -4, vmax = 4, normalize=False)\n",
    "SPI_DIFF_STYLE = Style(cmap='viridis', vmin = 0, vmax = 1, normalize=False)\n",
    "\n",
    "# Create figure with 2 columns\n",
    "fig = ekp.Figure(rows=1, columns=3, size=(12, 6))  \n",
    "\n",
    "# First subplot (left)\n",
    "subplot1 = fig.add_map(domain=horn, row=0, column=0)\n",
    "subplot1.grid_cells(map_1,style=SPI_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "subplot1.title(\"ERA5-Drought SPI-48 at 2024-12-01\")\n",
    "\n",
    "# Second subplot (left)\n",
    "subplot2 = fig.add_map(domain=horn, row=0, column=1)\n",
    "subplot2.grid_cells(map_2,style=SPI_STYLE)\n",
    "subplot2.legend(location=\"right\")\n",
    "subplot2.title(\"Reproduced SPI-48 at 2024-12-01\")\n",
    "\n",
    "# First subplot (left)\n",
    "subplot3 = fig.add_map(domain=horn, row=0, column=2)\n",
    "subplot3.grid_cells(spi_diff, x=lon_grid, y=lat_grid,style=SPI_DIFF_STYLE)\n",
    "subplot3.legend(location=\"right\")\n",
    "subplot3.title(\"Absolute difference, ERA5-Drought - Reproduced, SPI-48\")\n",
    "\n",
    "# Add decorations\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: compare SPEI-index regionally around Addis Ababa region. Good agreement / bad agreement across all of time, and at one timestamp? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.6 Comparison of S-W test on SPEI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select latitude and longitude box (40 x 40 here)\n",
    "dom_lon_min, dom_lon_max = 30, 53  \n",
    "dom_lat_min, dom_lat_max = -5, 18\n",
    "\n",
    "lon_min, lon_max = 30, 31  \n",
    "lat_min, lat_max = 17, 18\n",
    "\n",
    "# Subset (latitude is descending in ERA5) - should this be wrapped in a function now?\n",
    "ds_loc = era5_wb_monthly.sel(\n",
    "    latitude=slice(dom_lat_max, dom_lat_min),\n",
    "    longitude=slice(dom_lon_min, dom_lon_max))\n",
    "\n",
    "# Accumulate precipitations\n",
    "wb_example_global = accum_var(ds_loc, var = \"wb\") \n",
    "\n",
    "# Select reference precipitation window.\n",
    "wb_example_global_reference = wb_example_global.sel(**reference_window) \n",
    "\n",
    "# Fit distribution to reference window, extract parameters.\n",
    "params_fitted_global = fit_monthly_spei(wb_example_global_reference)\n",
    "\n",
    "# Calculate CDF  with parameters.\n",
    "cdf_global = compute_monthly_spei(wb_example_global, params_fitted_global)\n",
    "\n",
    "spei_global = cdf_to_znorm_transform(cdf_global, \"SPEI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_spei_global, sig_spei_global = xr_shapiro_test(spei_global, reference_window, index_name = \"SPEI\")\n",
    "sig_spei_agg_global = sig_spei_global.sum(dim = \"time\") # Aggregate across months.\n",
    "\n",
    "quality_spei_all = era5_drought_api_multiple(indicator = \"spei\", var = \"quality\", product_type = \"reanalysis\")\n",
    "quality_spei_all_agg = quality_spei_all.sum(dim=\"time\")\n",
    "\n",
    "accumulation_window = 1\n",
    "\n",
    "# --- Colormap as you already have ---\n",
    "beige_to_red = LinearSegmentedColormap.from_list(\n",
    "    \"beige_to_red\",\n",
    "    [\"#fbeec2\", \"#d47a57\", \"#8b0000\"]   # beige â†’ soft red â†’ dark red\n",
    ")\n",
    "\n",
    "ZERO_PRECIP_STYLE      = Style(cmap=beige_to_red, vmin=1,  vmax=12,  normalize=False)\n",
    "BLACK_POINT_STYLE      = Style(cmap=ListedColormap([\"black\"]), vmin=0, vmax=0.99, normalize=False)\n",
    "ZERO_PRECIP_DIFF_STYLE = Style(cmap=\"RdBu_r\", vmin=-12, vmax=12, normalize=False)\n",
    "\n",
    "# ------------------ LEFT PANEL ------------------\n",
    "da1 = sig_spei_agg_global[f\"significance_{accumulation_window}\"] # Reproduced\n",
    "non_ones1 = da1.where(da1 != 0)   # all values except 0\n",
    "ones1     = da1.where(da1 == 0)   # only values equal to 0\n",
    "\n",
    "# ------------------ MIDDLE PANEL ------------------\n",
    "da2 = quality_spei_all_agg[f\"significance_{accumulation_window}\"].sel(\n",
    "    lat=slice(lat_max, lat_min),\n",
    "    lon=slice(lon_min, lon_max)) # ERA5-D\n",
    "\n",
    "non_ones2 = da2.where(da2 != 0)\n",
    "ones2     = da2.where(da2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "horn = ekp.geo.domains.Domain(\n",
    "    bbox=[dom_lon_min, dom_lon_max, dom_lat_min, dom_lat_max],\n",
    "    name=\"Horn of Africa\",\n",
    ")\n",
    "\n",
    "# Create figure with 3 columns\n",
    "fig = ekp.Figure(rows=1, columns=2)\n",
    "\n",
    "# ---------- Subplot 1 ----------\n",
    "subplot1 = fig.add_map(domain=horn, row=0, column=0)\n",
    "mappable1 = subplot1.grid_cells(non_ones1.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot1.grid_cells(ones1.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax1 = subplot1.ax\n",
    "ax1.set_title(f\"Reproduced â€” # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ---------- Subplot 2 ----------\n",
    "subplot2 = fig.add_map(domain=horn, row=0, column=1)\n",
    "mappable2 = subplot2.grid_cells(non_ones2.persist(), style=ZERO_PRECIP_STYLE)\n",
    "subplot2.grid_cells(ones2.persist(), style=BLACK_POINT_STYLE, zorder=101)\n",
    "ax2 = subplot2.ax\n",
    "ax2.set_title(f\"ERA5 â€”  # Months S-W  Rejection ({accumulation_window}â€‘Month Window)\")\n",
    "\n",
    "# ------------------ SHARED FIGURE SETTINGS ------------------\n",
    "# Set figure size (wide)\n",
    "fig_mpl = ax1.figure\n",
    "fig_mpl.set_size_inches(18, 6)\n",
    "\n",
    "# Shared colorbar for the first two panels â€” use the mappable from panel 1\n",
    "cbar12 = fig_mpl.colorbar(\n",
    "    mappable1,\n",
    "    ax=[ax1, ax2],\n",
    "    orientation=\"horizontal\",\n",
    "    location=\"bottom\",    # remove this line if your Matplotlib is older than 3.6\n",
    "    fraction=0.06,\n",
    "    pad=0.08,\n",
    "    ticks=np.arange(1, 13)\n",
    ")\n",
    "\n",
    "cbar12.set_label(\"Number of months (1â€“12)\")\n",
    "\n",
    "decorate_fig(fig, title = \"\") # TODO Make colorbar discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: have shown that region around Yemen seems to disagreement in SW quality flag values. This is to be expected since the calculated dataset is not in fact in good agreement with ERA5-Drought. Check at another location with more precipitation (somewhere in mainland europe maybe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "Quality flags are imported from ERA5-Drought, for every calendar month, in the reference period, per accumulation period for the SPEI index.\n",
    "\n",
    "You are unable to send an API request to download this data for all accumulation periods at once. We have written a simple script that sends separate API requests, concatenating the data into a single xarray, to then use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_quality_spei_global = era5_drought_api_multiple(indicator = \"spei\", var = \"quality\", product_type = \"reanalysis\")\n",
    "era5_quality_spei_global_agg = era5_quality_spei_global.sum(dim=\"time\")\n",
    "\n",
    "era5_quality_spei_addis = era5_quality_spei_global.sel(sites_dict[\"Addis Ababa, Ethiopia\"])\n",
    "era5_quality_spei_tedd = era5_quality_spei_global.sel(sites_dict[\"Teddington, United Kingdom\"])\n",
    "era5_quality_spei_denv = era5_quality_spei_global.sel(sites_dict[\"Denver, USA\"])\n",
    "\n",
    "era5_sig_spei_addis_agg = era5_quality_spei_global_agg.sel(sites_dict[\"Addis Ababa, Ethiopia\"])\n",
    "era5_sig_spei_tedd_agg = era5_quality_spei_global_agg.sel(sites_dict[\"Teddington, United Kingdom\"])\n",
    "era5_sig_spei_denv_agg = era5_quality_spei_global_agg.sel(sites_dict[\"Denver, USA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "The calculated Shapiro-Wilks significance values are then compared with those from ERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XNOR Result \n",
    "xnor_spei_result = era5_quality_spei_addis == sig_spei\n",
    "\n",
    "#TODO: Wrap up all code in the function.\n",
    "\n",
    "# --- Config ---\n",
    "COLORS = {\n",
    "    \"era5\": \"#1f77b4\",\n",
    "    \"repro\": \"#ff7f0e\",\n",
    "    \"match\": \"#2ca02c\",\n",
    "}\n",
    "\n",
    "# --- X locations and bar width for 3 bars per group ---\n",
    "x = np.arange(len(ACCUMULATION_PERIODS))\n",
    "n_bars = 3\n",
    "width = 0.8 / n_bars  # keeps total group width ~0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for i, p in enumerate(ACCUMULATION_PERIODS):\n",
    "    # Existing values (assumed scalar fractions)\n",
    "    era5_val = float(era5_sig_spei_addis_agg[f\"significance_{p}\"].values)\n",
    "    repro_val = float(sig_spei_aggregated[f\"significance_{p}\"].values)\n",
    "\n",
    "    # Mismatch fraction from xnor_result (aggregate over time)\n",
    "    mean_true = xnor_spei_result[f\"significance_{p}\"].sum(dim=\"time\", skipna=True)\n",
    "    mean_true = float(mean_true.compute())\n",
    "\n",
    "    # Plot 3 bars: left (ERA5D), center (Reproduced), right (Mismatch)\n",
    "    ax.bar(\n",
    "        x[i] - width, era5_val, width,\n",
    "        label=\"ERA5-Drought\" if i == 0 else None, color=COLORS[\"era5\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i], repro_val, width,\n",
    "        label=\"Reproduced\" if i == 0 else None, color=COLORS[\"repro\"]\n",
    "    )\n",
    "    ax.bar(\n",
    "        x[i] + width, mean_true, width,\n",
    "        label=\"Match frequency\" if i == 0 else None, color=COLORS[\"match\"]\n",
    "    )\n",
    "\n",
    "# --- Cosmetics ---\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(p) for p in ACCUMULATION_PERIODS])\n",
    "ax.set_xlabel(\"Accumulation windows (months)\")\n",
    "ax.set_ylabel(\"# of months\")\n",
    "ax.set_title(\"Significance  and Match Frequency by Accumulation Period\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_ylim([-12, 12])\n",
    "ax.grid(axis=\"y\", alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: SW quality flag does show good agreement however at single location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "#### 4.7 Comparison of ERA5-Drought & Reproduced SPEI-index with quality flags (Addis Ababa, Ethiopia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "spei_ds_addis_masked = apply_sw_quality_mask(era5_quality_spei_addis, spei_ds.sel(sites=0), \"SPEI\")\n",
    "era5_masked = apply_sw_quality_mask(era5_quality_spei_addis, spei_addis_point, \"SPEI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_index_comparison_side_by_side(\"SPEI\", era5_masked, spei_ds_addis_masked, spei_categories) # takes a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: make comparison with quality flags- perhaps look at a location where there are more failed months? Otherwise, get rid since only 1 month fails SW test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.245188,
     "end_time": "2024-03-08T17:39:21.277354",
     "exception": false,
     "start_time": "2024-03-08T17:39:21.032166",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-ens)=\n",
    "### 5. Calculating the SPI/SPEI from ERA5 ensemble data.\n",
    "Once again, calculating the SPI-index for all 10 ensemble members are the same as calculating the SPI index for one member, with the only modification being that there is an extra coordinate, \"number\" that identifies each ensemble member. \n",
    "\n",
    "NOTE: Change to calculate difference in precipitation and PEV and then accumulate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "Download monthly ensemble precipitation data, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_precipitation_edmo = {\n",
    "    \"param\": \"228.128\",       # Variable: Total precipitation\n",
    "    \"stream\": \"edmo\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and convert to desired format\n",
    "era5_monthly_mean_ens = ekd.from_source(\"cds\", ID_ERA5, request_era5_precipitation_edmo)  # Download as field list\n",
    "era5_monthly_mean_ens = era5_monthly_mean_ens.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_tp_monthly_mean_ens = rechunk(era5_monthly_mean_ens)  # Re-chunk for speed gain in fitting\n",
    "era5_tp_monthly_mean_ens  # Display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "And we also download the monthly-mean total potential evaporation data from the ERA5 reanalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5_pev_edmo = {\n",
    "    \"param\": \"251.228\",       # Variable: Potential evaporation (pev)\n",
    "    \"stream\": \"edmo\",         # Data stream: Monthly mean reanalysis\n",
    "} | request_era5_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and convert to desired format\n",
    "era5_pev_monthly_mean_ens = ekd.from_source(\"cds\", ID_ERA5, request_era5_pev_edmo)  # Download as field list\n",
    "era5_pev_monthly_mean_ens = era5_pev_monthly_mean_ens.to_xarray(compat=\"equals\")  # Convert to xarray dataset\n",
    "era5_pev_monthly_mean_ens = rechunk(era5_pev_monthly_mean_ens)  # Re-chunk for speed gain in fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "Precipitation is accumulated for all the accumulation periods in ERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_site = {\"latitude\": 9.25, \"longitude\": 40.5,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the grid point and time slice for Ethiopia\n",
    "precipitation_example_site = era5_tp_monthly_mean_ens.sel(**example_site)\n",
    "\n",
    "# Perform the accumulation for each accumulation period\n",
    "precipitation_ens_example_site = accum_var(precipitation_example_site, \"tp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "As is potential evaporaiton, for all the accumulation periods in ERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the grid point and time slice for Ethiopia\n",
    "pev_example_site = era5_pev_monthly_mean_ens.sel(**example_site)\n",
    "\n",
    "# Perform the accumulation for each accumulation period\n",
    "pev_ens_example_site = accum_var(pev_example_site, \"pev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "The \"water balance\" is calculated, ensuring once again the -ve of the PET, as discussed earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_ens_example_site = xr.Dataset(\n",
    "    {\n",
    "        f\"wb_{var.replace('tp_', '')}\": precipitation_ens_example_site[var]\n",
    "                                       + pev_ens_example_site[var.replace(\"tp_\", \"pev_\")]\n",
    "        for var in precipitation_ens_example_site.data_vars # loop through every variable.\n",
    "        if var.startswith(\"tp_\") and var.replace(\"tp_\", \"pev_\") in pev_ens_example_site.data_vars\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "Gamma distribution is fitted for each calendar month (12) for each ensemble member (here, 10), with their parameters calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit log logistic distributions\n",
    "gamma_ens_params_fitted = fit_monthly_spi(precipitation_ens_example_site.sel(**reference_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "Log-logistic distribution is fitted for each calendar month (12) for each ensemble member (here, 10), with their parameters calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_logistic_ens_params_fitted = fit_monthly_spei(wb_ens_example_site.sel(**reference_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "SPI is calculated for each accumulation period and each ensemble member from the fitted gamma distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SPI series -- new function\n",
    "cdf_ens_ds = compute_monthly_spi(precipitation_ens_example_site, gamma_ens_params_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "SPEI is calculated for each accumulation period and each ensemble member from the fitted log-logistic distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SPEI series -- new function\n",
    "cdf_wb_ens_ds = compute_monthly_spei(wb_ens_example_site, log_logistic_ens_params_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "Quality control is tested with the Shapiro-Wilks test on calculated ensemble SPI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_spi_stat, ens_spi_pval, ens_spi_sig = xr_shapiro_test(spi_ens_ds, reference_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spi_ens_all = era5_drought_api_multiple(indicator = \"spi\", var = \"quality\", product_type = \"ensemble_members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "Quality control using the Shapiro-Wilks test on calculated ensemble SPEI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_spei_stat, ens_spei_pval, ens_spei_sig = xr_shapiro_test(spei_ens_ds, reference_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_spei_ens_all = era5_drought_api_multiple(indicator = \"spei\", var = \"quality\", product_type = \"ensemble_members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "Reading in ensemble SPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_spi_ens = era5_drought_index_multiple(\"SPI\", accum_period = [1,3])\n",
    "era5_spi_ens = era5_spi_ens.sel(lat = 9.25, lon = 40.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "Time-series comparison of SPI at one location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(0, 10):  #### patch this code so titles show which plot is which ensemble member. is obvious but for full thing?\n",
    "    plot_index_comparison_side_by_side(\"SPI\", era5_spi_ens.sel(number=m), spi_ens_ds.sel(number=m), accumulation_periods = [1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have a table for each accumulation window maybe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "Reading in ensemble SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_spei_ens = era5_drought_index_multiple(\"SPEI\", accum_period = [3, 24, 48])\n",
    "era5_spei_ens = era5_spei_ens.sel(lat = 9.25, lon = 40.5) ### data needs to be downloaded overnight, couldn't do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "Time-series comparison of SPEI at one location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(0, 10):  #### patch this code so titles show which plot is which ensemble member. is obvious but for full thing?\n",
    "    plot_index_comparison_side_by_side(\"SPEI\", era5_spei_ens.sel(number=m), spei_ens_ds.sel(number=m), [3, 24, 48])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "**TODO point-to-make**: each member from the drought-index ensemble is in fact reproduceable from each member from the precipitation / potential evaporation ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "(section-conclusion)=\n",
    "### 6. Conclusion\n",
    "\n",
    "**SPI**- As shown in Sectionâ€¯2, the C3S ERA5 Drought SPIâ€‘Indicator dataset and its manually reproduced counterpart are highly consistent. This agreement is evident across the full time series at a point location in Addis Ababa, Ethiopia, as well as regionally over the Horn of Africa. The median difference and median absolute difference are both close to 0, and the vast majority of pixels exhibit a nearâ€‘zero difference (defined here as |Î”| â‰¤ Îµ with Îµ = 1Ã—10â»âµ to avoid floatingâ€‘point artefacts) across all comparisons.\n",
    "\n",
    "Furthermore, the \"probability of zero precipitation\" quality flag was also shown to be reproducible, globally, with no discrepancy. Since the Shapiro-Wilks quality flag is dependent on the dataset after statistical fitting, there was discrepancy at certain months, for Addis Ababa, and other locations...\n",
    "\n",
    "We also examined discrepancies in droughtâ€‘severity categorisation between the C3S ERA5â€‘Drought dataset and the manually reproduced counterpart at individual locations, including Addis Ababa, London, and Denver. While the severity classifications were generally consistent between ERA5â€‘Drought and the reproduced dataset, there were instances at specific timestamps where the two SPI indicators diverged sufficiently to fall into different categories.\n",
    "\n",
    "Overall, \n",
    "\n",
    "**SPEI**- Following on from Section 3, \n",
    "\n",
    "**SPI/SPEI Ensemble**- Lastly, as was shown in Section 4, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.271597,
     "end_time": "2024-03-08T17:40:01.664067",
     "exception": false,
     "start_time": "2024-03-08T17:40:01.392470",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## â„¹ï¸ If you want to know more\n",
    "\n",
    "This notebook is extremely long (by necessity) so it would be good to end on a recap with general conclusions.\n",
    "For example, can you quantify the differences and tell me if those are a big deal in a general use case? As a user, I might not want to look at every panel in every figure, I just want to get a summary statistic and a general point.\n",
    "\n",
    "### Key resources\n",
    "The CDS catalogue entries for the data used were:\n",
    "* Complete ERA5 global atmospheric reanalysis: [reanalysis-era5-complete](https://doi.org/10.24381/cds.143582cf)\n",
    "* Monthly drought indices from 1940 to present derived from ERA5 reanalysis: [derived-drought-historical-monthly](https://doi.org/10.24381/9bea5e16)\n",
    "\n",
    "Code libraries used:\n",
    "* [earthkit](https://github.com/ecmwf/earthkit)\n",
    "  * [earthkit-data](https://github.com/ecmwf/earthkit-data)\n",
    "  * [earthkit-plots](https://github.com/ecmwf/earthkit-plots)\n",
    "    \n",
    "### References\n",
    "\n",
    "[[1]](https://doi.org/10.59327/IPCC/AR6-9789291691647) IPCC, 2023: Sections. In: Climate Change 2023: Synthesis Report. Contribution of Working Groups I, II and III to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change [Core Writing Team, H. Lee and J. Romero (eds.)]. IPCC, Geneva, Switzerland, pp. 35-115, doi: 10.59327/IPCC/AR6-9789291691647\n",
    "\n",
    "[[2]](https://mcusercontent.com/8ed7ad7972fae058e8f4fb7e8/files/6d02e6e7-8639-a44d-5a8c-2313124ef699/Costs_of_climate_analysis_011225.pdf) Estimated financial losses faced by UK farmers due dry weather impacts on key arable crops - Energy & Climate Intelligence Unit\n",
    "\n",
    "[[3]](https://www.unicef.org/media/165191/file/LACR-Flash-Update-11-November-2024.pdf) Climate-related crisis in the Amazon region Flash Update No. 2, Brazil, Peru, Bolivia, Columbia - UNICEF, 2024\n",
    "\n",
    "[[5]](https://doi.org/10.1080/01431160500296032) Vicenteâ€Serrano, S.M., Cuadratâ€Prats, J.M. and Romo, A., 2006. Early prediction of crop production using drought indices at different timeâ€scales and remote sensing data: application in the Ebro Valley (northâ€east Spain). International Journal of Remote Sensing, 27(3), pp.511-518.\n",
    "\n",
    "[[6]](https://doi.org/10.1016/j.agrformet.2017.01.021) Russo, A., Gouveia, C.M., PÃ¡scoa, P., DaCamara, C.C., Sousa, P.M. and Trigo, R.M., 2017. Assessing the role of drought events on wildfires in the Iberian Peninsula. Agricultural and Forest Meteorology, 237, pp.50-59.\n",
    "\n",
    "[[7]](https://climate.colostate.edu/pdfs/relationshipofdroughtfrequency.pdf) McKee, T.B., Doesken, N.J. and Kleist, J., 1993, January. The relationship of drought frequency and duration to time scales. In Proceedings of the 8th Conference on Applied Climatology (Vol. 17, No. 22, pp. 179-183).\n",
    "\n",
    "[[8]](https://doi.org/10.1175/2009JCLI2909.1) Vicente-Serrano, S.M., BeguerÃ­a, S. and LÃ³pez-Moreno, J.I., 2010. A multiscalar drought index sensitive to global warming: the standardized precipitation evapotranspiration index. Journal of climate, 23(7), pp.1696-1718.\n",
    "\n",
    "[[9]](https://doi.org/10.1038/s41597-025-04896-y) Keune, J., Di Giuseppe, F., Barnard, C., Damasio da Costa, E. and Wetterhall, F., 2025. ERA5â€“Drought: Global drought indices based on ECMWF reanalysis. Scientific Data, 12(1), p.616."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 968.520731,
   "end_time": "2024-03-08T17:40:03.783430",
   "environment_variables": {},
   "exception": null,
   "input_path": "D520.3.2.3b.SEASONAL_multimodel-bias_v5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T17:23:55.262699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
