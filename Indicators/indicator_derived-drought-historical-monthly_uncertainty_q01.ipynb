{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Quality Assessment for ERA5 Drought Indicator: Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Production date: 2026-xx-xx\n",
    "\n",
    "**Please note that this repository is used for development and review, so quality assessments should be considered work in progress until they are merged into the main branch.**\n",
    "\n",
    "Dataset version: 1.0.\n",
    "\n",
    "Produced by: Enis Gerxhalija, Olivier Burggraaff (National Physical Laboratory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## üåç Use case: Use case listed here in full "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## ‚ùì Quality assessment question\n",
    "* **In most cases there should be one question listed here in bold**\n",
    "* **(In some cases a second related/follow-up question may be included)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "**‚ÄòContext paragraph‚Äô (no title/heading)** - a very short introduction before the assessment statement describing approach taken to answer the user question. One or two key references could be useful,  if the assessment summarises literature . These can be referenced directly in the text, like `[Rodriguez et. al. 2018](https://doi.org/10.1038/s41598-018-20628-2)` giving: [Rodriguez et. al. 2018](https://doi.org/10.1038/s41598-018-20628-2). For major references numerical labels like this should be used (which should also listed at the end) `Rodriguez et. al. 2018, [[1]](https://doi.org/10.1038/s41598-018-20628-2))`giving: Rodriguez et. al. 2018, [[1]](https://doi.org/10.1038/s41598-018-20628-2)). Please use DOI links where possible.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üì¢ Quality assessment statement\n",
    "\n",
    "```{admonition} These are the key outcomes of this assessment\n",
    ":class: note\n",
    "* Finding 1\n",
    "* Finding 2\n",
    "* Finding 3\n",
    "* etc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## üìã Methodology\n",
    "\n",
    "A ‚Äòfree text‚Äô introduction to the data analysis steps or a description of the literature synthesis, with a justification of the approach taken, and limitations mentioned. **Mention which CDS catalogue entry is used, including a link, and also any other entries used for the assessment**.\n",
    "\n",
    "Followed by a numbered list of the methodology and results, with the same headings as the sections under ‚ÄòAnalysis and Results‚Äô. These should be links to the sections below, using the format `[](section-label)`. The title of the section will be automatically populated, so no need to repeat the title of the section when referecing it like this.\n",
    "\n",
    "```{note}\n",
    "The section labels for the links need to be manually set, as seen below (`(section-1)=`, followed by the heading). These labels will be shown in GitHub but will not appear when the Jupyter Book page is built.\n",
    "```\n",
    "\n",
    "* These headings can be specific to the quality assessment, and help guide the user through the ‚Äòstory‚Äô of the assessment. This means we cannot pre-define the sections and headings here, as they will be different for each assessment.\n",
    "* Sub-bullets could be used to outline what will be done/shown/discussed in each section\n",
    "* The list below is just an example, or may need more or fewer sections, with different headings\n",
    "\n",
    "E.g. 'The analysis and results are organised in the following steps, which are detailed in the sections below:' \n",
    "\n",
    "**[](section-codesetup)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-spi)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "<!-- **[](section-spei)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    "\n",
    "**[](section-4)**\n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings.\n",
    " \n",
    "**[](section-5)** \n",
    " * Sub-steps or key points listed in bullet below. No strict requirement to match and link to sub-headings. -->\n",
    "\n",
    "Any further notes on the method could go here (explanations, caveats or limitations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## üìà Analysis and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-codesetup)=\n",
    "### 1. Code setup\n",
    "```{note}\n",
    "This notebook uses [earthkit](https://github.com/ecmwf/earthkit) for downloading ([earthkit-data](https://github.com/ecmwf/earthkit-data)) and visualising ([earthkit-plots](https://github.com/ecmwf/earthkit-plots)) data. Because earthkit is in active development, some functionality may change after this notebook is published. If any part of the code stops functioning, please raise an issue on our GitHub repository so it can be fixed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Import required libraries\n",
    "In this section, we import all the relevant packages needed for running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Input / Output\n",
    "from pathlib import Path\n",
    "import earthkit.data as ekd\n",
    "import warnings\n",
    "\n",
    "# General data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from functools import partial\n",
    "\n",
    "# Analysis\n",
    "import calendar\n",
    "from scipy import stats\n",
    "\n",
    "# Visualisation\n",
    "import earthkit.plots as ekp\n",
    "from earthkit.plots.styles import Style\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "from tqdm import tqdm  # Progress bars\n",
    "\n",
    "# Visualisation in Jupyter book -- automatically ignored otherwise\n",
    "try:\n",
    "    from myst_nb import glue\n",
    "except ImportError:\n",
    "    glue = None\n",
    "\n",
    "# Type hints\n",
    "from typing import Callable, Iterable, Optional\n",
    "from scipy.stats import rv_continuous as Distribution\n",
    "from earthkit.plots.geo.domains import Domain\n",
    "AnyDomain = (Domain | str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "This section defines some functions and variables used in the following analysis, allowing code cells in later sections to be shorter and ensuring consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "##### Data (pre-)processing\n",
    "The following functions handle [data chunking in dask](https://docs.xarray.dev/en/latest/user-guide/dask.html) for computational efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunk(data: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\" Re-chunk a dataset into pre-determined optimal chunks. \"\"\"\n",
    "    # Might need to be adjusted for different coordinate names\n",
    "    return data.chunk({\"valid_time\": -1, \"latitude\": 103, \"longitude\": 360})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The following functions restructure the ensemble members in the ERA5-Drought dataset along a new dimension, as in ERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### Accumulation periods\n",
    "The following cells contain constants and functions used in accumulating variables (e.g. precipitation) over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants such as the accumulation periods to use\n",
    "ACCUMULATION_PERIODS = [1, 3, 6, 12, 24, 36, 48]  # Months\n",
    "MONTHS = range(1, 13)  # January to December (inclusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-spi)=\n",
    "### 2. The ERA5-Drought ensemble: SPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Download data \n",
    "Describe what is done in this step/section and what the `code` in the cell does(if code is included).\n",
    "\n",
    "_In this assessment,\n",
    "we will calculate SPI and SPEI for each month\n",
    "(with different accumulation periods, see below)\n",
    "for the years 1940‚Äì2024._\n",
    "For the reference period,\n",
    "we will use the World Meteorological Organization (WMO) current standard 30-year reference period of 1991‚Äì2020,\n",
    "which is also used in ERA5-Drought.\n",
    "Both of these date ranges can be adjusted in the cell below when running the analysis yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your preferred analysis and reference periods\n",
    "years           = (1940, 2024)  # Years for the analysis (inclusive)\n",
    "years_reference = (1991, 2020)  # Years for the reference period (inclusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Having defined our target years, we can now define our CDS request.\n",
    "First, we define a template with some default parameters\n",
    "(e.g. years, data format)\n",
    "that will also be used later in the notebook.\n",
    "Additional information for specific downloads\n",
    "(e.g. variable, data stream)\n",
    "is mixed into this template where relevant.\n",
    "\n",
    "This notebook uses [earthkit-data](https://github.com/ecmwf/earthkit-data) to download files from the CDS.\n",
    "If you intend to run this notebook multiple times, it is highly recommended that you [enable caching](https://earthkit-data.readthedocs.io/en/latest/guide/caching.html) to prevent having to download the same files multiple times.\n",
    "If you prefer not to use earthkit, the following requests can also be used with the [cdsapi module](https://cds.climate.copernicus.eu/how-to-api#linux-use-client-step).\n",
    "In either case (earthkit-data or cdsapi), it is required to set up a CDS account and API key as explained [on the CDS website](https://cds.climate.copernicus.eu/how-to-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_ERA5_DROUGHT = \"derived-drought-historical-monthly\"\n",
    "\n",
    "request_era5drought_template = {\n",
    "    \"version\": \"1_0\",\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [f\"{year}\" for year in range(years[0], years[1]+1)],\n",
    "    \"month\": [f\"{month:02}\" for month in MONTHS],\n",
    "}\n",
    "\n",
    "request_era5drought_ensemble = {\n",
    "    \"product_type\": [\"ensemble_members\"],\n",
    "} | request_era5drought_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_ensemble_spi = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\"12\"],\n",
    "    # \"area\":[10, 40, 9, 41]\n",
    "} | request_era5drought_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5drought_ensemble_spi = ekd.from_source(\"cds\", ID_ERA5_DROUGHT, request_ensemble_spi)\n",
    "era5drought_ensemble_spi = era5drought_ensemble_spi.to_xarray(compat=\"equals\") # Convert to xarray\n",
    "era5drought_ensemble_spi  # Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Give each ensemble member unique identifier (one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = np.unique(era5drought_ensemble_spi[\"time\"], return_index=True)\n",
    "era5drought_ensemble_spi = [era5drought_ensemble_spi.isel(time = index + i) for i in range(10)]\n",
    "era5drought_ensemble_spi = xr.concat(era5drought_ensemble_spi, dim=\"number\").assign_coords(number=np.arange(10))\n",
    "era5drought_ensemble_spi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5drought_ensemble_spi.sel(time=\"2012-12-01\", method=\"nearest\").mean(\"number\")[\"SPI12\"].plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5drought_ensemble_spi.sel(time=\"2012-12-01\", method=\"nearest\").std(\"number\")[\"SPI12\"].plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Code under this cell needs to be adapted still."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Plotting each ensemble member vs time (one location, SPI-1) w/ thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = drought_ens['SPI12']  # select the DataArray\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "da.transpose('time', 'number').plot.line(x='time', hue='number', add_legend=True,\n",
    "    figsize=(12, 5),\n",
    ")\n",
    "plt.hlines(y=[-1.5, 1.5], xmin=drought_ens_mean['time'].values.min(), xmax=drought_ens_mean['time'].values.max(), colors='r', linestyles='--', \n",
    "           label = \"Thresholds\")\n",
    "# Customize plot\n",
    "plt.title('SPI12 ‚Äî Ensemble members over time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('SPI-12')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### Plot ensemble mean & standard dev (one location, SPI-1) w/ Thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_ens_mean = drought_ens.mean(dim=\"number\")\n",
    "drought_ens_std = drought_ens.std(dim=\"number\")/np.sqrt(10-1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(drought_ens_mean['time'].values, drought_ens_mean[\"SPI12\"].values, label=f'SPI12')\n",
    "\n",
    "# Two threshold lines\n",
    "plt.hlines(y=[-1.5, 1.5],\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='r', linestyles='--', label='Extreme Wet/Dry Thresholds')\n",
    "\n",
    "# Mean line\n",
    "mean_spi12 = drought_ens_mean[\"SPI12\"].mean().values\n",
    "plt.hlines(y=mean_spi12,\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='b', linestyles='--', label='Mean SPI12 across time.')\n",
    "\n",
    "\n",
    "plt.title('Ensemble mean of SPI12 vs Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('SPI-12 Time Series')\n",
    "plt.legend(title='12-month accumulation period')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### Number of SPI greater than threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = drought_ens[\"SPI12\"]  # dims: ('time',)\n",
    "\n",
    "threshold_mag = 2\n",
    "\n",
    "exceeds = np.abs(da) > threshold_mag # boolean xarray with all values that are \"severely\" wet/drier than usual.\n",
    "\n",
    "count_per_time = exceeds.sum(dim='number') # count number that exceed along the ensemble dimension.\n",
    "\n",
    "fractional_count_per_time = count_per_time / 10\n",
    "\n",
    "percent_per_time = fractional_count_per_time*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Plot of Mean, Stdev & P( > Severe Threshold ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# --- Top panel: % members exceeding ---\n",
    "percent_per_time.plot(ax=ax1, color='tab:purple')\n",
    "ax1.set_title(f'% ensemble members with |SPI12| > {threshold_mag} per time')\n",
    "ax1.set_ylabel('Percent (%)')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- 2nd panel: Ensemble band ---\n",
    "ci = drought_ens_std['SPI12'].values*np.sqrt(10-1) # confidence interval\n",
    "y = drought_ens_mean['SPI12'].values\n",
    "t = drought_ens_mean['time'].values\n",
    "\n",
    "ax2.plot(t,y,label='SPI12 Mean', color='tab:blue')\n",
    "\n",
    "# Horizontal thresholds ¬±2\n",
    "ax2.hlines(y=[-threshold_mag, threshold_mag],\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='r', linestyles='--', label='Extreme Wet/Dry Thresholds')\n",
    "\n",
    "ax2.set_title('Mean SPI12 vs Time')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Mean SPI-12')\n",
    "ax2.legend(title='12-month accumulation period')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# --- 3nd panel: Ensemble band ---\n",
    "ci = drought_ens_std['SPI12'].values*np.sqrt(10-1) # confidence interval\n",
    "y = drought_ens_mean['SPI12'].values\n",
    "t = drought_ens_mean['time'].values\n",
    "\n",
    "# ax2.plot(t,y.values,label='SPI1 Mean', color='tab:blue')\n",
    "ax3.fill_between(t, (y-ci), (y+ci), color='b')\n",
    "\n",
    "# Horizontal thresholds ¬±1.5\n",
    "ax3.hlines(y=[-threshold_mag, threshold_mag],\n",
    "           xmin=drought_ens_mean['time'].values.min(),\n",
    "           xmax=drought_ens_mean['time'].values.max(),\n",
    "           colors='r', linestyles='--', label='Extreme Wet/Dry Thresholds')\n",
    "\n",
    "ax3.set_title('Mean SPI12 vs Time with St.dev Band')\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Mean SPI-12')\n",
    "ax3.legend(title='12-month accumulation period')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Heat map of probability exceeding threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "t = percent_per_time['time'].values\n",
    "y = percent_per_time.values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 2.8))\n",
    "\n",
    "# Create a 2D array with one \"row\" so imshow can display it\n",
    "heat = y[None, :]  # shape (1, T)\n",
    "\n",
    "# Use extent to align x with datetime range\n",
    "t_num = mdates.date2num(t)\n",
    "extent = (t_num.min(), t_num.max(), 0, 1)\n",
    "\n",
    "im = ax.imshow(\n",
    "    heat,\n",
    "    aspect='auto',\n",
    "    cmap='RdBu_r',\n",
    "    vmin=0, vmax=100,\n",
    "    extent=extent\n",
    ")\n",
    "\n",
    "ax.set_yticks([])  # hide vertical axis (it's a strip)\n",
    "ax.set_title('% ensemble members with |SPI12| > 1.5 (timeline color strip)')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "# Ticks on time\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax, pad=0.02)\n",
    "cbar.set_label('Percent exceeding (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of |Mean(SPI)| > 1.5  & P( |SPI| > 1.5 ) > 0.5 \n",
    "\n",
    "####  P( |SPI| > 1.5 )  calculated from # of ensemble members greater than threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = drought_ens_mean[\"SPI12\"]  # dims: ('time',)\n",
    "\n",
    "probability_mag = 0.5 \n",
    "\n",
    "cond_both = (np.abs(da > threshold_mag) & (fractional_count_per_time > probability_mag))\n",
    "\n",
    "da_hit = da.sel(time=cond_both)\n",
    "\n",
    "timestamps = da_hit.time.values\n",
    "\n",
    "print(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Histogram ensemble standard dev vs # quality flag (one location, one window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drought_ens_std[\"SPI12\"]\n",
    "sig = quality_spi_all[\"significance_12\"]\n",
    "\n",
    "thresholds = np.arange(0.1, 1.4, 0.1)  # 0.1 .. 1.5\n",
    "bad_counts_cum = np.zeros_like(thresholds, dtype=int)\n",
    "good_counts_cum = np.zeros_like(thresholds, dtype=int)\n",
    "\n",
    "for month in range(1,13):\n",
    "        significance = quality_spi_all[f\"significance_1\"].sel(time=f\"2020-{month:02d}-01\").compute().item()\n",
    "        if significance == 0:\n",
    "            vals = data.where(data.time.dt.month == month, drop=True).values\n",
    "            bad_counts_cum += (vals[:, None] < thresholds[None, :]).sum(axis=0) # then sum along column.\n",
    "            # bad_counts_cum[j] = number of vals that are < thresholds[j]\n",
    "            # B[i, j] = (vals[i] < thresholds[j])\n",
    "        else:\n",
    "            vals = data.where(data.time.dt.month == month, drop=True).values\n",
    "            good_counts_cum += (vals[:, None] < thresholds[None, :]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_bins  = np.diff(np.concatenate(([0], bad_counts_cum)))\n",
    "good_bins = np.diff(np.concatenate(([0], good_counts_cum)))\n",
    "total_bins = bad_bins + good_bins\n",
    "good_norm = good_bins / total_bins\n",
    "bad_norm = bad_bins / total_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(thresholds, good_norm, width = 0.1, color='blue', alpha=0.8, label='Quality = 1 (good)')\n",
    "ax.bar(thresholds, bad_norm, width = 0.1,  color='red', bottom = good_norm, alpha=0.9, label='Quality = 0 (bad)')\n",
    "\n",
    "ax.set_xlabel('Uncertainty in SPI1 (0.1 bin)')\n",
    "ax.set_ylabel('% count of good/bad months (stacked)')\n",
    "ax.set_title('% counts (stacked) for each uncertainty bin')\n",
    "ax.set_xticks(thresholds)\n",
    "ax.set_xticklabels([f'{t:.1f}' for t in thresholds])\n",
    "ax.grid(True, linestyle='--', alpha=0.35)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Reading in ERA5 SPI- Ensemble (global, 2 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"derived-drought-historical-monthly\"\n",
    "request = {\n",
    "    \"variable\": [\"standardised_precipitation_index\"],\n",
    "    \"accumulation_period\": [\"48\"],\n",
    "    \"version\": \"1_0\",\n",
    "    \"product_type\": [\"ensemble_members\"],\n",
    "    \"dataset_type\": \"consolidated_dataset\",\n",
    "    \"year\": [\n",
    "        \"2023\",\n",
    "        \"2024\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drought_ens = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "data_drought_ens = data_drought_ens.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "#### Function to create the data into ensemble set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reshape time into (time=24, ensemble=10)\n",
    "def make_ensemble(spi):\n",
    "    arr = np.asarray(spi)  # Convert memoryview or dask chunk to NumPy\n",
    "    reshaped = arr.reshape(24, 10)\n",
    "    return reshaped\n",
    " \n",
    "# Apply across all lat/lon\n",
    "ensemble = xr.apply_ufunc(\n",
    "    make_ensemble,\n",
    "    data_drought_ens[\"SPI48\"],  # DataArray with dims (time, lat, lon)\n",
    "    input_core_dims=[[\"time\"]],\n",
    "    output_core_dims=[[\"time\", \"ensemble\"]],\n",
    "    exclude_dims={\"time\"},\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[float],\n",
    "    dask_gufunc_kwargs={\"allow_rechunk\": True},\n",
    "    output_sizes={\"time\": 24, \"ensemble\": 10},\n",
    ")\n",
    " \n",
    "# Assign coordinates\n",
    "ensemble = ensemble.assign_coords({\n",
    "    \"time\": pd.date_range(\"2023-01-01\", periods=24, freq=\"MS\"),\n",
    "    \"ensemble\": range(1, 11)\n",
    "})\n",
    "ensemble.name = \"SPI48\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Selecting ensemble member 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.sel(ensemble=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_values = ensemble.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### Calculating mean and stdev over all ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean = ensemble.mean(dim=\"ensemble\", skipna=True)  # shape: (lat, lon, time)\n",
    "ensemble_std = ensemble.std(dim=\"ensemble\", skipna=True)  # shape: (lat, lon, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean_time = abs(ensemble_mean).mean(dim=\"time\", skipna=True)  # shape: (lat, lon, time)\n",
    "ensemble_std_time = ensemble_std.std(dim=\"time\", skipna=True)  # shape: (lat, lon, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_std = ensemble_std.compute()  # Load into memory for plotting\n",
    "ensemble_std_time = ensemble_std_time.compute() \n",
    "\n",
    "ensemble_mean = ensemble_mean.compute()  # Load into memory for plotting\n",
    "ensemble_mean_time = ensemble_mean_time.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Plot of mean vs stdev over all ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one time slice and one ensemble\n",
    "from earthkit.plots.styles import Style\n",
    "\n",
    "global_std_map = ensemble_std.sel(time=\"2024-05-01\")  # shape (lat, lon)\n",
    "global_mean_map = ensemble_mean.sel(time=\"2024-05-01\")\n",
    "\n",
    "# global_std_map = ensemble_std_time\n",
    "# global_mean_map = ensemble_mean_time\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "\n",
    "std_values = global_std_map.to_numpy()/np.sqrt(10)\n",
    "mean_values = global_mean_map.to_numpy()\n",
    "\n",
    "lat_values = global_mean_map.lat.to_numpy()\n",
    "lon_values = global_mean_map.lon.to_numpy()\n",
    " \n",
    "# Create meshgrid\n",
    "\n",
    "lon_grid, lat_grid = np.meshgrid(lon_values, lat_values)\n",
    " \n",
    "# # Plot with EarthKit\n",
    "\n",
    "SPI_STYLE = Style(cmap='RdBu', vmin = -3, vmax = 3, normalize=False)\n",
    "SPI_SEOM_STYLE = Style(cmap='cividis', vmin = 0, vmax = 0.5, normalize=False) # _r for reversing colorbar.\n",
    "\n",
    "# Create figure with 2 columns\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))  # <-- Important!\n",
    "\n",
    "# First subplot (left)\n",
    "subplot = fig.add_map(domain=\"Australia\", row=0, column=0)\n",
    "subplot.grid_cells(mean_values, x=lon_grid, y=lat_grid,style=SPI_STYLE)\n",
    "subplot.legend(location=\"right\")\n",
    "\n",
    "# Second subplot (right)\n",
    "subplot1 = fig.add_map(domain=\"Australia\", row=0, column=1)\n",
    "subplot1.grid_cells(std_values, x=lon_grid, y=lat_grid, style = SPI_SEOM_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "\n",
    "subplot.title(\"Mean | SPI | \")\n",
    "subplot1.title(\"SPI SEOM\")\n",
    "\n",
    "# Add decorations\n",
    "fig.title(\"SPI Ensemble (N=10) averaged across 2023-2024 month, 1 month accumulation period.\")\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### Plot of P( |SPI48| >= 2 ) over entire region, for one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ensemble  # dims: ('time',)\n",
    "\n",
    "threshold_mag = 2\n",
    "\n",
    "exceeds = np.abs(da) >= threshold_mag # boolean xarray with all values that are \"severely\" wet/drier than usual.\n",
    "\n",
    "count_per_time = exceeds.sum(dim='ensemble') # count number that exceed along the ensemble dimension.\n",
    "\n",
    "fractional_count_per_time = count_per_time / 10\n",
    "\n",
    "percent_per_time = fractional_count_per_time*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from earthkit.plots.styles import Style\n",
    "\n",
    "# --- Define mask FIRST ---\n",
    "extreme_mask = (mean_values >= 2) | (mean_values <= -2)\n",
    "\n",
    "# Build a constant-valued mask for the grey layer:\n",
    "# 1 where non-extreme (|SPI| ‚â§ 2), NaN where extreme (so extremes won't be drawn in grey)\n",
    "nonextreme_const = np.where(~extreme_mask, 1.0, np.nan)\n",
    "\n",
    "# For the colored overlay, keep only extreme values (NaN elsewhere)\n",
    "mean_extremes = np.where(extreme_mask, mean_values, np.nan)\n",
    "\n",
    "# --- Styles ---\n",
    "# Single-colour style for non-extremes: ensure a tight 0‚Äì1 range\n",
    "GREY_STYLE = Style(\n",
    "    cmap=ListedColormap([\"#d9d9d9\"]),\n",
    "    vmin=0, vmax=1,\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# SPI style for extremes; use a symmetric range that suits your data\n",
    "SPI_STYLE = Style(\n",
    "    cmap='RdBu',\n",
    "    vmin=-4, vmax=4,    # adjust to your expected SPI range\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# --- Plot ---\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))\n",
    "\n",
    "# Left subplot: mean SPI48 with grey background for non-extremes\n",
    "subplot = fig.add_map(domain=\"Australia\", row=0, column=0)\n",
    "\n",
    "# Draw grey background first\n",
    "subplot.grid_cells(\n",
    "    nonextreme_const, x=lon_grid, y=lat_grid,\n",
    "    style=GREY_STYLE, zorder=1, alpha=0.8  # alpha to ensure visibility\n",
    ")\n",
    "\n",
    "# Overlay extremes with color\n",
    "subplot.grid_cells(\n",
    "    mean_extremes, x=lon_grid, y=lat_grid,\n",
    "    style=SPI_STYLE, zorder=2\n",
    ")\n",
    "\n",
    "subplot.legend(location=\"right\")\n",
    "\n",
    "\n",
    "subplot.title(\"|Mean SPI48| ‚â• 2 | (grey = |Mean SPI| ‚â§ 2)\")\n",
    "\n",
    "longitude = [142]\n",
    "latitude = [-29.25]\n",
    "\n",
    "# Plot the point as a scatter plot\n",
    "subplot.scatter(x=longitude, y=latitude, color='red', marker='o', s=100, label = \"Tibooburra\")\n",
    "\n",
    "# Right subplot (unchanged example)\n",
    "SPI_SEOM_STYLE = Style(cmap='cividis', normalize=False)\n",
    "subplot1 = fig.add_map(domain=\"Australia\", row=0, column=1)\n",
    "subplot1.grid_cells(percent_values, x=lon_grid, y=lat_grid, style=SPI_SEOM_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "subplot1.title(\"P(| SPI48 | > 2)\")\n",
    "subplot1.scatter(x=longitude, y=latitude, color='red', marker='o', s=100, label = \"Tibooburra\")\n",
    "\n",
    "fig.title(\"SPI48 Ensemble (N=10) with Tibooburra pointed out.\")\n",
    "fig.land(); fig.coastlines(); fig.borders(); fig.gridlines()\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Analysing uncertainty correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_mean = ensemble_mean_time\n",
    "spi_seom = ensemble_std_time / np.sqrt(10)\n",
    "\n",
    "spi_mean = np.where((np.abs(spi_mean) <= 3) , spi_mean, np.nan)\n",
    "spi_seom = np.where((np.abs(spi_mean) <= 3), spi_seom, np.nan)\n",
    "\n",
    "# Flatten & mask\n",
    "x = spi_mean.ravel()          # SPI mean\n",
    "y = spi_seom.ravel()   # SPI standard uncertainty (N=10)\n",
    "mask = np.isfinite(x) & np.isfinite(y)\n",
    "x = x[mask]; y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 6), dpi=160)\n",
    "plt.scatter(x, y, s=8, alpha=0.5, color=\"#1f77b4\", edgecolors=\"none\")\n",
    "plt.xlabel(\"Mean | SPI-1 |\")\n",
    "plt.ylabel(\"SPI-1 Standard Uncertainty of the mean (N=10)\")\n",
    "plt.title(\"Mean | SPI-1 | vs SPI-1 SUOM for SPI <= 3 across 2023-2024\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Masks (ignore NaNs in x or y)\n",
    "mask_neg = (x < 0) & np.isfinite(x) & np.isfinite(y)\n",
    "mask_pos = (x >= 0) & np.isfinite(x) & np.isfinite(y)\n",
    "\n",
    "n_neg = np.count_nonzero(mask_neg)\n",
    "n_pos = np.count_nonzero(mask_pos)\n",
    "\n",
    "# Compute means; if group empty, set to None\n",
    "mean_y_neg = float(np.nanmean(y[mask_neg])) if n_neg > 0 else None\n",
    "mean_y_pos = float(np.nanmean(y[mask_pos])) if n_pos > 0 else None\n",
    "\n",
    "# Helper for formatting\n",
    "def fmt_mean(m):\n",
    "    return f\"{m:.4f}\" if m is not None and np.isfinite(m) else \"NA\"\n",
    "\n",
    "print(\"Group summaries:\")\n",
    "print(f\"  x < 0    -> n = {n_neg:4d}, mean(y) = {fmt_mean(mean_y_neg)}\")\n",
    "print(f\"  x >= 0   -> n = {n_pos:4d}, mean(y) = {fmt_mean(mean_y_pos)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- your scatter (unchanged) ---\n",
    "# Assign colors based on sign of x\n",
    "colors = np.where(x < 0, 'red', 'blue')\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=160)\n",
    "plt.scatter(\n",
    "    x, y,\n",
    "    s=8,\n",
    "    alpha=0.5,\n",
    "    color=colors,\n",
    "    edgecolors='none'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Mean SPI-1\")\n",
    "plt.ylabel(\"SPI-1 Standard Uncertainty of the mean (N=10)\")\n",
    "plt.title(\"Mean SPI-1 vs SPI-1 SUOM for SPI <= 3 across 2023-2024\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- compute means (ignore NaNs in x or y) ---\n",
    "mask_neg = (x < 0) & np.isfinite(x) & np.isfinite(y)\n",
    "mask_pos = (x >= 0) & np.isfinite(x) & np.isfinite(y)\n",
    "\n",
    "n_neg = int(np.count_nonzero(mask_neg))\n",
    "n_pos = int(np.count_nonzero(mask_pos))\n",
    "\n",
    "mean_y_neg = float(np.nanmean(y[mask_neg])) if n_neg > 0 else None\n",
    "mean_y_pos = float(np.nanmean(y[mask_pos])) if n_pos > 0 else None\n",
    "\n",
    "def fmt_mean(m):\n",
    "    return f\"{m:.4f}\" if (m is not None and np.isfinite(m)) else \"NA\"\n",
    "\n",
    "# --- print on the plot as text boxes ---\n",
    "ax = plt.gca()\n",
    "\n",
    "# Left text box (x < 0)\n",
    "left_text = f\"SPI-1 < 0:\\n  n = {n_neg}\\n  mean(SPI SUOM) = {fmt_mean(mean_y_neg)}\"\n",
    "ax.text(\n",
    "    0.02, 0.98, left_text,\n",
    "    transform=ax.transAxes,\n",
    "    va='top', ha='left',\n",
    "    color='red',\n",
    "    fontsize=10,\n",
    "    bbox=dict(facecolor='white', edgecolor='red', alpha=0.9, boxstyle='round,pad=0.3')\n",
    ")\n",
    "\n",
    "# Right text box (x >= 0)\n",
    "right_text = f\"SPI-1 >= 0:\\n  n = {n_pos}\\n  mean(SPI SUOM) = {fmt_mean(mean_y_pos)}\"\n",
    "ax.text(\n",
    "    0.98, 0.98, right_text,\n",
    "    transform=ax.transAxes,\n",
    "    va='top', ha='right',\n",
    "    color='blue',\n",
    "    fontsize=10,\n",
    "    bbox=dict(facecolor='white', edgecolor='blue', alpha=0.9, boxstyle='round,pad=0.3')\n",
    ")\n",
    "\n",
    "# Optional: vertical reference line at x = 0\n",
    "ax.axvline(0, color='k', linestyle=':', linewidth=1.2, alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of mean ensemble Standard Uncertainty of the Mean vs Time (Australia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------\n",
    "# 1) Global mean time series\n",
    "# -----------------------\n",
    "# Simple average over lat/lon for each time\n",
    "global_ts = ensemble_std.mean(dim=(\"lat\", \"lon\"), skipna=True)\n",
    "global_ts.name = \"global_mean_std\"\n",
    "\n",
    "# -----------------------\n",
    "# 2) Australia domain mean time series\n",
    "# -----------------------\n",
    "# Define a rectangular box that covers mainland Australia\n",
    "# Adjust if you want to include territories or use a different extent.\n",
    "AU_LAT_MIN, AU_LAT_MAX = -45.0, -5.0\n",
    "AU_LON_MIN, AU_LON_MAX = 100.0, 160.0\n",
    "\n",
    "# If your longitude coordinate is 0..360, these bounds already work.\n",
    "# If it's -180..180, they also work (Australia is at positive longitudes).\n",
    "\n",
    "lat_sel = (ensemble_std[\"lat\"] >= AU_LAT_MIN) & (ensemble_std[\"lat\"] <= AU_LAT_MAX)\n",
    "lon_sel = (ensemble_std[\"lon\"] >= AU_LON_MIN) & (ensemble_std[\"lon\"] <= AU_LON_MAX)\n",
    "\n",
    "ensemble_std_au = ensemble_std.where(lat_sel & lon_sel, drop=True)\n",
    "aus_ts = ensemble_std_au.mean(dim=(\"lat\", \"lon\"), skipna=True)\n",
    "aus_ts.name = \"australia_mean_std\"\n",
    "\n",
    "# -----------------------\n",
    "# 3) Plot\n",
    "# -----------------------\n",
    "plt.figure(figsize=(10, 5), dpi=140)\n",
    "plt.plot(global_ts[\"time\"], global_ts, label=\"Global mean std\", lw=1.8)\n",
    "plt.plot(aus_ts[\"time\"], aus_ts, label=\"Australia mean std\", lw=1.8)\n",
    "plt.title(\"Mean Ensemble Standard Deviation ‚Äî Global vs Australia (unweighted)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Standard deviation\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "pltplt.show()\n",
    "\n",
    "# -----------------------\n",
    "# 4) Optional: Export as DataFrame\n",
    "# -----------------------\n",
    "df = xr.merge([global_ts, aus_ts]).to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.245188,
     "end_time": "2024-03-08T17:39:21.277354",
     "exception": false,
     "start_time": "2024-03-08T17:39:21.032166",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-5)=\n",
    "### 5. Applying the Ensemble Data\n",
    "\n",
    "#### Results Subsections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Importing in GH Precipitation Station locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import io\n",
    "\n",
    "# --- Helper to download text ---\n",
    "def read_text(primary_url: str, fallback_url: str = None, *, timeout: int = 30) -> str:\n",
    "    last_err = None\n",
    "    for url in [primary_url, fallback_url]:\n",
    "        if not url:\n",
    "            continue\n",
    "        try:\n",
    "            with urlopen(url, timeout=timeout) as r:\n",
    "                return r.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed to download text from provided URLs. Last error: {last_err}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "DOMAIN = \"Australia\"\n",
    "GRID_DEG = 0.25\n",
    "USE_LOCAL = False\n",
    "LOCAL_STATIONS = \"./ghcnd-stations.txt\"\n",
    "LOCAL_INVENTORY = \"./ghcnd-inventory.txt\"\n",
    "\n",
    "NCEI_STATIONS_URL  = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "NCEI_INVENTORY_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n",
    "AWS_STATIONS_URL   = \"https://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\"\n",
    "AWS_INVENTORY_URL  = \"https://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-inventory.txt\"\n",
    "\n",
    "ACTIVE_YEAR = 2024  # <-- only keep stations operating in this year\n",
    "\n",
    "# --- SPI grid you've prepared earlier ---\n",
    "global_std_map  = ensemble_std_time  # (lat, lon)\n",
    "lat_values      = global_std_map.lat.to_numpy()\n",
    "lon_values      = global_std_map.lon.to_numpy()\n",
    "std_values      = global_std_map.to_numpy() / np.sqrt(10)\n",
    "lon_grid, lat_grid = np.meshgrid(lon_values, lat_values)\n",
    "\n",
    "# --- Helpers ---\n",
    "def to_year(s):\n",
    "    \"\"\"Extract a 4-digit year from inventory 'first'/'last' fields.\"\"\"\n",
    "    if s is None:\n",
    "        return np.nan\n",
    "    s = str(s).strip()\n",
    "    digits = ''.join(c for c in s if c.isdigit())\n",
    "    return int(digits[:4]) if len(digits) >= 4 else np.nan\n",
    "\n",
    "# --- 1) Read GHCNd station & inventory text files ---\n",
    "if USE_LOCAL and Path(LOCAL_STATIONS).exists():\n",
    "    stations_text = Path(LOCAL_STATIONS).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "else:\n",
    "    stations_text = read_text(NCEI_STATIONS_URL, AWS_STATIONS_URL)\n",
    "\n",
    "if USE_LOCAL and Path(LOCAL_INVENTORY).exists():\n",
    "    inventory_text = Path(LOCAL_INVENTORY).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "else:\n",
    "    inventory_text = read_text(NCEI_INVENTORY_URL, AWS_INVENTORY_URL)\n",
    "\n",
    "# Parse station metadata: ID, lat, lon\n",
    "stations_df = pd.read_csv(\n",
    "    io.StringIO(stations_text),\n",
    "    sep=r\"\\s+\",\n",
    "    header=None,\n",
    "    usecols=[0, 1, 2],\n",
    "    names=[\"station_id\", \"lat\", \"lon\"],\n",
    "    dtype={\"station_id\": str, \"lat\": float, \"lon\": float},\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "# Parse inventory: id, lat, lon, element, first, last\n",
    "inv_df = pd.read_csv(\n",
    "    io.StringIO(inventory_text),\n",
    "    sep=r\"\\s+\",\n",
    "    header=None,\n",
    "    names=[\"station_id\", \"lat_inv\", \"lon_inv\", \"element\", \"first\", \"last\"],\n",
    "    dtype={\"station_id\": str, \"lat_inv\": float, \"lon_inv\": float, \"element\": str, \"first\": str, \"last\": str},\n",
    "    engine=\"python\"\n",
    ")\n",
    "\n",
    "# Keep PRCP entries and compute begin/end years\n",
    "inv_prcp = inv_df[inv_df[\"element\"] == \"PRCP\"].copy()\n",
    "inv_prcp[\"first_year\"] = inv_prcp[\"first\"].apply(to_year)\n",
    "inv_prcp[\"last_year\"]  = inv_prcp[\"last\"].apply(to_year)\n",
    "\n",
    "# --- Filter: stations operating during ACTIVE_YEAR ---\n",
    "inv_prcp_active = inv_prcp[\n",
    "    (inv_prcp[\"first_year\"] <= ACTIVE_YEAR) & (inv_prcp[\"last_year\"] >= ACTIVE_YEAR)\n",
    "].copy()\n",
    "\n",
    "# Deduplicate station IDs and merge coordinates from stations file\n",
    "prcp_stations_active = (\n",
    "    inv_prcp_active[[\"station_id\"]]\n",
    "    .drop_duplicates()\n",
    "    .merge(stations_df, on=\"station_id\", how=\"left\")\n",
    "    .dropna(subset=[\"lat\", \"lon\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"# Active PRCP stations (operating in {ACTIVE_YEAR}): {len(prcp_stations_active)}\")\n",
    "\n",
    "# --- 2) Bin active PRCP stations onto a 0.25¬∞ grid ---\n",
    "lat_edges = np.arange(-90.0,  90.0 + GRID_DEG, GRID_DEG)\n",
    "lon_edges = np.arange(-180.0, 180.0 + GRID_DEG, GRID_DEG)\n",
    "H, _, _ = np.histogram2d(\n",
    "    prcp_stations_active[\"lat\"].values,\n",
    "    prcp_stations_active[\"lon\"].values,\n",
    "    bins=[lat_edges, lon_edges]\n",
    ")\n",
    "\n",
    "lat_centers = (lat_edges[:-1] + lat_edges[1:]) / 2\n",
    "lon_centers = (lon_edges[:-1] + lon_edges[1:]) / 2\n",
    "counts_da = xr.DataArray(H, coords={\"lat\": lat_centers, \"lon\": lon_centers}, dims=(\"lat\", \"lon\"))\n",
    "\n",
    "# Interpolate counts to your SPI grid (nearest keeps integer counts)\n",
    "counts_on_spi = counts_da.interp(lat=lat_values, lon=lon_values, method=\"nearest\")\n",
    "gauge_values  = counts_on_spi.to_numpy()\n",
    "\n",
    "# --- 3) Styles ---\n",
    "vmax_est = max(10, float(np.nanpercentile(gauge_values, 99)))\n",
    "GAUGE_STYLE    = Style(cmap=\"cividis\", vmin=0, vmax=vmax_est, normalize=False)\n",
    "SPI_SEOM_STYLE = Style(cmap=\"cividis\", normalize=False)\n",
    "\n",
    "# --- 4) EarthKit figure (side-by-side) ---\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))\n",
    "\n",
    "# LEFT: active PRCP station counts\n",
    "subplot_left = fig.add_map(domain=DOMAIN, row=0, column=0)\n",
    "subplot_left.grid_cells(gauge_values, x=lon_grid, y=lat_grid, style=GAUGE_STYLE)\n",
    "subplot_left.legend(location=\"right\")\n",
    "subplot_left.title(f\"GHCNd PRCP stations active in {ACTIVE_YEAR} per 0.25¬∞ cell\")\n",
    "\n",
    "# RIGHT: SPI standard uncertainty (N=10)\n",
    "subplot_right = fig.add_map(domain=DOMAIN, row=0, column=1)\n",
    "subplot_right.grid_cells(std_values, x=lon_grid, y=lat_grid, style=SPI_SEOM_STYLE)\n",
    "subplot_right.legend(location=\"right\")\n",
    "subplot_right.title(\"Mean SPI Standard Uncertainty across 2023‚Äì2024 (N=10)\")\n",
    "\n",
    "fig.land(); fig.coastlines(); fig.borders(); fig.gridlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from urllib.request import urlopen\n",
    "\n",
    "# --- Configuration ---\n",
    "USE_LOCAL = False\n",
    "LOCAL_STATIONS  = \"./ghcnd-stations.txt\"\n",
    "LOCAL_INVENTORY = \"./ghcnd-inventory.txt\"\n",
    "\n",
    "# Official NOAA endpoints (AWS mirror fallback)\n",
    "NCEI_STATIONS_URL  = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "NCEI_INVENTORY_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n",
    "AWS_STATIONS_URL   = \"https://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt\"\n",
    "AWS_INVENTORY_URL  = \"https://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-inventory.txt\"\n",
    "\n",
    "# Domain selection\n",
    "USE_COUNTRY_CODES = True  # default: filter by GHCN country codes\n",
    "AU_CODES = (\"AS\", \"CK\", \"KT\", \"NF\")  # Australia + territories (per GHCN countries list)\n",
    "\n",
    "def to_year(s):\n",
    "    \"\"\"Extract 4-digit year from 'YYYY' or 'YYYYMMDD' strings; return np.nan if not parseable.\"\"\"\n",
    "    if s is None:\n",
    "        return np.nan\n",
    "    s = str(s).strip()\n",
    "    digits = re.sub(r\"[^0-9]\", \"\", s)\n",
    "    return int(digits[:4]) if len(digits) >= 4 else np.nan\n",
    "\n",
    "# --- 1) Load stations & inventory (GHCN-Daily) ---\n",
    "if USE_LOCAL and Path(LOCAL_STATIONS).exists():\n",
    "    stations_text = Path(LOCAL_STATIONS).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "else:\n",
    "    stations_text = read_text(NCEI_STATIONS_URL, AWS_STATIONS_URL)\n",
    "\n",
    "if USE_LOCAL and Path(LOCAL_INVENTORY).exists():\n",
    "    inventory_text = Path(LOCAL_INVENTORY).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "else:\n",
    "    inventory_text = read_text(NCEI_INVENTORY_URL, AWS_INVENTORY_URL)\n",
    "\n",
    "# Stations file (metadata; we‚Äôll use ID+coords for country-code filter cross-check if desired)\n",
    "stations_df = pd.read_csv(\n",
    "    io.StringIO(stations_text),\n",
    "    sep=r\"\\s+\",\n",
    "    header=None,\n",
    "    usecols=[0, 1, 2],\n",
    "    names=[\"station_id\", \"lat\", \"lon\"],\n",
    "    dtype={\"station_id\": str, \"lat\": float, \"lon\": float},\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# Inventory: id, lat, lon, element, first, last (period-of-record per element)\n",
    "inv_df = pd.read_csv(\n",
    "    io.StringIO(inventory_text),\n",
    "    sep=r\"\\s+\",\n",
    "    header=None,\n",
    "    names=[\"station_id\", \"lat_inv\", \"lon_inv\", \"element\", \"first\", \"last\"],\n",
    "    dtype={\"station_id\": str, \"lat_inv\": float, \"lon_inv\": float, \"element\": str, \"first\": str, \"last\": str},\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# Keep PRCP entries and derive begin/end years\n",
    "inv_prcp = inv_df[inv_df[\"element\"] == \"PRCP\"].copy()\n",
    "inv_prcp[\"first_year\"] = inv_prcp[\"first\"].apply(to_year)\n",
    "inv_prcp[\"last_year\"]  = inv_prcp[\"last\"].apply(to_year)\n",
    "\n",
    "# Drop any rows with invalid years or inverted ranges\n",
    "inv_prcp = inv_prcp[np.isfinite(inv_prcp[\"first_year\"]) & np.isfinite(inv_prcp[\"last_year\"])]\n",
    "inv_prcp = inv_prcp[inv_prcp[\"last_year\"] >= inv_prcp[\"first_year\"]].copy()\n",
    "\n",
    "# Optional geographic bounding box (set to True to use instead of country codes)\n",
    "USE_BBOX = False\n",
    "AU_BBOX = dict(lat_min=-45, lat_max=-5, lon_min=100, lon_max=160)\n",
    "\n",
    "# --- Helpers ---\n",
    "# --- 2) Domain filter (Australia) ---\n",
    "if USE_COUNTRY_CODES:\n",
    "    # Filter by GHCN ID prefix: first two letters denote FIPS country code (e.g., 'AS' for Australia)\n",
    "    # Also include Australian territories: CK (Cocos-Keeling), KT (Christmas Island), NF (Norfolk Island).\n",
    "    mask_cc = inv_prcp[\"station_id\"].str.slice(0, 2).isin(AU_CODES)\n",
    "    au_prcp = inv_prcp[mask_cc].copy()\n",
    "else:\n",
    "    au_prcp = inv_prcp.copy()\n",
    "\n",
    "if USE_BBOX:\n",
    "    # Spatial clip (lat/lon from inventory)\n",
    "    m = (\n",
    "        (au_prcp[\"lat_inv\"] >= AU_BBOX[\"lat_min\"]) & (au_prcp[\"lat_inv\"] <= AU_BBOX[\"lat_max\"]) &\n",
    "        (au_prcp[\"lon_inv\"] >= AU_BBOX[\"lon_min\"]) & (au_prcp[\"lon_inv\"] <= AU_BBOX[\"lon_max\"])\n",
    "    )\n",
    "    au_prcp = au_prcp[m].copy()\n",
    "\n",
    "# --- 3) Count active stations per year ---\n",
    "if au_prcp.empty:\n",
    "    raise ValueError(\"No PRCP stations found for the Australia domain with the chosen filter.\")\n",
    "\n",
    "year_min = int(au_prcp[\"first_year\"].min())\n",
    "year_max = int(au_prcp[\"last_year\"].max())\n",
    "years = np.arange(year_min, year_max + 1)\n",
    "\n",
    "# Vectorized counting:\n",
    "# For each station: active in year Y if first_year <= Y <= last_year\n",
    "firsts = au_prcp[\"first_year\"].to_numpy()\n",
    "lasts  = au_prcp[\"last_year\"].to_numpy()\n",
    "\n",
    "counts = []\n",
    "for y in years:\n",
    "    counts.append(np.sum((firsts <= y) & (lasts >= y)))\n",
    "counts = np.array(counts, dtype=int)\n",
    "\n",
    "# --- 4) Plot ---\n",
    "plt.figure(figsize=(10, 5), dpi=160)\n",
    "plt.plot(years, counts, color=\"#1f77b4\", lw=1.8)\n",
    "plt.title(\"Active GHCNd PRCP Stations per Year ‚Äî Australia domain\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of active stations\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Elevation vs SPI uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-land\"\n",
    "request = {\n",
    "    \"variable\": [\"geopotential\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"unarchived\"\n",
    "}\n",
    "\n",
    "geo_pot = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "geo_pot = geo_pot.to_xarray() # Converts to xarray.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPI_SEOM_STYLE = Style(cmap=\"cividis\", vmin=0, vmax=0.5, normalize=False) # _r for reversing colorbar.\n",
    "\n",
    "RB_STYLE = Style(cmap=\"cividis\", normalize=False)  # let data drive the range\n",
    "\n",
    "geo_pot[\"h\"] = geo_pot[\"z\"]/9.81\n",
    "\n",
    "global_map = geo_pot[\"h\"] \n",
    "\n",
    "global_map.name = \"height\"\n",
    "fig = ekp.Figure(rows=1, columns=1, size=(8, 6))\n",
    "\n",
    "global_map.attrs[\"units\"] = \"m\"\n",
    "global_map.attrs[\"long_name\"] = \"Height above sea level\"\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "global_std_map = ensemble_std_time\n",
    "std_values = global_std_map.to_numpy()/np.sqrt(10)\n",
    "lat_values = global_std_map.lat.to_numpy()\n",
    "lon_values = global_std_map.lon.to_numpy()\n",
    " \n",
    "# Create meshgrid\n",
    "\n",
    "lon_grid, lat_grid = np.meshgrid(lon_values, lat_values)\n",
    " \n",
    "# # Plot with EarthKit\n",
    "\n",
    "# Create figure with 2 columns\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))  # <-- Important!\n",
    "\n",
    "# First subplot (left)\n",
    "subplot = fig.add_map(domain=\"Australia\", row=0, column=0)\n",
    "subplot.grid_cells(global_map, style= RB_STYLE)\n",
    "subplot.legend(location=\"right\")\n",
    "\n",
    "# Second subplot (right)\n",
    "subplot1 = fig.add_map(domain=\"Australia\", row=0, column=1)\n",
    "subplot1.grid_cells(std_values, x=lon_grid, y=lat_grid, style = SPI_SEOM_STYLE)\n",
    "subplot1.legend(location=\"right\")\n",
    "\n",
    "subplot.title(\"Elevation map (m)\")\n",
    "subplot1.title(\"Standard Uncertainty of the Mean across 2023-2024 (N=10)\")\n",
    "\n",
    "# Add decorations\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Find nearest PRCP station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Domain bounds (Australia) ---\n",
    "\n",
    "# --- Domain bounds (Australia example) ---\n",
    "DOMAIN = \"Australia\"\n",
    "BOUNDS = {\n",
    "    \"Australia\": {\"lat_min\": -45, \"lat_max\": -10, \"lon_min\": 110, \"lon_max\": 155},    \n",
    "    \"Europe\":    {\"lat_min\": 25,  \"lat_max\": 72,  \"lon_min\": -25,  \"lon_max\": 45},\n",
    "    # add others if needed...\n",
    "}\n",
    "\n",
    "bounds = BOUNDS[DOMAIN]\n",
    "MARGIN = 2.0  # degrees\n",
    "\n",
    "# ‚úÖ Correct, uncluttered assignments\n",
    "lat_min = bounds[\"lat_min\"] - MARGIN\n",
    "lat_max = bounds[\"lat_max\"] + MARGIN\n",
    "lon_min = bounds[\"lon_min\"] - MARGIN\n",
    "lon_max = bounds[\"lon_max\"] + MARGIN\n",
    "\n",
    "# --- Grid centers from edges ---\n",
    "lat_centers = 0.5 * (lat_edges[:-1] + lat_edges[1:])\n",
    "lon_centers = 0.5 * (lon_edges[:-1] + lon_edges[1:])\n",
    "\n",
    "# Subset to domain\n",
    "lat_mask = (lat_centers >= lat_min) & (lat_centers <= lat_max)\n",
    "lon_mask = (lon_centers >= lon_min) & (lon_centers <= lon_max)\n",
    "\n",
    "lat_centers_sub = lat_centers[lat_mask]\n",
    "lon_centers_sub = lon_centers[lon_mask]\n",
    "\n",
    "# Meshgrid + longitude normalization (to [-180, 180])\n",
    "lonc2d_sub, latc2d_sub = np.meshgrid(lon_centers_sub, lat_centers_sub)\n",
    "lonc2d_sub_norm = ((lonc2d_sub + 180) % 360) - 180\n",
    "\n",
    "# Filter stations to the same box\n",
    "stations_dom = prcp_stations_active[\n",
    "    (prcp_stations_active[\"lat\"] >= lat_min) & (prcp_stations_active[\"lat\"] <= lat_max) &\n",
    "    (prcp_stations_active[\"lon\"] >= lon_min) & (prcp_stations_active[\"lon\"] <= lon_max)\n",
    "].copy()\n",
    "stations_dom[\"lon\"] = ((stations_dom[\"lon\"] + 180) % 360) - 180  # normalize station lons\n",
    "\n",
    "if stations_dom.empty:\n",
    "    raise RuntimeError(\"No PRCP stations found in the selected domain after filtering.\")\n",
    "\n",
    "stations_lat = stations_dom[\"lat\"].to_numpy()\n",
    "stations_lon = stations_dom[\"lon\"].to_numpy()\n",
    "stations_lon = ((stations_lon + 180) % 360) - 180  # ‚úÖ normalize stations\n",
    "\n",
    "# --- Nearest neighbor (KD-tree if available; else chunked haversine) ---\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "nlat_sub, nlon_sub = latc2d_sub.shape\n",
    "\n",
    "nearest_station_index = np.empty((nlat_sub, nlon_sub), dtype=np.int32)\n",
    "nearest_distance_km   = np.empty((nlat_sub, nlon_sub), dtype=np.float32)\n",
    "\n",
    "try:\n",
    "    from scipy.spatial import cKDTree\n",
    "\n",
    "    def sph2xyz(lat_deg, lon_deg):\n",
    "        lat_rad = np.deg2rad(lat_deg)\n",
    "        lon_rad = np.deg2rad(lon_deg)\n",
    "        x = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "        y = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "        z = np.sin(lat_rad)\n",
    "        return np.column_stack([x, y, z])\n",
    "\n",
    "    station_xyz = sph2xyz(stations_lat, stations_lon)                   # (Ns, 3)\n",
    "    grid_xyz    = sph2xyz(latc2d_sub.ravel(), lonc2d_sub_norm.ravel())  # (Ng, 3)\n",
    "\n",
    "    tree = cKDTree(station_xyz)\n",
    "    _, idx = tree.query(grid_xyz, k=1)\n",
    "\n",
    "    # Accurate haversine distance for the nearest candidate\n",
    "    phi1 = np.deg2rad(latc2d_sub.ravel())\n",
    "    lam1 = np.deg2rad(lonc2d_sub_norm.ravel())\n",
    "    phi2 = np.deg2rad(stations_lat[idx])\n",
    "    lam2 = np.deg2rad(stations_lon[idx])\n",
    "    dphi = phi2 - phi1\n",
    "    dlam = lam2 - lam1\n",
    "    a = np.sin(dphi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlam / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "    dist_km_flat = (EARTH_RADIUS_KM * c).astype(np.float32)\n",
    "\n",
    "    nearest_station_index[:] = idx.reshape(nlat_sub, nlon_sub)\n",
    "    nearest_distance_km[:]   = dist_km_flat.reshape(nlat_sub, nlon_sub)\n",
    "\n",
    "except Exception:\n",
    "    # Memory‚Äësafe chunked haversine fallback (no sklearn/scipy)\n",
    "    BR, BC = 32, 32  # block size (tune if needed)\n",
    "    stations_lat32 = stations_lat.astype(np.float32)\n",
    "    stations_lon32 = stations_lon.astype(np.float32)\n",
    "\n",
    "    for i0 in range(0, nlat_sub, BR):\n",
    "        i1 = min(i0 + BR, nlat_sub)\n",
    "        for j0 in range(0, nlon_sub, BC):\n",
    "            j1 = min(j0 + BC, nlon_sub)\n",
    "\n",
    "            lat_blk = latc2d_sub[i0:i1, j0:j1].astype(np.float32)\n",
    "            lon_blk = lonc2d_sub_norm[i0:i1, j0:j1].astype(np.float32)\n",
    "\n",
    "            phi1 = np.deg2rad(lat_blk)[..., None]                  # (br, bc, 1)\n",
    "            lam1 = np.deg2rad(lon_blk)[..., None]\n",
    "            phi2 = np.deg2rad(stations_lat32)[None, None, :]       # (1, 1, Ns)\n",
    "            lam2 = np.deg2rad(stations_lon32)[None, None, :]\n",
    "\n",
    "            dphi = phi2 - phi1\n",
    "            dlam = lam2 - lam1\n",
    "            a = np.sin(dphi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlam / 2.0) ** 2\n",
    "            c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "            D = (EARTH_RADIUS_KM * c).astype(np.float32)           # (br, bc, Ns)\n",
    "\n",
    "            idx_blk  = np.argmin(D, axis=-1).astype(np.int32)\n",
    "            dist_blk = np.take_along_axis(D, idx_blk[..., None], axis=-1)[..., 0]\n",
    "\n",
    "            nearest_station_index[i0:i1, j0:j1] = idx_blk\n",
    "            nearest_distance_km[i0:i1, j0:j1]   = dist_blk\n",
    "\n",
    "# --- Package as xarray & (optionally) plot ---\n",
    "nearest_station_lat = stations_lat[nearest_station_index]\n",
    "nearest_station_lon = stations_lon[nearest_station_index]\n",
    "\n",
    "nearest_ds = xr.Dataset(\n",
    "    {\n",
    "        \"nearest_station_index\": ((\"lat\", \"lon\"), nearest_station_index),\n",
    "        \"nearest_distance_km\":   ((\"lat\", \"lon\"), nearest_distance_km),\n",
    "        \"nearest_station_lat\":   ((\"lat\", \"lon\"), nearest_station_lat),\n",
    "        \"nearest_station_lon\":   ((\"lat\", \"lon\"), nearest_station_lon),\n",
    "    },\n",
    "    coords={\"lat\": lat_centers_sub, \"lon\": lon_centers_sub},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Find k-th nearest PRCP station (up to k=5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Parameters ---\n",
    "K_TARGET = 5  # request up to 5 nearest stations\n",
    "\n",
    "# --- Domain bounds (Australia example) ---\n",
    "DOMAIN = \"Australia\"\n",
    "BOUNDS = {\n",
    "    \"Australia\": {\"lat_min\": -45, \"lat_max\": -10, \"lon_min\": 110, \"lon_max\": 155},    \n",
    "    \"Europe\":    {\"lat_min\": 25,  \"lat_max\": 72,  \"lon_min\": -25,  \"lon_max\": 45},\n",
    "    # add others if needed...\n",
    "}\n",
    "\n",
    "bounds = BOUNDS[DOMAIN]\n",
    "MARGIN = 2.0  # degrees\n",
    "\n",
    "lon_centers = 0.5 * (lon_edges[:-1] + lon_edges[1:])\n",
    "lat_min = bounds[\"lat_min\"] - MARGIN\n",
    "\n",
    "# Subset to domain\n",
    "lat_mask = (lat_centers >= lat_min) & (lat_centers <= lat_max)\n",
    "lon_mask = (lon_centers >= lon_min) & (lon_centers <= lon_max)\n",
    "\n",
    "lat_centers_sub = lat_centers[lat_mask]\n",
    "lon_centers_sub = lon_centers[lon_mask]\n",
    "\n",
    "# Meshgrid + longitude normalization (to [-180, 180])\n",
    "lonc2d_sub, latc2d_sub = np.meshgrid(lon_centers_sub, lat_centers_sub)\n",
    "lonc2d_sub_norm = ((lonc2d_sub + 180) % 360) - 180\n",
    "\n",
    "# Filter stations to the same box\n",
    "stations_dom = prcp_stations_active[\n",
    "    (prcp_stations_active[\"lat\"] >= lat_min) & (prcp_stations_active[\"lat\"] <= lat_max) &\n",
    "    (prcp_stations_active[\"lon\"] >= lon_min) & (prcp_stations_active[\"lon\"] <= lon_max)\n",
    "].copy()\n",
    "\n",
    "# Normalize station longitudes to [-180, 180]\n",
    "stations_dom[\"lon\"] = ((stations_dom[\"lon\"] + 180) % 360) - 180\n",
    "\n",
    "if stations_dom.empty:\n",
    "    raise RuntimeError(\"No PRCP stations found in the selected domain after filtering.\")\n",
    "\n",
    "stations_lat = stations_dom[\"lat\"].to_numpy()\n",
    "stations_lon = stations_dom[\"lon\"].to_numpy()\n",
    "\n",
    "# Effective K: cap at number of stations\n",
    "Ns = len(stations_lat)\n",
    "K = int(min(K_TARGET, Ns))\n",
    "if K < 1:\n",
    "    raise RuntimeError(\"Effective K is < 1 (no stations).\")\n",
    "\n",
    "# --- Allocate outputs ---\n",
    "EARTH_RADIUS_KM = 6371.0\n",
    "nlat_sub, nlon_sub = latc2d_sub.shape\n",
    "shape3 = (nlat_sub, nlon_sub, K)\n",
    "\n",
    "nearest_station_index_k = np.empty(shape3, dtype=np.int32)\n",
    "nearest_distance_km_k   = np.empty(shape3, dtype=np.float32)\n",
    "\n",
    "# --- Utility: haversine distance (vectorized for k neighbors) ---\n",
    "def haversine_k(phi1, lam1, phi2, lam2):\n",
    "    # phi1, lam1: (Ng, 1)\n",
    "    # phi2, lam2: (Ng, K)\n",
    "    dphi = phi2 - phi1\n",
    "    dlam = lam2 - lam1\n",
    "    a = np.sin(dphi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlam / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "    return (EARTH_RADIUS_KM * c).astype(np.float32)  # (Ng, K)\n",
    "\n",
    "# --- Fast path: cKDTree k-nearest on unit sphere, then exact haversine ---\n",
    "try:\n",
    "    from scipy.spatial import cKDTree\n",
    "\n",
    "    def sph2xyz(lat_deg, lon_deg):\n",
    "        lat_rad = np.deg2rad(lat_deg)\n",
    "        lon_rad = np.deg2rad(lon_deg)\n",
    "        x = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "        y = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "        z = np.sin(lat_rad)\n",
    "        return np.column_stack([x, y, z])\n",
    "\n",
    "    station_xyz = sph2xyz(stations_lat, stations_lon)                   # (Ns, 3)\n",
    "    grid_xyz    = sph2xyz(latc2d_sub.ravel(), lonc2d_sub_norm.ravel())  # (Ng, 3)\n",
    "\n",
    "    tree = cKDTree(station_xyz)\n",
    "    # Query k neighbors in Euclidean unit sphere space (we will recompute haversine)\n",
    "    _, idx_k_flat = tree.query(grid_xyz, k=K)  # (Ng, K)\n",
    "\n",
    "    # Build haversine distances for the k neighbors\n",
    "    phi1 = np.deg2rad(latc2d_sub.ravel())[:, None]           # (Ng, 1)\n",
    "    lam1 = np.deg2rad(lonc2d_sub_norm.ravel())[:, None]\n",
    "    phi2 = np.deg2rad(stations_lat[idx_k_flat])              # (Ng, K)\n",
    "    lam2 = np.deg2rad(stations_lon[idx_k_flat])              # (Ng, K)\n",
    "\n",
    "    dist_km_flat = haversine_k(phi1, lam1, phi2, lam2)       # (Ng, K)\n",
    "\n",
    "    # Reshape to (lat, lon, k)\n",
    "    nearest_station_index_k[:] = idx_k_flat.reshape(shape3)\n",
    "    nearest_distance_km_k[:]   = dist_km_flat.reshape(shape3)\n",
    "\n",
    "except Exception:\n",
    "    # --- Memory‚Äësafe fallback: chunked haversine and argpartition for k-nearest ---\n",
    "    BR, BC = 32, 32  # block size (tune if needed)\n",
    "    stations_lat32 = stations_lat.astype(np.float32)\n",
    "    stations_lon32 = stations_lon.astype(np.float32)\n",
    "\n",
    "    for i0 in range(0, nlat_sub, BR):\n",
    "        i1 = min(i0 + BR, nlat_sub)\n",
    "        for j0 in range(0, nlon_sub, BC):\n",
    "            j1 = min(j0 + BC, nlon_sub)\n",
    "\n",
    "            lat_blk = latc2d_sub[i0:i1, j0:j1].astype(np.float32)\n",
    "            lon_blk = lonc2d_sub_norm[i0:i1, j0:j1].astype(np.float32)\n",
    "\n",
    "            phi1 = np.deg2rad(lat_blk)[..., None]                  # (br, bc, 1)\n",
    "            lam1 = np.deg2rad(lon_blk)[..., None]\n",
    "            phi2 = np.deg2rad(stations_lat32)[None, None, :]       # (1, 1, Ns)\n",
    "            lam2 = np.deg2rad(stations_lon32)[None, None, :]\n",
    "\n",
    "            dphi = phi2 - phi1\n",
    "            dlam = lam2 - lam1\n",
    "            a = np.sin(dphi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlam / 2.0) ** 2\n",
    "            c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1.0 - a))\n",
    "            D = (EARTH_RADIUS_KM * c).astype(np.float32)           # (br, bc, Ns)\n",
    "\n",
    "            # k-nearest indices via argpartition (then sort the k slice)\n",
    "            part_idx = np.argpartition(D, K-1, axis=-1)[..., :K]   # (br, bc, K)\n",
    "            # Gather distances for those K\n",
    "            part_dist = np.take_along_axis(D, part_idx, axis=-1)   # (br, bc, K)\n",
    "            # Sort the K by distance\n",
    "            order = np.argsort(part_dist, axis=-1)\n",
    "            idx_k  = np.take_along_axis(part_idx,  order, axis=-1) # (br, bc, K)\n",
    "            dist_k = np.take_along_axis(part_dist, order, axis=-1) # (br, bc, K)\n",
    "\n",
    "            nearest_station_index_k[i0:i1, j0:j1, :] = idx_k.astype(np.int32)\n",
    "            nearest_distance_km_k[i0:i1, j0:j1, :]   = dist_k.astype(np.float32)\n",
    "\n",
    "# --- Derive lat/lon of k neighbors by indexing ---\n",
    "nearest_station_lat_k = stations_lat[nearest_station_index_k]  # (lat, lon, k)\n",
    "nearest_station_lon_k = stations_lon[nearest_station_index_k]  # (lat, lon, k)\n",
    "\n",
    "# --- Package as xarray ---\n",
    "k_coord = np.arange(1, K+1, dtype=np.int32)\n",
    "nearest_ds = xr.Dataset(\n",
    "    {\n",
    "        \"nearest_station_index\": ((\"lat\", \"lon\", \"k\"), nearest_station_index_k),\n",
    "        \"nearest_distance_km\":   ((\"lat\", \"lon\", \"k\"), nearest_distance_km_k),\n",
    "        \"nearest_station_lat\":   ((\"lat\", \"lon\", \"k\"), nearest_station_lat_k),\n",
    "        \"nearest_station_lon\":   ((\"lat\", \"lon\", \"k\"), nearest_station_lon_k),\n",
    "    },\n",
    "    coords={\n",
    "        \"lat\": lat_centers_sub,\n",
    "        \"lon\": lon_centers_sub,  # note: lon centers here are not normalized; adjust if you prefer [-180, 180]\n",
    "        \"k\": k_coord,\n",
    "    },\n",
    ")\n",
    "\n",
    "# --- (Optional) Quick density metrics from k neighbors ---\n",
    "# Inverse-distance sum S_k and mean distance among k neighbors\n",
    "inv_dist_sum_k = 1.0 / np.maximum(nearest_distance_km_k, 1e-6)  # avoid div-by-zero if station on the cell\n",
    "S_k = inv_dist_sum_k.sum(axis=-1)        # (lat, lon)\n",
    "Dmean_k = nearest_distance_km_k.mean(axis=-1)  # (lat, lon)\n",
    "\n",
    "nearest_ds[\"inv_distance_sum_k\"] = ((\"lat\", \"lon\"), S_k)\n",
    "nearest_ds[\"mean_distance_k\"]    = ((\"lat\", \"lon\"), Dmean_k)\n",
    "\n",
    "# Example: distance to the k-th neighbor (useful threshold metric)\n",
    "\n",
    "nearest_ds[\"nearest_distance_km\"] = xr.DataArray(\n",
    "    nearest_distance_km_k,\n",
    "    dims=(\"lat\", \"lon\", \"k\"),\n",
    "    coords={\"lat\": lat_centers_sub, \"lon\": lon_centers_sub, \"k\": np.arange(1, K+1)},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Distance to nearest precipitation station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_ds[\"nearest_distance_km\"].attrs.update(\n",
    "    long_name=\"Distance to nearest PRCP station\",\n",
    "    units=\"km\"\n",
    ")\n",
    "\n",
    "# Create a 2-column figure (Australia left, SPI right)\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))\n",
    "\n",
    "# --- Left subplot: nearest station distance (Australia) ---\n",
    "subplot = fig.add_map(domain=\"Australia\", row=0, column=0)\n",
    "\n",
    "m_left = subplot.grid_cells(nearest_ds[\"nearest_distance_km\"].sel(k=1), style=RB_STYLE)\n",
    "\n",
    "subplot.title(\"Distance to nearest PRCP station (km)\")\n",
    "subplot.legend(location=\"right\")\n",
    "# Optional: show a colorbar label to explain colours\n",
    "cbar_left = getattr(m_left, \"colour_bar\", None) or getattr(m_left, \"color_bar\", None) or getattr(m_left, \"colorbar\", None)\n",
    "if cbar_left is not None:\n",
    "    cbar_left.set_label(\"Distance (km)\")\n",
    "\n",
    "subplot1 = fig.add_map(domain=\"Australia\", row=0, column=1)\n",
    "m_right = subplot1.grid_cells(std_values, x=lon_grid, y=lat_grid, style=SPI_SEOM_STYLE)\n",
    "subplot1.title(\"Standard Uncertainty of the Mean (N=10)\")\n",
    "subplot1.legend(location=\"right\")\n",
    "cbar_right = getattr(m_right, \"colour_bar\", None) or getattr(m_right, \"color_bar\", None) or getattr(m_right, \"colorbar\", None)\n",
    "if cbar_right is not None:\n",
    "    cbar_right.set_label(\"SPI Std. Uncertainty\")\n",
    "\n",
    "# --- Decorations and render (applied to both panels) ---\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of SPI Unc vs k-th nearest precipitation station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "\n",
    "# 1) Take distance array with k dimension and coords\n",
    "dist_da = nearest_ds[\"nearest_distance_km\"]  # dims: (lat, lon, k), coords: lat, lon, k\n",
    "\n",
    "# 2) SPI uncertainty (SEM if /sqrt(10)) on same date\n",
    "spi_da = ensemble_std_time / np.sqrt(10)     # dims: (lat, lon) OR (time, lat, lon) after selecting a date\n",
    "\n",
    "# 3) Interpolate SPI to the distance grid (lat/lon of nearest_ds)\n",
    "spi_on_dist_grid = spi_da.interp(\n",
    "    lat=dist_da[\"lat\"], lon=dist_da[\"lon\"], method=\"nearest\"\n",
    ")\n",
    "\n",
    "# 4) Loop over k=1..5, flatten & mask NaNs, plot\n",
    "k_vals = dist_da[\"k\"].values\n",
    "k_max = int(min(5, k_vals.max()))\n",
    "k_list = [k for k in k_vals if 1 <= k <= k_max]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9), dpi=140)\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "spearman_stats = []\n",
    "\n",
    "for i, k in enumerate(k_list):\n",
    "    ax = axes[i]\n",
    "    x_k = dist_da.sel(k=k).values.ravel()                 # distance to k-th neighbor\n",
    "    y = spi_on_dist_grid.values.ravel()                   # SPI uncertainty\n",
    "    mask = np.isfinite(x_k) & np.isfinite(y)\n",
    "    x_k = x_k[mask]; y_k = y[mask]\n",
    "    \n",
    "    # Scatter\n",
    "    ax.scatter(x_k, y_k, s=8, alpha=0.35, color=colors[i], edgecolors=\"none\")\n",
    "    ax.set_xlabel(f\"Distance to {k}-th nearest PRCP station (km)\")\n",
    "    ax.set_ylabel(\"SPI standard uncertainty (N=10)\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.set_title(f\"Australia: k = {k}\")\n",
    "\n",
    "# Hide unused subplot (6th cell)\n",
    "for j in range(len(k_list), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle(\"Distance vs SPI uncertainty by k-th nearest station (Australia)\", y=0.99)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Pick your domain grid (either Australia or Europe dataset you computed)\n",
    "dist_da = nearest_ds[\"nearest_distance_km\"].sel(k=1)  # (lat, lon) in km\n",
    "\n",
    "# 2) Select SPI uncertainty on the same date & convert to std. uncertainty (N=10 here)\n",
    "spi_da = ensemble_std_time / np.sqrt(10)\n",
    "\n",
    "# Interpolate SPI uncertainty to the distance grid\n",
    "spi_on_dist_grid = spi_da.interp(lat=dist_da[\"lat\"], lon=dist_da[\"lon\"], method=\"nearest\")\n",
    "\n",
    "# 4) Flatten & mask NaNs\n",
    "x = dist_da.values.ravel()                # distance (km)\n",
    "y = spi_on_dist_grid.values.ravel()       # SPI uncertainty\n",
    "mask = np.isfinite(x) & np.isfinite(y)\n",
    "x = x[mask]; y = y[mask]\n",
    "\n",
    "# 5) Plot: scatter\n",
    "plt.figure(figsize=(8, 6), dpi=160)\n",
    "plt.scatter(x, y, s=8, alpha=0.5, color=\"#1f77b4\", edgecolors=\"none\")\n",
    "plt.xlabel(\"Distance to nearest PRCP station (km)\")\n",
    "plt.ylabel(\"SPI standard uncertainty (N=10)\")\n",
    "plt.title(\"Distance vs. SPI uncertainty in Australia region.\")\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot of ensemble uncertainty vs climate zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"sis-biodiversity-cmip5-global\"\n",
    "request = {\n",
    "    \"variable\": [\"koeppen_geiger_class\"],\n",
    "    \"model\": [\"csiro_mk3_6_0\"],\n",
    "    \"ensemble_member\": [\"r1i1p1\"],\n",
    "    \"experiment\": [\"rcp4_5\"],\n",
    "    \"temporal_aggregation\": [\"climatology\"],\n",
    "    \"statistic\": [\"mean\"],\n",
    "    \"version\": [\"1_0\"]\n",
    "}\n",
    "\n",
    "koep_ens = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "koep_ens = koep_ens.to_xarray(compat=\"equals\") # Converts to xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cftime\n",
    "\n",
    "# --- 1) Extract variable and pick a slice close to 2024 if \"time\" exists\n",
    "varname = \"koeppen-geiger-class\"\n",
    "da = koep_ens[varname]\n",
    "\n",
    "# Pick the timestamp closest to 2019-01-01 in the same CF calendar\n",
    "target = cftime.DatetimeNoLeap(2024, 1, 1)\n",
    "da_sel = da.sel(time=target, method=\"nearest\")\n",
    "\n",
    "# Create a 2-column figure (Australia left, SPI right)\n",
    "fig = ekp.Figure(rows=1, columns=2, size=(12, 6))\n",
    "\n",
    "# --- Left subplot: nearest station distance (Australia) ---\n",
    "subplot = fig.add_map(domain=\"Australia\", row=0, column=0)\n",
    "subplot.grid_cells(da_sel)\n",
    "subplot.legend(location=\"right\")\n",
    "\n",
    "subplot1 = fig.add_map(domain=\"Australia\", row=0, column=1)\n",
    "subplot1.grid_cells(std_values, x=lon_grid, y=lat_grid, style=SPI_SEOM_STYLE)\n",
    "subplot1.title(\"Standard Uncertainty of the Mean (N=10)\")\n",
    "subplot1.legend(location=\"right\")\n",
    "\n",
    "\n",
    "# --- Decorations and render (applied to both panels) ---\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.borders()\n",
    "fig.gridlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_sel.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Reading in ensemble precipitation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-single-levels-monthly-means\"\n",
    "request = {\n",
    "    \"product_type\": [\"monthly_averaged_ensemble_members\"],\n",
    "    \"variable\": [\"total_precipitation\"],\n",
    "    \"year\": [\n",
    "        \"1940\", \"1941\", \"1942\",\n",
    "        \"1943\", \"1944\", \"1945\",\n",
    "        \"1946\", \"1947\", \"1948\",\n",
    "        \"1949\", \"1950\", \"1951\",\n",
    "        \"1952\", \"1953\", \"1954\",\n",
    "        \"1955\", \"1956\", \"1957\",\n",
    "        \"1958\", \"1959\", \"1960\",\n",
    "        \"1961\", \"1962\", \"1963\",\n",
    "        \"1964\", \"1965\", \"1966\",\n",
    "        \"1967\", \"1968\", \"1969\",\n",
    "        \"1970\", \"1971\", \"1972\",\n",
    "        \"1973\", \"1974\", \"1975\",\n",
    "        \"1976\", \"1977\", \"1978\",\n",
    "        \"1979\", \"1980\", \"1981\",\n",
    "        \"1982\", \"1983\", \"1984\",\n",
    "        \"1985\", \"1986\", \"1987\",\n",
    "        \"1988\", \"1989\", \"1990\",\n",
    "        \"1991\", \"1992\", \"1993\",\n",
    "        \"1994\", \"1995\", \"1996\",\n",
    "        \"1997\", \"1998\", \"1999\",\n",
    "        \"2000\", \"2001\", \"2002\",\n",
    "        \"2003\", \"2004\", \"2005\",\n",
    "        \"2006\", \"2007\", \"2008\",\n",
    "        \"2009\", \"2010\", \"2011\",\n",
    "        \"2012\", \"2013\", \"2014\",\n",
    "        \"2015\", \"2016\", \"2017\",\n",
    "        \"2018\", \"2019\", \"2020\",\n",
    "        \"2021\", \"2022\", \"2023\",\n",
    "        \"2024\", \"2025\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"time\": [\"00:00\"],\n",
    "    \"data_format\": \"grib\",\n",
    "    \"download_format\": \"unarchived\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_ens = ekd.from_source(\"cds\", dataset, request) # Sends request for this dataset to CDS.\n",
    "tp_ens = tp_ens.to_xarray() # Converts to xarray.\n",
    "tp_ens = tp_ens.assign_coords(number=tp_ens.number + 1)\n",
    "tp_ens_sel = tp_ens.sel(latitude=9.5, longitude=40.5, method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Selecting drought index from ensemble at one location (lon=9.5, lat = 40.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_drought_ens_ = point_drought_ens_.sel(lon = 9.5, lat = 40.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble(da, time_dim=\"time\", member_dim=\"member\"):\n",
    "    times = pd.Index(da[time_dim].values)\n",
    "    member_ids = pd.Series(times).groupby(times).cumcount().to_numpy()\n",
    "    \n",
    "    # Create MultiIndex\n",
    "    mi = pd.MultiIndex.from_arrays([times, member_ids], names=[time_dim, member_dim])\n",
    "    \n",
    "    # Assign MultiIndex and rename the dimension to something temporary\n",
    "    da = da.rename({time_dim: \"tmp\"}).assign_coords(tmp=mi)\n",
    "    \n",
    "    # Unstack to get (time, member)\n",
    "    return da.unstack()\n",
    "\n",
    "ens = make_ensemble(point_drought_ens_[\"SPI1\"])  # or pass the whole Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### Ensemble spread along with 48-month rolling average in SPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ens_std = ens.std(dim = \"member\") / np.sqrt(10)\n",
    "point_ens_mean = ens.mean(dim = \"member\")\n",
    "\n",
    "point_ens_std_rolling = point_ens_mean.rolling(\n",
    "    time=48, center=False, min_periods=48\n",
    ").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a clean time vs stdev plot\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(point_ens_mean.time.values, point_ens_mean.values,\n",
    "         color='tab:blue', lw=1.3, alpha=0.45, label='Mean (monthly)')\n",
    "plt.plot(point_ens_std_rolling.time.values, point_ens_std_rolling.values,\n",
    "         color='tab:green', lw=2.4, marker='o', label='4-year mean (calendar)')\n",
    "\n",
    "plt.xlabel(\"Forecast reference time\")\n",
    "plt.ylabel(\"Mean across ensemble (SPI)\")\n",
    "plt.title(\"Ensemble mean of SPI over time (yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute rolling mean for the ensemble mean SPI (same window as stdev)\n",
    "point_ens_mean_rolling = point_ens_mean.rolling(\n",
    "    time=3, center=False, min_periods=3\n",
    ").mean()\n",
    "\n",
    "# Create side-by-side subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 5), sharex=True)\n",
    "\n",
    "# --- Left: Std. dev over time ---\n",
    "axes[0].plot(\n",
    "    point_ens_std.time.values, point_ens_std.values,\n",
    "    color='tab:blue', lw=1.3, alpha=0.45, label='Std. dev (monthly)'\n",
    ")\n",
    "axes[0].plot(\n",
    "    point_ens_std_rolling.time.values, point_ens_std_rolling.values,\n",
    "    color='tab:green', lw=2.4, marker='o', label='4-year stdev mean (calendar)'\n",
    ")\n",
    "axes[0].set_xlabel(\"Forecast reference time\")\n",
    "axes[0].set_ylabel(\"Std. dev across ensemble (SPI)\")\n",
    "axes[0].set_title(\"Ensemble spread of SPI over time\\n(yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# --- Right: Mean SPI over time ---\n",
    "axes[1].plot(\n",
    "    point_ens_mean.time.values, point_ens_mean.values,\n",
    "    color='tab:orange', lw=1.3, alpha=0.55, label='Mean SPI (monthly)'\n",
    ")\n",
    "axes[1].plot(\n",
    "    point_ens_mean_rolling.time.values, point_ens_mean_rolling.values,\n",
    "    color='tab:red', lw=2.4, marker='o', label='4-year mean (calendar)'\n",
    ")\n",
    "axes[1].set_xlabel(\"Forecast reference time\")\n",
    "axes[1].set_ylabel(\"Mean SPI\")\n",
    "axes[1].set_title(\"Ensemble mean SPI over time\\n(yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rolling mean for ensemble mean SPI (same window as stdev)\n",
    "point_ens_mean_rolling = point_ens_mean.rolling(\n",
    "    time=3, center=False, min_periods=3\n",
    ").mean()\n",
    "\n",
    "fig, ax_left = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# --- Left y-axis: Std. dev ---\n",
    "lns1 = ax_left.plot(\n",
    "    point_ens_std.time.values, point_ens_std.values,\n",
    "    color='tab:blue', lw=1.3, alpha=0.45, label='Std. dev (monthly)'\n",
    ")\n",
    "lns2 = ax_left.plot(\n",
    "    point_ens_std_rolling.time.values, point_ens_std_rolling.values,\n",
    "    color='tab:green', lw=2.0, marker='o', label='Std. dev 4-year mean'\n",
    ")\n",
    "ax_left.set_ylabel(\"Std. dev across ensemble (SPI)\", color='tab:blue')\n",
    "ax_left.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax_left.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Right y-axis: Mean SPI ---\n",
    "ax_right = ax_left.twinx()\n",
    "lns3 = ax_right.plot(\n",
    "    point_ens_mean.time.values, point_ens_mean.values,\n",
    "    color='tab:orange', lw=1.3, alpha=0.55, label='Mean SPI (monthly)'\n",
    ")\n",
    "lns4 = ax_right.plot(\n",
    "    point_ens_mean_rolling.time.values, point_ens_mean_rolling.values,\n",
    "    color='tab:red', lw=2.0, marker='o', label='Mean SPI 4-year mean'\n",
    ")\n",
    "ax_right.set_ylabel(\"Mean SPI\", color='tab:orange')\n",
    "ax_right.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "# --- Shared x-axis and title ---\n",
    "ax_left.set_xlabel(\"Forecast reference time\")\n",
    "ax_left.set_title(\"Std. dev vs Mean SPI over time (two y-axes)\\n(lon, lat) : (40.5, 9.5)\")\n",
    "\n",
    "# --- Unified legend combining both axes ---\n",
    "lines = lns1 + lns2 + lns3 + lns4\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax_left.legend(lines, labels, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ens_mean = ens.mean(dim=\"member\")\n",
    "point_ens_stdev = ens.std(dim=\"member\")  # replace with SEM if preferred\n",
    "\n",
    "t = point_ens_mean.time.values\n",
    "y = point_ens_mean.values\n",
    "spread = point_ens_stdev.values  # or SEM\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Mean line\n",
    "plt.plot(t, y, color='tab:orange', lw=1.8, label='Mean SPI')\n",
    "\n",
    "# Shaded band for uncertainty\n",
    "plt.fill_between(\n",
    "    t, y - spread, y + spread,\n",
    "    color='tab:blue', alpha=0.2, label='¬± stdev across ensemble'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Forecast reference time\")\n",
    "plt.ylabel(\"Mean SPI\")\n",
    "plt.title(\"Ensemble mean SPI with ¬± stdev band over time\\n(lon, lat): (40.5, 9.5)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([-3, 3])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "#### Ensemble spread of total precipitation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stdev across ensemble members (dimension: 'number')\n",
    "tp_ens_sel = tp_ens_sel.sel(forecast_reference_time = slice('1940-01-31T18:00:00.000000000','2025-09-30T18:00:00.000000000'))\n",
    "tp_std = tp_ens_sel['tp'].std(dim='number')  # result dims: (forecast_reference_time)\n",
    "tp_mean = tp_ens_sel['tp'].mean(dim='number')  # result dims: (forecast_reference_time)\n",
    "\n",
    "# Make a clean time vs stdev plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(point_ens_mean.values, point_ens_std.values, color='tab:blue')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "#### Ensemble spread along with 48-month rolling average in tp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stdev across ensemble members\n",
    "tp_std = tp_ens_sel['tp'].mean(dim='number')\n",
    "\n",
    "tp_std_monthly = tp_std.rolling(\n",
    "    forecast_reference_time=48, center=False, min_periods=48\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(tp_std['forecast_reference_time'].values, tp_std.values,\n",
    "         color='tab:blue', lw=1.3, alpha=0.45, label='Std. dev (monthly)')\n",
    "plt.plot(tp_std_monthly['forecast_reference_time'].values, tp_std_monthly.values,\n",
    "         color='tab:green', lw=2.4, marker='o', label='4-year mean (calendar)')\n",
    "\n",
    "plt.xlabel(\"Forecast reference time\")\n",
    "plt.ylabel(\"Mean across ensemble (tp)\")\n",
    "plt.title(\"Ensemble mean of tp over time (yearly mean overlay) at (lon, lat) : (40.5, 9.5)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 3.749769,
     "end_time": "2024-03-08T17:24:00.248720",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.498951",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# collapsable code cell\n",
    "\n",
    "# code is included for transparency but also learning purposes and gives users the chance to adapt the code used for the assessment as they wish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.271597,
     "end_time": "2024-03-08T17:40:01.664067",
     "exception": false,
     "start_time": "2024-03-08T17:40:01.392470",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ‚ÑπÔ∏è If you want to know more\n",
    "\n",
    "### Key resources\n",
    "\n",
    "List some key resources related to this assessment. E.g. CDS entries, applications, dataset documentation, external pages.\n",
    "Also list any code libraries used (if applicable).\n",
    "\n",
    "Code libraries used:\n",
    "* [C3S EQC custom functions](https://github.com/bopen/c3s-eqc-automatic-quality-control/tree/main/c3s_eqc_automatic_quality_control), `c3s_eqc_automatic_quality_control`,  prepared by [B-Open](https://www.bopen.eu/)\n",
    "\n",
    "### References\n",
    "\n",
    "List the references used in the Notebook here.\n",
    "\n",
    "E.g.\n",
    "\n",
    "[[1]](https://doi.org/10.1038/s41598-018-20628-2) Rodriguez, D., De Voil, P., Hudson, D., Brown, J. N., Hayman, P., Marrou, H., & Meinke, H. (2018). Predicting optimum crop designs using crop models and seasonal climate forecasts. Scientific reports, 8(1), 2231."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 968.520731,
   "end_time": "2024-03-08T17:40:03.783430",
   "environment_variables": {},
   "exception": null,
   "input_path": "D520.3.2.3b.SEASONAL_multimodel-bias_v5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T17:23:55.262699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
