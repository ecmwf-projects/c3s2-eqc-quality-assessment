{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Validation and intercomparison of AgERA5 and other reanalysis datasets for agricultural applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Production date: DD-MM-2025\n",
    "\n",
    "**Please note that this repository is used for development and review, so quality assessments should be considered work in progress until they are merged into the main branch.**\n",
    "\n",
    "Dataset version: 2.0.\n",
    "\n",
    "Produced by: C3S2_521 contract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸŒ Use case: Agricultural yield estimation and prediction based on reanalysis data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## â“ Quality assessment question\n",
    "* Is AgERA5 fit-for-purpose as an input dataset for crop yield models?\n",
    "* How do reanalysis datasets compare to observations, and to each other, for agriculturally relevant variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "Introduction:\n",
    "\n",
    "* Introduction to crop yield estimation -> [PCSE/WOFOST](https://github.com/ajwdewit/pcse) [[deWit+19]](https://doi.org/10.1016/j.agsy.2018.06.018).\n",
    "* AgERA5 dataset, reanalysis in general\n",
    "* Validation of datasets\n",
    "\n",
    "[[CDS AgERA5]](https://doi.org/10.24381/cds.6c68c9bb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸ“¢ Quality assessment statement\n",
    "\n",
    "```{admonition} These are the key outcomes of this assessment\n",
    ":class: note\n",
    "* The AgERA5 dataset (\"Agrometeorological indicators from 1979 to present derived from reanalysis\") is well-suited to agricultural studies because it provides daily aggregates and statistics of important parameters such as irradiation, temperature, vapour pressure, precipitation, wind speed, and snow depth. \n",
    "* The resolution of AgERA5 (daily, 0.1Â°) is comparable to other data sources and is sufficient for simulations at plot-level and larger spatial scales. Comparisons can be made at scales larger than the cell size, i.e. 0.1Â° or ~11 km.\n",
    "* The AgERA5 and ERA5-Land datasets, both derived from ERA5, provide different values due to their different methods for downscaling and bias adjustment. While these differences are (in some cases) statistically significant, they are small compared to the typical uncertainty in ERA5 and compared to the uncertainties in agricultural models. Differences may be larger in specific areas, e.g. at higher elevation.\n",
    "* etc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ðŸ“‹ Methodology\n",
    "\n",
    "**Agrometeorological indicators from 1979 to present derived from reanalysis** (*AgERA5*; [doi 10.24381/cds.6c68c9bb](https://doi.org/10.24381/cds.6c68c9bb)).\n",
    "\n",
    "A â€˜free textâ€™ introduction to the data analysis steps or a description of the literature synthesis, with a justification of the approach taken, and limitations mentioned. **Mention which CDS catalogue entry is used, including a link, and also any other entries used for the assessment**.\n",
    "\n",
    "Variables of interest for a crop growth simulator such as [PCSE/WOFOST](https://github.com/ajwdewit/pcse):\n",
    "\n",
    "| Variable name     | Statistic (24h) | Unit     | Example assessment |\n",
    "|-------------------|-----------------|----------|--------------------|\n",
    "| Solar irradiation | Total           | J/mÂ²/day | example |\n",
    "| Temperature (2m)  | Minimum         | Â°C       | example |\n",
    "|                   | Maximum         |          |         |\n",
    "|                   | Mean            |          |         |\n",
    "| Vapour pressure   | Mean            | kPa      | example |\n",
    "| Rain              | Total           | cm/day   | example |\n",
    "| Wind speed (2m)   | Mean            | m/s      | example |\n",
    "| Snow depth        | Mean            | cm       | example |\n",
    "\n",
    "[Source](https://pcse.readthedocs.io/en/stable/code.html#pcse.base.WeatherDataContainer)\n",
    "E0, ES0, ET0 are taken from evapotranspiration calculation\n",
    "\n",
    "The analysis and results are organised in the following steps, which are detailed in the sections below:\n",
    "\n",
    "**[](section-setup)**\n",
    "\n",
    "**[](section-download)**\n",
    " * AgERA5, ERA5-Land, E-OBS, ...\n",
    "\n",
    "**[](section-results)** \n",
    " * Point-by-point comparison\n",
    " * Time series comparison\n",
    " * Geospatial comparison\n",
    "\n",
    "Any further notes on the method could go here (explanations, caveats or limitations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Analysis and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-setup)=\n",
    "### 1. Code setup\n",
    "#### Imports\n",
    "```{note}\n",
    "This notebook uses [earthkit](https://github.com/ecmwf/earthkit) for \n",
    "downloading ([earthkit-data](https://github.com/ecmwf/earthkit-data)) \n",
    "and \n",
    "visualising ([earthkit-plots](https://github.com/ecmwf/earthkit-plots)) data.\n",
    "Because earthkit is in active development, some functionality may change after this notebook is published.\n",
    "If any part of the code stops functioning, please raise an issue on our GitHub repository so it can be fixed.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Earthkit\n",
    "import earthkit.data as ekd\n",
    "import earthkit.plots as ekp\n",
    "\n",
    "# General data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "from myst_nb import glue  # For web version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type hints for helper functions\n",
    "from typing import Callable, Optional, Iterable\n",
    "\n",
    "# For pre-defining functions\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data downloading\n",
    "def domain_to_request(domain: ekp.geo.domains.Domain) -> dict:\n",
    "    \"\"\" From an earthkit-plots domain, generate a request for earthkit-data / cdsapi. \"\"\"\n",
    "    bbox = domain.bbox.to_latlon_bbox()\n",
    "\n",
    "    # Round\n",
    "    north = int(np.ceil(bbox.north) + 1)\n",
    "    south = int(np.floor(bbox.south) - 1)\n",
    "    west = int(np.floor(bbox.west) - 1)\n",
    "    east = int(np.ceil(bbox.east) + 1)\n",
    "    \n",
    "    area = [north, west, south, east]\n",
    "    return {\"area\": area}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data (pre-)processing\n",
    "# Pre-defined variables to ensure consistent order\n",
    "variables = [\"Temperature_Air_2m_Min_24h\", \"Temperature_Air_2m_Mean_24h\", \"Temperature_Air_2m_Max_24h\", \n",
    "             \"Solar_Radiation_Flux\", \n",
    "             \"Wind_Speed_10m_Mean_24h\",\n",
    "             \"Precipitation_Flux\",\n",
    "             \"Snow_Thickness_Mean_24h\"\n",
    "            ]\n",
    "variables_temperature = [var for var in variables if \"Temperature\" in var]\n",
    "variables_not_temperature = [var for var in variables if var not in variables_temperature]\n",
    "\n",
    "# Metadata handling\n",
    "def adjust_metadata(data_array: xr.DataArray, **updates) -> xr.DataArray:\n",
    "    \"\"\" Adjust metadata using a new dictionary. Returns a new object. \"\"\"\n",
    "    metadata_old = data_array.attrs\n",
    "    metadata_new = metadata_old | updates\n",
    "    data_array = data_array.assign_attrs(**metadata_new)\n",
    "    return data_array\n",
    "\n",
    "def adjust_names(dataset: xr.Dataset, dataset_name: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Adjust the names and units of pre-defined variables in a dataset.\n",
    "    Also adds the name of the dataset to its attrs for easy acces.\n",
    "    \"\"\"\n",
    "    # Rename variables\n",
    "    dataset[\"Solar_Radiation_Flux\"]        = adjust_metadata(dataset[\"Solar_Radiation_Flux\"], \n",
    "                                                             long_name=\"Surface solar radiation downwards\", units=\"J m-2 d-1\")\n",
    "    dataset[\"Temperature_Air_2m_Min_24h\"]  = adjust_metadata(dataset[\"Temperature_Air_2m_Min_24h\"], \n",
    "                                                             long_name=\"Minimum air temperature\", units=\"Â°C\")\n",
    "    dataset[\"Temperature_Air_2m_Mean_24h\"] = adjust_metadata(dataset[\"Temperature_Air_2m_Mean_24h\"], \n",
    "                                                             long_name=\"Mean air temperature\", units=\"Â°C\")\n",
    "    dataset[\"Temperature_Air_2m_Max_24h\"]  = adjust_metadata(dataset[\"Temperature_Air_2m_Max_24h\"], \n",
    "                                                             long_name=\"Maximum air temperature\", units=\"Â°C\")\n",
    "    dataset[\"Wind_Speed_10m_Mean_24h\"]     = adjust_metadata(dataset[\"Wind_Speed_10m_Mean_24h\"], \n",
    "                                                             long_name=\"Wind speed at 10m\", units=\"m s-1\")\n",
    "    dataset[\"Precipitation_Flux\"]          = adjust_metadata(dataset[\"Precipitation_Flux\"], \n",
    "                                                             long_name=\"Total precipitation\", units=\"mm d-1\")\n",
    "    dataset[\"Snow_Thickness_Mean_24h\"]     = adjust_metadata(dataset[\"Snow_Thickness_Mean_24h\"], \n",
    "                                                             long_name=\"Snow depth\", units=\"cm\")\n",
    "    \n",
    "    # Assign dataset name\n",
    "    dataset = dataset.assign_attrs({\"name\": dataset_name})\n",
    "    return dataset\n",
    "\n",
    "# Unit conversion\n",
    "def convert_unit(dataset: xr.Dataset, key: str, conversion: Callable, new_unit: str) -> None:\n",
    "    \"\"\" Convert the units of dataset[key] to new_unit using a conversion function (e.g. lambda x: x*1000 for m to mm), in-place. \"\"\"\n",
    "    # Metadata handling\n",
    "    metadata_old = dataset[key].attrs\n",
    "    metadata_new = metadata_old | {\"units\": new_unit}\n",
    "\n",
    "    # Apply changes\n",
    "    dataset[key] = conversion(dataset[key]).assign_attrs(**metadata_new)\n",
    "\n",
    "convert_m2cm = partial(convert_unit, conversion=(lambda x: x*100), new_unit=\"cm\")  # meter -> centimeter\n",
    "convert_m2mm = partial(convert_unit, conversion=(lambda x: x*1000), new_unit=\"cm\")  # meter -> millimeter\n",
    "convert_K2C = partial(convert_unit, conversion=(lambda x: x-273.15), new_unit=\"Â°C\")  # Kelvin -> Celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time series\n",
    "# Time series selection\n",
    "def extract_timeseries(site: dict, *datasets: xr.Dataset, method: str=\"nearest\", **kwargs) -> list[xr.Dataset]:\n",
    "    \"\"\" Extract time series for a given site from a number of datasets. \"\"\"\n",
    "    selection = site | {\"method\": method} | kwargs\n",
    "    timeseries = [dataset.sel(**selection) for dataset in datasets]\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display\n",
    "# Labels\n",
    "def label_with_unit(data: xr.Dataset, key: str, *, latex=False) -> str:\n",
    "    \"\"\" Extract the full name with unit for a key in a dataset. \"\"\"\n",
    "    long_name = data[key].long_name\n",
    "    unit = data[key].units\n",
    "    if latex:\n",
    "        unit = ekp.metadata.units.format_units(data[key].units)\n",
    "    return f\"{long_name} [{unit}]\"\n",
    "\n",
    "# Plotting\n",
    "plt.rcParams[\"grid.linestyle\"] = \"--\"\n",
    "nvars = len(variables)\n",
    "nvars_half = int(np.ceil(nvars / 2))\n",
    "\n",
    "def subplots_2byN(layout=\"constrained\", **kwargs) -> tuple[plt.Figure, np.ndarray[plt.Axes]]:\n",
    "    \"\"\"\n",
    "    Create a figure with 2 x (N/2) panels, with N the number of variables.\n",
    "    Return them unravelled, turning off and removing any spares (e.g. 8th panel for 7 variables).\n",
    "    \"\"\"\n",
    "    # Create figure, panels\n",
    "    fig, axs = plt.subplots(nrows=nvars_half, ncols=2, layout=layout, **kwargs)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    # White out last panel if odd number of variables\n",
    "    for ax in axs[nvars:]:\n",
    "        ax.set_axis_off()\n",
    "    axs = axs[:nvars]\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistics\n",
    "# Difference between datasets\n",
    "def difference_between_datasets(dataset1: xr.Dataset, dataset2: xr.Dataset, *, \n",
    "                                diff_variables: Iterable[str]=variables) -> xr.Dataset:\n",
    "    \"\"\" Calculate the difference between two datasets, preserving CRS and updating metadata. \"\"\"\n",
    "    # Subtract\n",
    "    difference = dataset1 - dataset2\n",
    "\n",
    "    # Adjust metadata\n",
    "    for var in diff_variables:\n",
    "        old_metadata = dataset1[var].attrs\n",
    "        updated_metadata = old_metadata | {\"long_name\": r\"Î” \" + old_metadata[\"long_name\"]}\n",
    "        difference[var] = adjust_metadata(difference[var], **updated_metadata)\n",
    "\n",
    "    # Add name\n",
    "    name1, name2 = [dataset.name if hasattr(dataset, \"name\") else \"<unspecified>\" for dataset in (dataset1, dataset2)]\n",
    "    difference = difference.assign_attrs({\"name\": f\"Difference: {name1} â€“ {name2}\"})\n",
    "        \n",
    "    return difference\n",
    "\n",
    "def relative_difference_between_datasets(dataset1: xr.Dataset, dataset2: xr.Dataset, *,\n",
    "                                         diff_variables: Iterable[str]=variables, \n",
    "                                         exclude_substrings: Iterable[str]=[\"Temperature\"]) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Calculate the relative [%] difference between two datasets, preserving CRS and updating metadata.\n",
    "    Relative difference is calculated symmetrically, i.e. divided by (dataset1 + dataset2)/2.\n",
    "    Where dataset1 == 0 and dataset2 == 0, the relative difference is set to 0 too.\n",
    "    Variables can be excluded with exclude_substrings; e.g. don't divide temperatures.\n",
    "    \"\"\"\n",
    "    # Select and calculate\n",
    "    exclude_variables = [var for var in diff_variables if any(sub in var for sub in exclude_substrings)]\n",
    "    diff_variables = [var for var in diff_variables if not var in exclude_variables]\n",
    "    dataset1, dataset2 = [dataset.drop_vars(exclude_variables) for dataset in (dataset1, dataset2)]\n",
    "    \n",
    "    relative_difference = (dataset1 - dataset2) / (dataset1 + dataset2) * 200.\n",
    "\n",
    "    # Replace 0/0 with 0\n",
    "    both_zero = ((dataset1 + dataset2) <= 1e-5)  # Threshold slightly > 0 because of floating-point errors\n",
    "    relative_difference = relative_difference.where(~both_zero, 0.)\n",
    "\n",
    "    # Adjust metadata\n",
    "    for var in diff_variables:\n",
    "        old_metadata = dataset1[var].attrs\n",
    "        updated_metadata = old_metadata | {\"long_name\": r\"rÎ” \" + old_metadata[\"long_name\"], \"units\": \"%\"}\n",
    "        relative_difference[var] = adjust_metadata(relative_difference[var], **updated_metadata)\n",
    "\n",
    "    # Add name\n",
    "    name1, name2 = [dataset.name if hasattr(dataset, \"name\") else \"<unspecified>\" for dataset in (dataset1, dataset2)]\n",
    "    relative_difference = relative_difference.assign_attrs({\"name\": f\"% Difference: {name1} â€“ {name2}\"})\n",
    "\n",
    "    return relative_difference\n",
    "    \n",
    "\n",
    "def difference_statistics(dataset1: xr.Dataset, dataset2: xr.Dataset, *,\n",
    "                          diff_variables: Iterable[str]=variables) -> pd.DataFrame:\n",
    "    \"\"\" Given two datasets, calculate a number of statistics for each variable and return the result in a table. \"\"\"\n",
    "    differences = difference_between_datasets(dataset1, dataset2, diff_variables=diff_variables)\n",
    "    differences_rel = relative_difference_between_datasets(dataset1, dataset2, diff_variables=diff_variables)\n",
    "\n",
    "    # Convert to pandas\n",
    "    differences = differences.to_dataframe()\n",
    "    differences_abs = differences.abs()\n",
    "\n",
    "    differences_rel = differences_rel.to_dataframe()\n",
    "    differences_rel_abs = differences_rel.abs()\n",
    "\n",
    "    # Calculate aggregate statistics\n",
    "    # Add quantiles?\n",
    "    md = differences.agg([\"median\", \"mean\"])  \\\n",
    "                    .rename({\"median\": r\"Median\", \"mean\": \"Mean\"})\n",
    "    mad = differences_abs.agg([\"median\"])  \\\n",
    "                         .rename({\"median\": r\"Median Abs.\"})\n",
    "    mapd = differences_rel_abs.agg([\"median\"])  \\\n",
    "                              .rename({\"median\": r\"Median Abs. %\"})\n",
    "    stats = pd.concat([md, mad, mapd])\n",
    "    stats = stats[diff_variables].T\n",
    "    return stats\n",
    "\n",
    "def display_difference_stats(dataset1: xr.Dataset, dataset2: xr.Dataset, **kwargs) -> str:\n",
    "    \"\"\" Given two datasets, calculate a number of statistics for each variable and display the result in a table. \"\"\"\n",
    "    differences = difference_between_datasets(dataset1, dataset2, **kwargs)\n",
    "    difference_stats = difference_statistics(dataset1, dataset2, **kwargs)\n",
    "    formatted = difference_stats.style \\\n",
    "                                .format(precision=3)  \\\n",
    "                                .set_caption(differences.name)  \\\n",
    "                                .relabel_index([label_with_unit(differences, var) for var in difference_stats.index])\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting functions\n",
    "# Colour maps\n",
    "def find_percentile(*data_arrays: Iterable[xr.DataArray], percentile: float, round: str=None) -> float:\n",
    "    \"\"\"\n",
    "    Find the specified percentile across all of the provided datasets.\n",
    "    Used for making consistent colour maps.\n",
    "    \"\"\"\n",
    "    data_flat = np.concatenate([arr.to_numpy().ravel() for arr in data_arrays])\n",
    "    perc = np.nanpercentile(data_flat, percentile)\n",
    "    if round == \"up\":\n",
    "        perc = np.ceil(perc)\n",
    "    elif round == \"down\":\n",
    "        perc = np.floor(perc)\n",
    "    return perc\n",
    "\n",
    "cmap_percentile = 0.5\n",
    "find_vmin = partial(find_percentile, percentile=cmap_percentile)\n",
    "find_vmax = partial(find_percentile, percentile=100-cmap_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "(section-download)=\n",
    "### 2. Download data\n",
    "#### General setup\n",
    "This notebook uses [earthkit-data](https://github.com/ecmwf/earthkit-data) to download files from the CDS.\n",
    "If you intend to run this notebook multiple times, it is highly recommended that you [enable caching](https://earthkit-data.readthedocs.io/en/latest/guide/caching.html) to prevent having to download the same files multiple times.\n",
    "\n",
    "We will be downloading multiple datasets in this notebook.\n",
    "In this section, we define the parameters common to all datasets: time and space.\n",
    "This way, these only need to be changed in one place if you wish to modify the notebook for your own use case.\n",
    "\n",
    "In this example, we will be looking at data for the United Kingdom and Ireland every day in Januaryâ€“September 2023.\n",
    "We will also do a time series comparison in one site within this area.\n",
    "These settings can easily be changed by editing the variables in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = ekp.geo.domains.union([\"United Kingdom\", \"Ireland\"], name=\"UK & Ireland\")\n",
    "site = {\"lat\": 52.5, \"lon\": 0.0}\n",
    "year = 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We now set up default CDS request parameters based on these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_domain = domain_to_request(domain)\n",
    "\n",
    "request_time = {\n",
    "    \"year\": year,\n",
    "    \"month\": [f\"{mo:02}\" for mo in range(1, 10)],\n",
    "    \"day\": [f\"{d:02}\" for d in range(1, 32)],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We can define a helper function that adds the time and domain parameters, as well as a dictionary of parameters specific to one dataset (e.g. AgERA5, ERA5-Land), to a number of requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_request(request_dataset: dict, *requests: dict) -> dict:\n",
    "    \"\"\" Combine default requests (time + domain) with a dataset-specific default request (request_dataset) and any number of individual (e.g. variable-specific) requests. \"\"\"\n",
    "    base_request = request_time | request_domain | request_dataset\n",
    "    updated_requests = [base_request | req for req in requests]\n",
    "    return updated_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### AgERA5\n",
    "We now define parameters unique to AgERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "agera5_ID = \"sis-agrometeorological-indicators\"\n",
    "\n",
    "request_agera5 = {\n",
    "    \"version\": \"2_0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Next, we specify the variables of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_irradiation = {\n",
    "    \"variable\": \"solar_radiation_flux\",\n",
    "}\n",
    "\n",
    "# Temperature has to be split into separate requests because of size limits\n",
    "request_temperature_min = {\n",
    "    \"variable\": \"2m_temperature\",\n",
    "    \"statistic\": [\"24_hour_minimum\"],\n",
    "}\n",
    "\n",
    "request_temperature_max = {\n",
    "    \"variable\": \"2m_temperature\",\n",
    "    \"statistic\": [\"24_hour_maximum\"],\n",
    "}\n",
    "\n",
    "request_temperature_mean = {\n",
    "    \"variable\": \"2m_temperature\",\n",
    "    \"statistic\": [\"24_hour_mean\"],\n",
    "}\n",
    "\n",
    "request_rain = {\n",
    "    \"variable\": \"precipitation_flux\",\n",
    "}\n",
    "\n",
    "request_wind = {\n",
    "    \"variable\": \"10m_wind_speed\",\n",
    "    \"statistic\": [\"24_hour_mean\"],\n",
    "}\n",
    "\n",
    "request_snow = {\n",
    "    \"variable\": \"snow_thickness\",\n",
    "    \"statistic\": [\"24_hour_mean\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "The requests for specific variables are combined with the default, time, and domain parameters and passed to earthkit for download from the CDS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_agera5_combined = make_full_request(request_agera5,\n",
    "                                             request_irradiation,\n",
    "                                             request_temperature_min, request_temperature_max, request_temperature_mean,\n",
    "                                             request_rain,\n",
    "                                             request_wind,\n",
    "                                             request_snow,\n",
    "                                            )\n",
    "\n",
    "ds_agera5 = ekd.from_source(\"cds\", agera5_ID, *requests_agera5_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Earthkit-data downloads the dataset as a [field list](https://earthkit-data.readthedocs.io/en/latest/guide/data.html), which can be manipulated directly.\n",
    "Here, we convert it to an Xarray object for ease of use later (when comparing multiple datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AgERA5 data type from earthkit-data:\", type(ds_agera5))\n",
    "data_agera5 = ds_agera5.to_xarray(compat=\"equals\")\n",
    "print(\"AgERA5 data type in Xarray:\", type(data_agera5))\n",
    "data_agera5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "#### ERA5-Land\n",
    "We now define parameters unique to ERA5-Land and the variables of interest.\n",
    "This will involve two CDS datasets.\n",
    "The [*ERA5-Land hourly data from 1950 to present*](https://doi.org/10.24381/cds.e2161bac) dataset contains hourly data for variables like 2m temperature as well as accumulated data for variables like solar irradiation.\n",
    "For the present use case, we are only interested in the daily accumulated data.\n",
    "[*ERA5-Land post-processed daily statistics from 1950 to present*](https://doi.org/10.24381/cds.e9c9c792) provides daily minimum/maximum/mean statistics for the hourly variables, saving us the effort of aggregating these ourselves.\n",
    "In the following subsections, we will download the data (accumulated and daily statistics) and harmonise these to the same format as AgERA5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "##### Accumulated data from ERA5-Land\n",
    "Per the [documentation for ERA5-Land](https://confluence.ecmwf.int/display/CKB/ERA5-Land%3A+data+documentation#heading-Accumulations):\n",
    "The accumulations in the short forecasts of ERA5-Land (with hourly steps from 01 to 24) are treated the same as those in ERA-Interim or ERA-Interim/Land, i.e., they are accumulated from the beginning of the forecast to the end of the forecast step. For example, runoff at day=D, step=12 will provide runoff accumulated from day=D, time=0 to day=D, time=12. The maximum accumulation is over 24 hours, i.e., from day=D, time=0 to day=D+1,time=0 (step=24). For the CDS time, or validity time, of 00 UTC, the accumulations are over the 24 hours ending at 00 UTC i.e. the accumulation is during the previous day.\n",
    "\n",
    "In practice, this means that we need to download data for *day+1* (e.g. 2 January 2024) to get the total accumulated value for *day* (1 January 2024).\n",
    "Our existing `request_time` will provide data for 2023-12-31 â€“ 2024-09-29, so we need to add one extra day.\n",
    "This is achieved by adding a second request for just the last day to the earthkit-data download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5land_ID = \"reanalysis-era5-land\"\n",
    "\n",
    "request_era5land = {\n",
    "    \"time\": [\"00:00\"],\n",
    "    \"data_format\": \"grib\",\n",
    "    \"download_format\": \"unarchived\",\n",
    "}\n",
    "\n",
    "request_era5land_extratime = {\n",
    "    \"month\": \"10\",\n",
    "    \"day\": \"01\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_irradation = {\n",
    "    \"variable\": [\"surface_solar_radiation_downwards\"],\n",
    "}\n",
    "\n",
    "request_rain = {\n",
    "    \"variable\": [\"total_precipitation\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_era5land_irradiation, request_era5land_rain = make_full_request(request_era5land,\n",
    "                                                                        request_irradation,\n",
    "                                                                        request_rain,\n",
    "                                                                       )\n",
    "\n",
    "ds_era5land_accumulated = ekd.from_source(\"cds\", era5land_ID, request_era5land_irradiation, request_era5land_irradiation | request_era5land_extratime,\n",
    "                                                              request_era5land_rain, request_era5land_rain | request_era5land_extratime)\n",
    "data_era5land_accumulated = ds_era5land_accumulated.to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "When we inspect the resulting dataset (in Xarray format), we see that the `forecast_reference_time` coordinate conveniently matches the variable to the day of accumulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_era5land_accumulated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Daily statistics from the post-processed ERA5-Land dataset\n",
    "The daily statistics are indexed according to the day they apply to, meaning we do not need to worry about adding extra days here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5land_ID = \"derived-era5-land-daily-statistics\"\n",
    "\n",
    "request_era5land = {\n",
    "    \"time_zone\": \"utc+00:00\",\n",
    "    \"frequency\": \"1_hourly\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature has to be split into separate requests because of size limits\n",
    "request_temperature_min = {\n",
    "    \"variable\": \"2m_temperature\",\n",
    "    \"daily_statistic\": \"daily_minimum\",\n",
    "}\n",
    "\n",
    "request_temperature_max = {\n",
    "    \"variable\": \"2m_temperature\",\n",
    "    \"daily_statistic\": [\"daily_maximum\"],\n",
    "}\n",
    "\n",
    "request_temperature_mean = {\n",
    "    \"variable\": \"2m_temperature\",\n",
    "    \"daily_statistic\": [\"daily_mean\"],\n",
    "}\n",
    "\n",
    "# request_vapour_pressure : Not available\n",
    "\n",
    "request_wind_u = {\n",
    "    \"variable\": \"10m_u_component_of_wind\",\n",
    "    \"daily_statistic\": [\"daily_mean\"],\n",
    "}\n",
    "\n",
    "request_wind_v = {\n",
    "    \"variable\": \"10m_v_component_of_wind\",\n",
    "    \"daily_statistic\": [\"daily_mean\"],\n",
    "}\n",
    "\n",
    "request_snow = {\n",
    "    \"variable\": \"snow_depth\",\n",
    "    \"daily_statistic\": [\"daily_mean\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "We download the different variables separately in anticipation of the harmonisation step in the next subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_era5land_combined = make_full_request(request_era5land,\n",
    "                                               request_temperature_min, request_temperature_max, request_temperature_mean,\n",
    "                                               request_wind_u, request_wind_v,\n",
    "                                               request_snow,\n",
    "                                              )\n",
    "\n",
    "ds_era5land = [ekd.from_source(\"cds\", era5land_ID, req) for req in requests_era5land_combined]\n",
    "data_era5land = [ds.to_xarray() for ds in ds_era5land]\n",
    "data_era5land_temperature_min, data_era5land_temperature_max, data_era5land_temperature_mean, data_era5land_wind_u, data_era5land_wind_v, data_era5land_snow = data_era5land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "##### Harmonisation\n",
    "The ERA5-Land dataset is structured differently from AgERA5 and requires some processing before the two can be intercompared.\n",
    "This involves renaming coordinates and variables, and adjusting units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "For the accumulated data, we need to rename the variables and coordinates to match those in AgERA5, and convert the units for precipitation to mm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_era5land_accumulated = data_era5land_accumulated.rename({\"ssrd\": \"Solar_Radiation_Flux\",\n",
    "                                                              \"tp\": \"Precipitation_Flux\",\n",
    "                                                              \"forecast_reference_time\": \"time\", \n",
    "                                                              \"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "\n",
    "convert_m2mm(data_era5land_accumulated, \"Precipitation_Flux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The temperature statistics are downloaded as simply `t2m`.\n",
    "These need to be renamed before they can be combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_era5land_temperature_min = data_era5land_temperature_min.rename({\"t2m\": \"Temperature_Air_2m_Min_24h\"})\n",
    "data_era5land_temperature_max = data_era5land_temperature_max.rename({\"t2m\": \"Temperature_Air_2m_Max_24h\"})\n",
    "data_era5land_temperature_mean = data_era5land_temperature_mean.rename({\"t2m\": \"Temperature_Air_2m_Mean_24h\"})\n",
    "data_era5land_temperature = xr.merge([data_era5land_temperature_min, data_era5land_temperature_max, data_era5land_temperature_mean], compat=\"equals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "The 10 m wind speed is calculated from the two variables representing its U (eastâ€“west) and V (northâ€“south) components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_era5land_wind = xr.merge([data_era5land_wind_u, data_era5land_wind_v], compat=\"equals\")\n",
    "data_era5land_wind = data_era5land_wind.assign(\n",
    "    {\"Wind_Speed_10m_Mean_24h\": xr.ufuncs.sqrt(data_era5land_wind[\"u10\"]**2 + data_era5land_wind[\"v10\"]**2)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Lastly, we rename the snow variable and change its unit to match AgERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_era5land_snow = data_era5land_snow.rename({\"sde\": \"Snow_Thickness_Mean_24h\"})\n",
    "convert_m2cm(data_era5land_snow, \"Snow_Thickness_Mean_24h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Now we can combine the pre-processed variables into a single Xarray dataset.\n",
    "We also rename the coordinates to match AgERA5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_era5land = xr.merge([data_era5land_temperature, data_era5land_wind, data_era5land_snow], compat=\"equals\")\n",
    "data_era5land = data_era5land.rename({\"valid_time\": \"time\", \"latitude\": \"lat\", \"longitude\": \"lon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Lastly, we combine the pre-processed and accumulated variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_era5land = xr.merge([data_era5land_accumulated, data_era5land], compat=\"equals\")\n",
    "data_era5land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### E-OBS\n",
    "We now define parameters unique to E-OBS and the variables of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "eobs_ID = \"insitu-gridded-observations-europe\"\n",
    "request_eobs = {\n",
    "    \"product_type\": \"ensemble_mean\",\n",
    "    \"variable\": [\n",
    "        \"mean_temperature\",\n",
    "        \"minimum_temperature\",\n",
    "        \"maximum_temperature\",\n",
    "        \"precipitation_amount\",\n",
    "        \"surface_shortwave_downwelling_radiation\",\n",
    "        \"wind_speed\"\n",
    "    ],\n",
    "    \"grid_resolution\": \"0_1deg\",\n",
    "    \"period\": \"2011_2024\",\n",
    "    \"version\": [\"30_0e\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eobs = ekd.from_source(\"cds\", eobs_ID, request_eobs | request_domain)\n",
    "data_eobs = ds_eobs.to_xarray(compat=\"equals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "eobs_ID = \"insitu-gridded-observations-europe\"\n",
    "request_eobs = {\n",
    "    \"product_type\": \"ensemble_mean\",\n",
    "    \"variable\": [\n",
    "        \"mean_temperature\",\n",
    "    ],\n",
    "    \"grid_resolution\": \"0_1deg\",\n",
    "    \"period\": \"2011_2024\",\n",
    "    \"version\": [\"30_0e\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do individually to avoid xr error?\n",
    "ds_eobs = ekd.from_source(\"cds\", eobs_ID, request_eobs | request_domain)\n",
    "data_eobs = ds_eobs.to_xarray(compat=\"equals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Rename variables and coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### General harmonisation\n",
    "##### Grid alignment\n",
    "Before the data can be analysed, we must make sure their coordinates are aligned equally.\n",
    "All of the datasets used here are provided on a regular 0.1Â° by 0.1Â° grid, so no regridding is necessary.\n",
    "However, two steps need to be taken before the data can be compared directly:\n",
    "* Their representation as floating-point numbers can cause very small differences to appear, which do not reflect any real differences in the data but are difficult for software (in this case Xarray) to work with. Knowing that the data are on a regular 0.1Â° by 0.1Â° grid, we can simply round all of the coordinates to 2 digits to force them to be the same.\n",
    "* The bounds of the datasets (in space and in time) are slightly different and need to be aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [data_agera5, data_era5land]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round coordinates before alignment to avoid floating-point errors\n",
    "# Cannot be a dict-comp with coord in lat/lon because of symbol table issues\n",
    "d = 2\n",
    "round_mapping = {\"lat\": (lambda dataset: dataset[\"lat\"].round(d)),\n",
    "                 \"lon\": (lambda dataset: dataset[\"lon\"].round(d))}\n",
    "\n",
    "datasets_rounded = [dataset.assign_coords(round_mapping) for dataset in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align data using an inner join\n",
    "data_agera5, data_era5land, data_eobs = xr.align(*datasets_rounded, data_eobs, join=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "##### Units and variable names\n",
    "We also convert the temperatures from Kelvin to degrees Celsius, which are more commonly used in agricultural studies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in variables_temperature:\n",
    "    convert_K2C(data_agera5, variable)\n",
    "    convert_K2C(data_era5land, variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "Lastly, we harmonise the variable \"long\" names and units, as stored in xarray metadata, to simplify the analysis steps and figures later on.\n",
    "This is not strictly necessary, but it is convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agera5 = adjust_names(data_agera5, \"AgERA5\")\n",
    "data_era5land = adjust_names(data_era5land, \"ERA5-Land\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.245188,
     "end_time": "2024-03-08T17:39:21.277354",
     "exception": false,
     "start_time": "2024-03-08T17:39:21.032166",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(section-results)=\n",
    "### 3. Results\n",
    "Describe what is done in this step/section and what the `code` in the cell does (if code is included). \n",
    "\n",
    "If this is the **results section**, we expect the final plots to be created here with a description of how to interpret them, and what information can be extracted for the specific use case and user question. The information in the 'quality assessment statement' should be derived here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Point-by-point comparison\n",
    "Here, we compare the different variables 1-to-1 between the different datasets, across their entire spatial and temporal domain, to determine the typical differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "We start with a histogram of the 1-to-1 differences as well as the corresponding statistics: median absolute deviation (MAD), median bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = difference_between_datasets(data_agera5, data_era5land)\n",
    "display_difference_stats(data_agera5, data_era5land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axs = subplots_2byN(figsize=(8, 8))\n",
    "\n",
    "# Plot individual scatter plots\n",
    "for ax, var in zip(axs, variables):\n",
    "    diff = differences[var]\n",
    "\n",
    "    # Symmetric x axis\n",
    "    low, high = find_vmin(diff), find_vmax(diff)\n",
    "    lim = max(abs(low), abs(high))\n",
    "    bins = np.linspace(-lim, lim, 500)\n",
    "\n",
    "    # Plot differences\n",
    "    try:  # Account for missing variables\n",
    "        ax.hist(diff.values.ravel(),\n",
    "                bins=bins, color=\"black\")\n",
    "    except KeyError:\n",
    "        print(f\"KeyError: no variable `{var}` in one of the datasets\")\n",
    "    \n",
    "    # X-axis settings\n",
    "    ax.set_xlabel(label_with_unit(differences, var, latex=True))\n",
    "    ax.set_xlim(-lim, lim)\n",
    "\n",
    "# Visual settings\n",
    "for ax in axs:\n",
    "    grid = ax.grid(True, axis=\"both\")\n",
    "    ax.set_title(\"\")\n",
    "\n",
    "    # Highlight 0\n",
    "    ax.axvline(0, color=plt.rcParams[\"grid.color\"], linewidth=1.5, linestyle=\"-\")\n",
    "\n",
    "fig.suptitle(f\"{differences.name} (overall distribution)\")\n",
    "\n",
    "# Uncomment if running this notebook yourself:\n",
    "# Show result\n",
    "plt.show()\n",
    "\n",
    "# Uncomment if building the Jupyter-book web version:\n",
    "glue(\"fig_histogram\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "```{glue:figure} fig_histogram\n",
    ":figwidth: 700px\n",
    ":name: \"fig_histogram\"\n",
    "\n",
    "Distributions of point-by-point differences between AgERA5 and ERA5-Land for each variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "We can use a scatter density plot to look for patterns in the 1-to-1 comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axs = subplots_2byN(figsize=(10, 20))\n",
    "\n",
    "# Plot individual scatter plots\n",
    "for ax, var in zip(axs, variables):\n",
    "    try:  # Account for missing variables\n",
    "        ax.hexbin(data_agera5[var].values.ravel(), data_era5land[var].values.ravel(),\n",
    "                  gridsize=500, mincnt=1, cmap=\"cividis\")\n",
    "    except KeyError:\n",
    "        print(f\"KeyError: no variable `{var}` in one of the datasets\")\n",
    "\n",
    "    ax.text(0.05, 0.95, label_with_unit(data_agera5, var, latex=True), \n",
    "            horizontalalignment=\"left\", verticalalignment=\"top\", transform=ax.transAxes,\n",
    "            bbox={\"facecolor\": \"white\", \"edgecolor\": \"black\", \"alpha\": 1})\n",
    "\n",
    "# Visual settings\n",
    "for ax in axs:\n",
    "    ax.grid(True, axis=\"both\")\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.set_xlabel(data_agera5.name)\n",
    "    ax.set_ylabel(data_era5land.name)\n",
    "    ax.set_title(\"\")\n",
    "\n",
    "    # Highlight diagonal\n",
    "    ax.axline((0, 0), slope=1, color=plt.rcParams[\"grid.color\"], linewidth=1.5, linestyle=\"-\")\n",
    "\n",
    "fig.suptitle(f\"Point-by-point comparison\")\n",
    "\n",
    "# Uncomment if running this notebook yourself:\n",
    "# Show result\n",
    "plt.show()\n",
    "\n",
    "# Uncomment if building the Jupyter-book web version:\n",
    "glue(\"fig_scatter\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "```{glue:figure} fig_scatter\n",
    ":figwidth: 700px\n",
    ":name: \"fig_scatter\"\n",
    "\n",
    "Point-by-point comparison (density scatter) between AgERA5 and ERA5-Land for each variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "#### Time series comparison\n",
    "Crop yield estimates make use of the time series of the relevant variables in a given site.\n",
    "Therefore, a comparison between time series from the different datasets should provide a better estimate of the differences downstream than an overall point-by-point comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "First, we select data at our test site (defined at the top of this notebook) and display the differences between the time series from the different datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_agera5, timeseries_era5land = extract_timeseries(site, data_agera5, data_era5land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_difference_stats(timeseries_agera5, timeseries_era5land)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "We now create a plot showing all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axs = plt.subplots(nrows=nvars, figsize=(10, 20), sharex=True, layout=\"constrained\")\n",
    "\n",
    "# Plot individual time series\n",
    "for j, timeseries in enumerate([timeseries_agera5, timeseries_era5land]):\n",
    "    for ax, var in zip(axs, variables):\n",
    "        try:  # Account for missing variables\n",
    "            timeseries[var].plot(ax=ax, alpha=0.7)\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: no variable `{var}` in dataset {j}\")\n",
    "\n",
    "        ax.set_ylabel(label_with_unit(timeseries, var, latex=True))\n",
    "\n",
    "# Visual settings\n",
    "for ax in axs:\n",
    "    ax.grid(True, axis=\"both\")\n",
    "    ax.tick_params(\"x\", labelbottom=True)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_title(\"\")\n",
    "\n",
    "    ax.axhline(0, color=plt.rcParams[\"grid.color\"], linewidth=1.5, linestyle=\"-\")\n",
    "\n",
    "fig.suptitle(f\"Time series comparison at ({site['lat']} Â°N, {site['lon']} Â°E)\")\n",
    "\n",
    "# Uncomment if running this notebook yourself:\n",
    "# Show result\n",
    "plt.show()\n",
    "\n",
    "# Uncomment if building the Jupyter-book web version:\n",
    "glue(\"fig_timeseries\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "```{glue:figure} fig_timeseries\n",
    ":figwidth: 700px\n",
    ":name: \"fig_timeseries\"\n",
    "\n",
    "Time series comparison between AgERA5 and ERA5-Land for each variable, in one site.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.007051,
     "end_time": "2024-03-08T17:23:56.492658",
     "exception": false,
     "start_time": "2024-03-08T17:23:56.485607",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Geospatial comparison\n",
    "In this section, we visually compare the datasets in space on a single day, using earthkit-plots.\n",
    "This requires a bit of setup, namely defining the domain (time, space, datasets) and defining some plot parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain for plot (time, space, datasets)\n",
    "date = f\"{year}0101\"  # 1 January\n",
    "datasets_singleday = [dataset.sel(time=date) for dataset in (data_agera5, data_era5land)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define styles for earthkit-plots\n",
    "# Temperature: Share between min/mean/max\n",
    "temperature_data = [dataset[key] for key in variables_temperature for dataset in datasets_singleday]\n",
    "temperature_vmin = find_vmin(*temperature_data, round=\"down\")\n",
    "temperature_vmax = find_vmax(*temperature_data, round=\"up\")\n",
    "style_temperature = ekp.styles.Style(\n",
    "    levels = np.arange(temperature_vmin, temperature_vmax+2, 2),\n",
    "    extend = \"both\",\n",
    ")\n",
    "\n",
    "# Surface radiation: Start at 0, in steps of 0.5 x 10^(order)\n",
    "ceil_half = lambda x: np.ceil(x * 2) / 2\n",
    "surface_radiation_data = [dataset[\"Solar_Radiation_Flux\"] for dataset in datasets_singleday]\n",
    "surface_radiation_vmax = find_vmax(*surface_radiation_data)\n",
    "order = int(np.log10(surface_radiation_vmax))\n",
    "vmax_nomag = surface_radiation_vmax / 10**order\n",
    "surface_radiation_vmax = ceil_half(vmax_nomag) * 10**order\n",
    "style_surface_radiation = ekp.styles.Style(\n",
    "    levels = np.arange(0, surface_radiation_vmax+0.1, 0.5 * 10**order),\n",
    "    extend = \"max\",\n",
    ")\n",
    "\n",
    "# Precipitation: Start at 0\n",
    "precipitation_data = [dataset[\"Precipitation_Flux\"] for dataset in datasets_singleday]\n",
    "precipitation_vmax = find_vmax(*precipitation_data, round=\"up\")\n",
    "style_precipitation = ekp.styles.Style(\n",
    "    levels = np.arange(0, precipitation_vmax, 2),\n",
    "    extend = \"max\",\n",
    ")\n",
    "\n",
    "# Wind speed: Start at 0\n",
    "wind_data = [dataset[\"Wind_Speed_10m_Mean_24h\"] for dataset in datasets_singleday]\n",
    "wind_vmax = find_vmax(*wind_data, round=\"up\")\n",
    "style_wind = ekp.styles.Style(\n",
    "    levels = np.arange(0, wind_vmax, 1),\n",
    "    extend = \"max\",\n",
    ")\n",
    "\n",
    "# Snow depth: Start at 0\n",
    "snow_data = [dataset[\"Snow_Thickness_Mean_24h\"] for dataset in datasets_singleday]\n",
    "snow_vmax = find_vmax(*snow_data, round=\"up\")\n",
    "style_snow = ekp.styles.Style(\n",
    "    levels = np.arange(0, snow_vmax, 2),\n",
    "    extend = \"both\",\n",
    ")\n",
    "\n",
    "## Link styles to variable names\n",
    "style_dict = {\n",
    "    variable: style_temperature for variable in variables_temperature\n",
    "} | {  # Combine temperature variables (all the same) with others (all different)\n",
    "    \"Solar_Radiation_Flux\": style_surface_radiation,\n",
    "    \"Precipitation_Flux\": style_precipitation,\n",
    "    \"Wind_Speed_10m_Mean_24h\": style_wind,\n",
    "    \"Snow_Thickness_Mean_24h\": style_snow,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = ekp.Figure(rows=nvars, columns=2, size=(6, 20))\n",
    "\n",
    "# Plot variables\n",
    "for row, variable in enumerate(variables):\n",
    "    # Get style from dictionary\n",
    "    style = style_dict[variable]\n",
    "\n",
    "    # Plot individual datasets\n",
    "    for col, dataset in enumerate(datasets_singleday):\n",
    "        subplot = fig.add_map(domain=domain)\n",
    "        try:  # Account for missing variables\n",
    "            subplot.grid_cells(dataset, z=variable, style=style)\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: no variable `{variable}` in dataset {col}\")\n",
    "\n",
    "        # Text\n",
    "        ax = subplot.ax\n",
    "        ax.text(0.05, 0.95, dataset.name, \n",
    "                horizontalalignment=\"left\", verticalalignment=\"top\", transform=ax.transAxes,\n",
    "                bbox={\"facecolor\": \"white\", \"edgecolor\": \"black\", \"alpha\": 1})\n",
    "\n",
    "    # Colour bar on the right (last ax)\n",
    "    subplot.legend(label=\"{variable_name} [{units:~F}]\", location=\"right\")\n",
    "\n",
    "# Decorate figures\n",
    "fig.land()\n",
    "fig.coastlines()\n",
    "fig.gridlines(linestyle=plt.rcParams[\"grid.linestyle\"])\n",
    "fig.title(\"Geospatial intercomparison: {time:%-d %B %Y}\")\n",
    "\n",
    "# Uncomment if running this notebook yourself:\n",
    "# Show result\n",
    "plt.show()\n",
    "\n",
    "# Uncomment if building the Jupyter-book web version:\n",
    "fig = fig.fig  # Extract matplotlib fig from earthkit fig\n",
    "glue(\"fig_geo\", fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "```{glue:figure} fig_geo\n",
    ":figwidth: 700px\n",
    ":name: \"fig_geo\"\n",
    "\n",
    "Geospatial comparison between AgERA5 and ERA5-Land.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "Note that differences in snow thickness are highest in areas where you wouldn't generally have crops anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.271597,
     "end_time": "2024-03-08T17:40:01.664067",
     "exception": false,
     "start_time": "2024-03-08T17:40:01.392470",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## â„¹ï¸ If you want to know more\n",
    "\n",
    "### Key resources\n",
    "The CDS catalogue entries for the data used were:\n",
    "* Agrometeorological indicators from 1979 to present derived from reanalysis (AgERA5): [sis-agrometeorological-indicators](https://doi.org/10.24381/cds.6c68c9bb)\n",
    "* ERA5-Land hourly data from 1950 to present: [reanalysis-era5-land](https://doi.org/10.24381/cds.e2161bac)\n",
    "* ERA5-Land post-processed daily statistics from 1950 to present: [derived-era5-land-daily-statistics](https://doi.org/10.24381/cds.e9c9c792)\n",
    "\n",
    "Code libraries used:\n",
    "* [earthkit](https://github.com/ecmwf/earthkit)\n",
    "  * [earthkit-data](https://github.com/ecmwf/earthkit)\n",
    "  * [earthkit-plots](https://github.com/ecmwf/earthkit-plots)\n",
    "\n",
    "More about crop yield estimation:\n",
    "* [A gentle introduction to WOFOST](https://www.wur.nl/en/show/a-gentle-introduction-to-wofost.htm)\n",
    "* [Crop yield prediction based on reanalysis and crop phenology data in the agroclimatic zones](https://doi.org/10.1007/s00704-024-05046-x)\n",
    "* [Historical trends and future projections of compound cloudy-rainy events during the global winter wheat harvest phase](https://doi.org/10.1016/j.agrformet.2025.110637)\n",
    "\n",
    "More about reanalysis data:\n",
    "* [The ERA5 global reanalysis](https://doi.org/10.1002/qj.3803)\n",
    "* [ERA5-Land: a state-of-the-art global reanalysis dataset for land applications](https://doi.org/10.5194/essd-13-4349-2021)\n",
    "* AgERA5\n",
    "  * [Algorithm Theoretical Basis (ATBD)](https://confluence.ecmwf.int/pages/viewpage.action?pageId=278550984)\n",
    "  * [Product User Guide and Specification (PUGS)](https://confluence.ecmwf.int/pages/viewpage.action?pageId=278551004)\n",
    "  * [Global Agriculture Downscaling and bias correction](https://confluence.ecmwf.int/display/CKB/Global+Agriculture+Downscaling+and+bias+correction)\n",
    "  * [AgERA5tools Python package](https://github.com/ajwdewit/agera5tools)\n",
    "\n",
    "Validation case studies for AgERA5:\n",
    "* [AgERA5, MERRA-2 in Guinea-Bissau](https://doi.org/10.3390/hydrology12070161)\n",
    "* [AgERA5, ERA5, CHIRP/CHIRPS, ENACTS, TAMSAT, PERSIANN-CDR/PERSIANN-CCS-CDR in Ghana and Zambia](https://doi.org/10.1007/s00704-025-05462-7)\n",
    "* [AgERA5, ERA5-Land, E-OBS, CHELSA-W5E5, MSWEP, CHIRPS05, IMERG V06 in Greece](https://doi.org/10.1016/j.atmosres.2024.107888)\n",
    "* [AgERA5, ERA5-Land, MERRA-2, PERSIANN-CDR in Peru](https://doi.org/10.1590/2318-0331.302520240068)\n",
    "* [AgERA5, CHIRPS, PERSIANN in Sri Lanka](https://doi.org/10.1109/MERCon63886.2024.10689062)\n",
    "\n",
    "### References\n",
    "[[CDS AgERA5]](https://doi.org/10.24381/cds.6c68c9bb) Boogaard, H., Schubert, J., De Wit, A., Lazebnik, J., Hutjes, R., Van der Grijn, G., (2020): Agrometeorological indicators from 1979 to present derived from reanalysis. Copernicus Climate Change Service (C3S) Climate Data Store (CDS). DOI: 10.24381/cds.6c68c9bb (Accessed on DD-MMM-YYYY)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 968.520731,
   "end_time": "2024-03-08T17:40:03.783430",
   "environment_variables": {},
   "exception": null,
   "input_path": "D520.3.2.3b.SEASONAL_multimodel-bias_v5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T17:23:55.262699",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
